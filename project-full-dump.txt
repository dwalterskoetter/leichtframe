===== FILE: .github/workflows/cd.yml =====
name: LeichtFrame CD (Release)

on:
  workflow_dispatch:
    inputs:
      version:
        description: "Version to publish (e.g. 0.1.0)"
        required: true
        default: "0.1.0-alpha"
  push:
    tags:
      - "v*"

permissions:
  contents: write

jobs:
  # JOB 1: NuGet Release
  nuget-release:
    name: Publish NuGet
    runs-on: ubuntu-latest
    env:
      VERSION: "0.0.0-ci"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET 8
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 8.0.x

      # Find Version
      - name: Set Version (Manual)
        if: github.event_name == 'workflow_dispatch'
        run: echo "VERSION=${{ inputs.version }}" >> $GITHUB_ENV

      - name: Set Version (Tag)
        if: github.event_name == 'push'
        run: echo "VERSION=${GITHUB_REF#refs/tags/v}" >> $GITHUB_ENV

      # Build & Push
      - name: Restore
        run: dotnet restore

      - name: Build & Test
        run: |
          dotnet build --no-restore -c Release -p:Version=$VERSION
          dotnet test --no-build -c Release

      - name: Pack
        run: dotnet pack --no-build -c Release -o nupkgs -p:Version=$VERSION

      - name: Push to NuGet
        run: |
          dotnet nuget push nupkgs/*.nupkg \
            --api-key ${{ secrets.NUGET_API_KEY }} \
            --source https://api.nuget.org/v3/index.json \
            --skip-duplicate

    outputs:
      release_version: ${{ env.VERSION }}

  # JOB 2: Docusaurus Update
  update-docs:
    needs: nuget-release
    uses: ./.github/workflows/docs.yml
    with:
      version_tag: ${{ needs.nuget-release.outputs.release_version }}
    secrets: inherit

===== FILE: .github/workflows/ci.yml =====
name: LeichtFrame CI (Integration)

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]
  workflow_dispatch:

jobs:
  build-and-test:
    name: Build & Test
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET 8
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 8.0.x
          cache: true
          cache-dependency-path: "**/*.csproj"

      - name: Restore Dependencies
        run: dotnet restore

      - name: Build
        run: dotnet build --no-restore -c Release

      - name: Test
        run: dotnet test --no-build -c Release --verbosity normal

      - name: Restore Local Tools
        run: dotnet tool restore

      - name: Verify Documentation Generation
        run: |
          chmod +x scripts/generate_docs.sh
          ./scripts/generate_docs.sh

      # We pack to ensure metadata is valid and code is packable.
      # This is a "Dry Run" - we do not publish this artifact to NuGet.org.
      - name: Pack (Dry Run)
        run: dotnet pack --no-build -c Release -o nupkgs

      - name: Upload Artifacts (Verification)
        uses: actions/upload-artifact@v4
        with:
          name: pre-release-packages
          path: nupkgs/*.nupkg
          retention-days: 1

===== FILE: .github/workflows/docs.yml =====
name: LeichtFrame Docs

on:
  # 1. Automatically on changes to docs or core code on main
  push:
    branches: ["main"]
    paths:
      - "website/**"
      - "src/LeichtFrame.Core/**" # Because XML docs need to be generated
      - "src/LeichtFrame.IO/**"
      - "docs/**"

  # 2. Manual trigger (only update docs)
  workflow_dispatch:

  # 3. Callable by other workflows (e.g. CD)
  workflow_call:
    inputs:
      version_tag:
        description: "Version context for the deployment message"
        required: false
        type: string
        default: "latest"

permissions:
  contents: write

jobs:
  deploy-docs:
    name: Build & Deploy Docs
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # --- Step A: .NET Build for API Reference ---
      - name: Setup .NET 8
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 8.0.x

      - name: Build for DocGen (Publish to include dependencies)
        run: dotnet publish -c Release

      - name: Generate API Docs (Markdown)
        run: |
          chmod +x scripts/generate_docs.sh
          ./scripts/generate_docs.sh

      # --- Step B: Docusaurus Build ---
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: "npm"
          cache-dependency-path: website/package-lock.json

      - name: Install & Build Website
        working-directory: website
        run: |
          npm ci
          npm run build

      # --- Step C: Deploy ---
      - name: Determine Commit Message
        id: msg
        run: |
          if [ "${{ inputs.version_tag }}" != "" ] && [ "${{ inputs.version_tag }}" != "latest" ]; then
             echo "content=Docs: Deploy release ${{ inputs.version_tag }}" >> $GITHUB_OUTPUT
          else
             echo "content=Docs: Update from main branch" >> $GITHUB_OUTPUT
          fi

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./website/build
          user_name: "github-actions[bot]"
          user_email: "github-actions[bot]@users.noreply.github.com"
          commit_message: ${{ steps.msg.outputs.content }}

===== FILE: src/LeichtFrame.Benchmarks/BenchmarkData.cs =====
using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Order;
using LeichtFrame.Core;
using DuckDB.NET.Data;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    [Orderer(SummaryOrderPolicy.FastestToSlowest)]
    [RankColumn]
    public abstract class BenchmarkData
    {
        [Params(1_000_000)]
        public int N;

        protected LeichtFrame.Core.DataFrame _lfFrame = null!;
        protected DuckDBConnection _duckConnection = null!;

        protected List<TestPoco> _pocoList = null!;

        public record TestPoco(int Id, double Val, string Category, string UniqueId);

        [GlobalSetup]
        public virtual void GlobalSetup()
        {
            var rnd = new Random(42);
            _pocoList = new List<TestPoco>(N);
            var categories = new[] { "A", "B", "C", "D", "E" };

            for (int i = 0; i < N; i++)
            {
                int id = rnd.Next(0, 100_000);
                double val = rnd.NextDouble() * 1000.0;
                string cat = categories[rnd.Next(categories.Length)];
                string uid = Guid.NewGuid().ToString();

                _pocoList.Add(new TestPoco(id, val, cat, uid));
            }

            // --- 1. Setup LeichtFrame ---
            _lfFrame = DataFrame.FromObjects(_pocoList);

            // --- 2. Setup DuckDB ü¶Ü ---
            _duckConnection = new DuckDBConnection("DataSource=:memory:");
            _duckConnection.Open();

            using var cmd = _duckConnection.CreateCommand();

            cmd.CommandText = "CREATE TABLE BenchData (Id INTEGER, Val DOUBLE, Category VARCHAR, UniqueId VARCHAR)";
            cmd.ExecuteNonQuery();

            using (var appender = _duckConnection.CreateAppender("BenchData"))
            {
                foreach (var item in _pocoList)
                {
                    var row = appender.CreateRow();
                    row.AppendValue(item.Id);
                    row.AppendValue(item.Val);
                    row.AppendValue(item.Category);
                    row.AppendValue(item.UniqueId);
                    row.EndRow();
                }
            }
        }

        [GlobalCleanup]
        public virtual void GlobalCleanup()
        {
            _lfFrame?.Dispose();
            _duckConnection?.Dispose();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/AggregationBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core.Operations.Aggregate;

namespace LeichtFrame.Benchmarks
{
    public class AggregationBenchmarks : BenchmarkData
    {
        // =========================================================
        // SUM
        // =========================================================

        [Benchmark(Baseline = true, Description = "DuckDB Sum")]
        public double DuckDB_Sum()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT SUM(Val) FROM BenchData";
            return (double)cmd.ExecuteScalar()!;
        }

        [Benchmark(Description = "LeichtFrame Sum")]
        public double LF_Sum()
        {
            return _lfFrame.Sum("Val");
        }

        // =========================================================
        // MEAN
        // =========================================================

        [Benchmark(Description = "DuckDB Mean")]
        public double DuckDB_Mean()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT AVG(Val) FROM BenchData";
            return (double)cmd.ExecuteScalar()!;
        }

        [Benchmark(Description = "LeichtFrame Mean")]
        public double LF_Mean()
        {
            return _lfFrame.Mean("Val");
        }

        // =========================================================
        // MIN
        // =========================================================

        [Benchmark(Description = "DuckDB Min")]
        public double DuckDB_Min()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT MIN(Val) FROM BenchData";
            return (double)cmd.ExecuteScalar()!;
        }

        [Benchmark(Description = "LeichtFrame Min")]
        public double LF_Min()
        {
            return _lfFrame.Min("Val");
        }

        // =========================================================
        // MAX
        // =========================================================

        [Benchmark(Description = "DuckDB Max")]
        public double DuckDB_Max()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT MAX(Val) FROM BenchData";
            return (double)cmd.ExecuteScalar()!;
        }

        [Benchmark(Description = "LeichtFrame Max")]
        public double LF_Max()
        {
            return _lfFrame.Max("Val");
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/CalculationBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core.Operations.Transform;
using LeichtFrame.Core;

namespace LeichtFrame.Benchmarks
{
    public class CalculationBenchmarks : BenchmarkData
    {
        // =========================================================
        // VECTORIZED ARITHMETIC (SIMD)
        // =========================================================

        [Benchmark(Baseline = true, Description = "DuckDB Vec Add (Col + Col)")]
        public double DuckDB_Vec_Add()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Val + Val FROM BenchData";
            using var reader = cmd.ExecuteReader();

            double sum = 0;
            while (reader.Read())
            {
                sum += reader.GetDouble(0);
            }
            return sum;
        }

        [Benchmark(Description = "LeichtFrame Vec Add (Col + Col)")]
        public DoubleColumn LF_Vec_Add()
        {
            var col = (DoubleColumn)_lfFrame["Val"];
            // Uses SIMD instructions via Vector<T>
            return col + col;
        }

        [Benchmark(Description = "DuckDB Vec Scalar (Col * 1.5)")]
        public double DuckDB_Vec_Scalar()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Val * 1.5 FROM BenchData";
            using var reader = cmd.ExecuteReader();

            double sum = 0;
            while (reader.Read())
            {
                sum += reader.GetDouble(0);
            }
            return sum;
        }

        [Benchmark(Description = "LeichtFrame Vec Scalar (Col * 1.5)")]
        public DoubleColumn LF_Vec_Scalar()
        {
            var col = (DoubleColumn)_lfFrame["Val"];
            return col * 1.5;
        }

        // =========================================================
        // COMPUTED COLUMNS (Transformation)
        // =========================================================

        [Benchmark(Description = "DuckDB Computed (Val * Id)")]
        public void DuckDB_Computed()
        {
            using var cmd = _duckConnection.CreateCommand();
            // SQL handles mixed types (Double * Int) automatically
            cmd.CommandText = "SELECT Val * Id FROM BenchData";
            using var reader = cmd.ExecuteReader();

            while (reader.Read()) { }
        }

        [Benchmark(Description = "LeichtFrame AddColumn (Val * Id)")]
        public DataFrame LF_Computed()
        {
            // Creates a new column using a row-based delegate
            return _lfFrame.AddColumn("Computed", row =>
                row.Get<double>("Val") * row.Get<int>("Id")
            );
        }

        // =========================================================
        // STRING TRANSFORMATION
        // =========================================================

        [Benchmark(Description = "DuckDB String Concat")]
        public void DuckDB_String_Concat()
        {
            using var cmd = _duckConnection.CreateCommand();
            // SQL Concat operator
            cmd.CommandText = "SELECT Category || '_' || UniqueId FROM BenchData";
            using var reader = cmd.ExecuteReader();

            while (reader.Read()) { }
        }

        [Benchmark(Description = "LeichtFrame String Concat")]
        public DataFrame LF_String_Concat()
        {
            // This tests the overhead of string allocation per row
            return _lfFrame.AddColumn("Concat", row =>
                row.Get<string>("Category") + "_" + row.Get<string>("UniqueId")
            );
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/CleaningBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.Transform;

namespace LeichtFrame.Benchmarks
{
    public class CleaningBenchmarks : BenchmarkData
    {
        public override void GlobalSetup()
        {
            base.GlobalSetup();

            // 1. Inject Nulls into LeichtFrame
            // We set every 5th row of 'Category' to null (~20% null rate)
            var catCol = (StringColumn)_lfFrame["Category"];
            for (int i = 0; i < N; i += 5)
            {
                catCol.SetNull(i);
            }

            // 2. Inject Nulls into DuckDB to match the state
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "UPDATE BenchData SET Category = NULL WHERE (Id % 5) = 0";
            cmd.ExecuteNonQuery();
        }

        // =========================================================
        // DISTINCT
        // =========================================================

        [Benchmark(Baseline = true, Description = "DuckDB Distinct")]
        public long DuckDB_Distinct()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT DISTINCT Category FROM BenchData";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LeichtFrame Distinct")]
        public DataFrame LF_Distinct()
        {
            return _lfFrame.Distinct("Category");
        }

        // =========================================================
        // DROP NULLS
        // =========================================================

        [Benchmark(Description = "DuckDB DropNulls (Filter)")]
        public long DuckDB_DropNulls()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT * FROM BenchData WHERE Category IS NOT NULL";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LeichtFrame DropNulls")]
        public DataFrame LF_DropNulls()
        {
            // Removes rows where ANY column is null (checks all columns)
            return _lfFrame.DropNulls();
        }

        // =========================================================
        // FILL NULL (Coalesce)
        // =========================================================

        [Benchmark(Description = "DuckDB FillNull (Coalesce)")]
        public long DuckDB_FillNull()
        {
            using var cmd = _duckConnection.CreateCommand();
            // Simulating creating a new projection with filled values
            cmd.CommandText = "SELECT COALESCE(Category, 'MISSING') FROM BenchData";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LeichtFrame FillNull")]
        public DataFrame LF_FillNull()
        {
            return _lfFrame.FillNull("Category", "MISSING");
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/FilterBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.Filter;


namespace LeichtFrame.Benchmarks
{
    public class FilterBenchmarks : BenchmarkData
    {
        // =========================================================
        // INTEGER FILTER (Numeric)
        // =========================================================

        [Benchmark(Baseline = true, Description = "DuckDB Filter (Int)")]
        public long DuckDB_Filter_Int()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT * FROM BenchData WHERE Id < 50000";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LeichtFrame Where (Int - Delegate)")]
        public DataFrame LF_Where_Int()
        {
            return _lfFrame.Where(row => row.Get<int>("Id") < 50_000);
        }

        [Benchmark(Description = "LeichtFrame WhereView (Zero-Copy)")]
        public DataFrame LF_WhereView_Int()
        {
            return _lfFrame.WhereView(row => row.Get<int>("Id") < 50_000);
        }

        [Benchmark(Description = "LeichtFrame WhereVec (Int - SIMD)")]
        public DataFrame LF_WhereVec_Int()
        {
            // Hardware accelerated filtering
            return _lfFrame.WhereVec("Id", CompareOp.LessThan, 50_000);
        }

        // =========================================================
        // STRING FILTER (Text comparison)
        // =========================================================

        [Benchmark(Description = "DuckDB Filter (String)")]
        public long DuckDB_Filter_String()
        {
            using var cmd = _duckConnection.CreateCommand();
            // Category has values "A", "B", "C", "D", "E"
            cmd.CommandText = "SELECT * FROM BenchData WHERE Category = 'A'";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LeichtFrame Where (String)")]
        public DataFrame LF_Where_String()
        {
            // Vectorized string filtering is not yet implemented, so we use the delegate approach
            return _lfFrame.Where(row => row.Get<string>("Category") == "A");
        }

        // =========================================================
        // COMPOUND FILTER (Multi-Column)
        // =========================================================

        [Benchmark(Description = "DuckDB Filter (Compound)")]
        public long DuckDB_Filter_Compound()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT * FROM BenchData WHERE Id < 10000 AND Category = 'A'";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LeichtFrame Where (Compound)")]
        public DataFrame LF_Where_Compound()
        {
            // Accessing multiple columns per row increases overhead
            return _lfFrame.Where(row =>
                row.Get<int>("Id") < 10_000 &&
                row.Get<string>("Category") == "A"
            );
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/GroupByBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.GroupBy;
using LeichtFrame.Core.Operations.Aggregate;

namespace LeichtFrame.Benchmarks
{
    public class GroupByBenchmarks : BenchmarkData
    {

        // =========================================================
        // COUNT (Low Cardinality INT)
        // =========================================================

        [Benchmark(Description = "DuckDB: GroupBy -> Reader (Stream)")]
        public long DuckDB_Reader()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Id, COUNT(*) FROM BenchData GROUP BY Id";

            using var reader = cmd.ExecuteReader();
            long total = 0;

            while (reader.Read())
            {
                int k = reader.GetInt32(0);
                int c = reader.GetInt32(1);
                total++;
            }
            return total;
        }

        [Benchmark(Description = "DuckDB GroupBy Count (Int)")]
        public long DuckDB_Group_Count_Low()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Id, COUNT(*) FROM BenchData GROUP BY Id";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LF: GroupBy -> GetCountReader (Raw)")]
        public long LF_Stream_Raw()
        {
            using var gdf = _lfFrame.GroupBy("Id");
            var reader = gdf.GetCountReader();

            long total = 0;
            while (reader.Read(out int key, out int count))
            {
                total++;
            }
            return total;
        }

        [Benchmark(Description = "LF: GroupBy -> CountStream (Fluent)")]
        public long LF_Stream_Fluent()
        {
            long total = 0;

            foreach (var (key, count) in _lfFrame.GroupBy("Id").CountStream())
            {
                total++;
            }
            return total;
        }

        [Benchmark(Description = "DuckDB: GroupBy -> List (Materialized)")]
        public List<(int, int)> DuckDB_Materialized()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Id, COUNT(*) FROM BenchData GROUP BY Id";
            using var reader = cmd.ExecuteReader();

            var result = new List<(int, int)>();

            while (reader.Read())
            {
                result.Add((reader.GetInt32(0), reader.GetInt32(1)));
            }

            return result;
        }

        [Benchmark(Description = "LF: GroupBy -> Count (DataFrame)")]
        public DataFrame LF_Materialized()
        {
            return _lfFrame.GroupBy("Id").Count();
        }

        // =========================================================
        // COUNT (Low & High Cardinality STRING)
        // =========================================================

        // [Benchmark(Baseline = true, Description = "DuckDB GroupBy Count (LowCard)")]
        // public long DuckDB_Group_Count_Low_String()
        // {
        //     using var cmd = _duckConnection.CreateCommand();
        //     cmd.CommandText = "SELECT Category, COUNT(*) FROM BenchData GROUP BY Category";
        //     using var reader = cmd.ExecuteReader();

        //     long count = 0;
        //     while (reader.Read()) count++;
        //     return count;
        // }

        // [Benchmark(Description = "LeichtFrame GroupBy Count (LowCard)")]
        // public DataFrame LF_Group_Count_Low_String()
        // {
        //     return _lfFrame.GroupBy("Category").Count();
        // }

        // [Benchmark(Description = "DuckDB GroupBy Count (HighCard)")]
        // public long DuckDB_Group_Count_High()
        // {
        //     using var cmd = _duckConnection.CreateCommand();
        //     cmd.CommandText = "SELECT UniqueId, COUNT(*) FROM BenchData GROUP BY UniqueId";
        //     using var reader = cmd.ExecuteReader();

        //     long count = 0;
        //     while (reader.Read()) count++;
        //     return count;
        // }

        // [Benchmark(Description = "LeichtFrame GroupBy Count (HighCard)")]
        // public DataFrame LF_Group_Count_High()
        // {
        //     return _lfFrame.GroupBy("UniqueId").Count();
        // }

        // =========================================================
        // SUM
        // =========================================================

        // [Benchmark(Description = "DuckDB GroupBy Sum")]
        // public double DuckDB_Group_Sum()
        // {
        //     using var cmd = _duckConnection.CreateCommand();
        //     cmd.CommandText = "SELECT Category, SUM(Val) FROM BenchData GROUP BY Category";
        //     using var reader = cmd.ExecuteReader();

        //     double sumCheck = 0;
        //     while (reader.Read()) sumCheck += reader.GetDouble(1);
        //     return sumCheck;
        // }

        // [Benchmark(Description = "LeichtFrame GroupBy Sum")]
        // public DataFrame LF_Group_Sum()
        // {
        //     return _lfFrame.GroupBy("Category").Sum("Val");
        // }

        // =========================================================
        // MEAN
        // =========================================================

        // [Benchmark(Description = "DuckDB GroupBy Mean")]
        // public double DuckDB_Group_Mean()
        // {
        //     using var cmd = _duckConnection.CreateCommand();
        //     cmd.CommandText = "SELECT Category, AVG(Val) FROM BenchData GROUP BY Category";
        //     using var reader = cmd.ExecuteReader();

        //     double check = 0;
        //     while (reader.Read()) check += reader.GetDouble(1);
        //     return check;
        // }

        // [Benchmark(Description = "LeichtFrame GroupBy Mean")]
        // public DataFrame LF_Group_Mean()
        // {
        //     return _lfFrame.GroupBy("Category").Mean("Val");
        // }

        // =========================================================
        // MIN
        // =========================================================

        // [Benchmark(Description = "DuckDB GroupBy Min")]
        // public double DuckDB_Group_Min()
        // {
        //     using var cmd = _duckConnection.CreateCommand();
        //     cmd.CommandText = "SELECT Category, MIN(Val) FROM BenchData GROUP BY Category";
        //     using var reader = cmd.ExecuteReader();

        //     double check = 0;
        //     while (reader.Read()) check += reader.GetDouble(1);
        //     return check;
        // }

        // [Benchmark(Description = "LeichtFrame GroupBy Min")]
        // public DataFrame LF_Group_Min()
        // {
        //     return _lfFrame.GroupBy("Category").Min("Val");
        // }

        // =========================================================
        // MAX
        // =========================================================

        // [Benchmark(Description = "DuckDB GroupBy Max")]
        // public double DuckDB_Group_Max()
        // {
        //     using var cmd = _duckConnection.CreateCommand();
        //     cmd.CommandText = "SELECT Category, MAX(Val) FROM BenchData GROUP BY Category";
        //     using var reader = cmd.ExecuteReader();

        //     double check = 0;
        //     while (reader.Read()) check += reader.GetDouble(1);
        //     return check;
        // }

        // [Benchmark(Description = "LeichtFrame GroupBy Max")]
        // public DataFrame LF_Group_Max()
        // {
        //     return _lfFrame.GroupBy("Category").Max("Val");
        // }

        // =========================================================
        // STRING GROUPBY
        // =========================================================

        // [Benchmark(Description = "DuckDB GroupBy String")]
        // public long DuckDB_String()
        // {
        //     using var cmd = _duckConnection.CreateCommand();
        //     cmd.CommandText = "SELECT UniqueId, COUNT(*) FROM BenchData GROUP BY UniqueId";
        //     using var reader = cmd.ExecuteReader();

        //     long count = 0;
        //     while (reader.Read()) count++;
        //     return count;
        // }

        // [Benchmark(Description = "LeichtFrame GroupBy String")]
        // public DataFrame LF_String()
        // {
        //     return _lfFrame.GroupBy("UniqueId").Count();
        // }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/GroupByNewBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Order;
using DuckDB.NET.Data;
using LeichtFrame.Core;
using LeichtFrame.Core.Expressions; // Wichtig f√ºr F.Col(), F.Count()

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    [Orderer(SummaryOrderPolicy.FastestToSlowest)]
    [RankColumn]
    public class GroupByNewBenchmarks
    {
        [Params(1_000_000)]
        public int N;

        private DataFrame _lfFrame = null!;
        private DuckDBConnection _duckConnection = null!;

        [GlobalSetup]
        public void Setup()
        {
            Console.WriteLine($"Generating {N} rows of test data...");

            var ids = new int[N];
            var zipCodes = new int[N];
            var categories = new string[N];
            var uuids = new string[N];
            var values = new double[N];

            var rnd = new Random(42);
            var cats = new[] { "Electronics", "Books", "Garden", "Auto", "Food", "Music", "Toys", "Tools", "Pets", "Sports" };

            var cId = new IntColumn("Id", N);
            var cZip = new IntColumn("Zip", N);
            var cCat = new StringColumn("Category", N);
            var cUuid = new StringColumn("UUID", N);
            var cVal = new DoubleColumn("Val", N);

            for (int i = 0; i < N; i++)
            {
                int id = i * 2;
                int zip = rnd.Next(0, 1000);
                string cat = cats[rnd.Next(cats.Length)];
                string uid = Guid.NewGuid().ToString();
                double val = rnd.NextDouble() * 100.0;

                ids[i] = id;
                zipCodes[i] = zip;
                categories[i] = cat;
                uuids[i] = uid;
                values[i] = val;

                cId.Append(id);
                cZip.Append(zip);
                cCat.Append(cat);
                cUuid.Append(uid);
                cVal.Append(val);
            }

            _lfFrame = new DataFrame(new IColumn[] { cId, cZip, cCat, cUuid, cVal });

            _duckConnection = new DuckDBConnection("DataSource=:memory:");
            _duckConnection.Open();

            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "CREATE TABLE BenchData (Id INTEGER, Zip INTEGER, Category VARCHAR, UUID VARCHAR, Val DOUBLE)";
            cmd.ExecuteNonQuery();

            using (var appender = _duckConnection.CreateAppender("BenchData"))
            {
                for (int i = 0; i < N; i++)
                {
                    var row = appender.CreateRow();
                    row.AppendValue(ids[i]);
                    row.AppendValue(zipCodes[i]);
                    row.AppendValue(categories[i]);
                    row.AppendValue(uuids[i]);
                    row.AppendValue(values[i]);
                    row.EndRow();
                }
            }
        }

        [GlobalCleanup]
        public void Cleanup()
        {
            _lfFrame?.Dispose();
            _duckConnection?.Dispose();
        }

        // =========================================================
        // SCENARIO A: DENSE INT (Histogram)
        // =========================================================

        [Benchmark(Description = "DuckDB: Dense Int (Stream)")]
        public long DuckDB_A_Stream()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Zip, COUNT(*) FROM BenchData GROUP BY Zip";
            using var reader = cmd.ExecuteReader();

            long checkSum = 0;
            while (reader.Read())
            {
                // Access Data to measure Marshalling overhead
                int key = reader.GetInt32(0);
                long cnt = reader.GetInt64(1);
                checkSum += key + cnt;
            }
            return checkSum;
        }

        [Benchmark(Description = "LF: Dense Int (Stream)")]
        public long LF_A_Stream()
        {
            var stream = _lfFrame.Lazy()
                                 .GroupBy("Zip")
                                 .Agg(F.Count().As("Cnt"))
                                 .CollectStream();

            long checkSum = 0;
            foreach (var row in stream)
            {
                // Access Data via RowView
                int key = row.Get<int>(0);
                int cnt = row.Get<int>(1);
                checkSum += key + cnt;
            }
            return checkSum;
        }

        [Benchmark(Description = "DuckDB: Dense Int (Materialized)")]
        public List<object[]> DuckDB_A_Mat()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Zip, COUNT(*) FROM BenchData GROUP BY Zip";
            using var reader = cmd.ExecuteReader();

            var result = new List<object[]>();
            while (reader.Read())
            {
                // Force Materialization into managed objects
                result.Add(new object[] { reader.GetValue(0), reader.GetValue(1) });
            }
            return result;
        }

        [Benchmark(Description = "LF: Dense Int (Materialized)")]
        public DataFrame LF_A_Mat()
        {
            return _lfFrame.Lazy()
                           .GroupBy("Zip")
                           .Agg(F.Count().As("Cnt"))
                           .Collect(); // Builds DataFrame
        }

        // =========================================================
        // SCENARIO B: SPARSE INT (SwissMap / Hash)
        // =========================================================

        [Benchmark(Description = "DuckDB: Sparse Int (Stream)")]
        public long DuckDB_B_Stream()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Id, COUNT(*) FROM BenchData GROUP BY Id";
            using var reader = cmd.ExecuteReader();

            long checkSum = 0;
            while (reader.Read())
            {
                int key = reader.GetInt32(0);
                long cnt = reader.GetInt64(1);
                checkSum += key + cnt;
            }
            return checkSum;
        }

        [Benchmark(Description = "LF: Sparse Int (Stream)")]
        public long LF_B_Stream()
        {
            var stream = _lfFrame.Lazy()
                                 .GroupBy("Id")
                                 .Agg(F.Count().As("Cnt"))
                                 .CollectStream();

            long checkSum = 0;
            foreach (var row in stream)
            {
                int key = row.Get<int>(0);
                int cnt = row.Get<int>(1);
                checkSum += key + cnt;
            }
            return checkSum;
        }

        [Benchmark(Description = "DuckDB: Sparse Int (Materialized)")]
        public List<object[]> DuckDB_B_Mat()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Id, COUNT(*) FROM BenchData GROUP BY Id";
            using var reader = cmd.ExecuteReader();

            var result = new List<object[]>();
            while (reader.Read())
            {
                result.Add(new object[] { reader.GetValue(0), reader.GetValue(1) });
            }
            return result;
        }

        [Benchmark(Description = "LF: Sparse Int (Materialized)")]
        public DataFrame LF_B_Mat()
        {
            return _lfFrame.Lazy()
                           .GroupBy("Id")
                           .Agg(F.Count().As("Cnt"))
                           .Collect();
        }

        // =========================================================
        // SCENARIO C: LOW CARDINALITY STRING (Dictionary)
        // =========================================================

        [Benchmark(Description = "DuckDB: LowCard String (Stream)")]
        public double DuckDB_C_Stream()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Category, SUM(Val) FROM BenchData GROUP BY Category";
            using var reader = cmd.ExecuteReader();

            double checkSum = 0;
            while (reader.Read())
            {
                string cat = reader.GetString(0);
                double sum = reader.GetDouble(1);
                checkSum += sum + cat.Length;
            }
            return checkSum;
        }

        [Benchmark(Description = "LF: LowCard String (Stream)")]
        public double LF_C_Stream()
        {
            var stream = _lfFrame.Lazy()
                                 .GroupBy("Category")
                                 .Agg(F.Sum(F.Col("Val")).As("Total"))
                                 .CollectStream();

            double checkSum = 0;
            foreach (var row in stream)
            {
                string cat = row.Get<string>(0);
                double sum = row.Get<double>(1);
                checkSum += sum + cat.Length;
            }
            return checkSum;
        }

        [Benchmark(Description = "DuckDB: LowCard String (Materialized)")]
        public List<object[]> DuckDB_C_Mat()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Category, SUM(Val) FROM BenchData GROUP BY Category";
            using var reader = cmd.ExecuteReader();

            var result = new List<object[]>();
            while (reader.Read())
            {
                result.Add(new object[] { reader.GetValue(0), reader.GetValue(1) });
            }
            return result;
        }

        [Benchmark(Description = "LF: LowCard String (Materialized)")]
        public DataFrame LF_C_Mat()
        {
            return _lfFrame.Lazy()
                           .GroupBy("Category")
                           .Agg(F.Sum(F.Col("Val")).As("Total"))
                           .Collect();
        }

        // =========================================================
        // SCENARIO D: HIGH CARDINALITY STRING (Parallel / SwissMap)
        // =========================================================

        [Benchmark(Description = "DuckDB: HighCard String (Stream)")]
        public long DuckDB_D_Stream()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT UUID, COUNT(*) FROM BenchData GROUP BY UUID";
            using var reader = cmd.ExecuteReader();

            long checkSum = 0;
            while (reader.Read())
            {
                string uuid = reader.GetString(0);
                long cnt = reader.GetInt64(1);
                checkSum += cnt + uuid.Length;
            }
            return checkSum;
        }

        [Benchmark(Description = "LF: HighCard String (Stream)")]
        public long LF_D_Stream()
        {
            var stream = _lfFrame.Lazy()
                                 .GroupBy("UUID")
                                 .Agg(F.Count().As("Cnt"))
                                 .CollectStream();

            long checkSum = 0;
            foreach (var row in stream)
            {
                string uuid = row.Get<string>(0);
                int cnt = row.Get<int>(1);
                checkSum += cnt + uuid.Length;
            }
            return checkSum;
        }

        [Benchmark(Description = "DuckDB: HighCard String (Materialized)")]
        public List<object[]> DuckDB_D_Mat()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT UUID, COUNT(*) FROM BenchData GROUP BY UUID";
            using var reader = cmd.ExecuteReader();

            var result = new List<object[]>();
            while (reader.Read())
            {
                result.Add(new object[] { reader.GetValue(0), reader.GetValue(1) });
            }
            return result;
        }

        [Benchmark(Description = "LF: HighCard String (Materialized)")]
        public DataFrame LF_D_Mat()
        {
            return _lfFrame.Lazy()
                           .GroupBy("UUID")
                           .Agg(F.Count().As("Cnt"))
                           .Collect();
        }

        // =========================================================
        // SCENARIO E: MULTI-COLUMN (Row Packing / Hash)
        // =========================================================

        [Benchmark(Description = "DuckDB: Multi-Col (Stream)")]
        public long DuckDB_E_Stream()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Zip, Id, COUNT(*) FROM BenchData GROUP BY Zip, Id";
            using var reader = cmd.ExecuteReader();

            long checkSum = 0;
            while (reader.Read())
            {
                int zip = reader.GetInt32(0);
                int id = reader.GetInt32(1);
                long cnt = reader.GetInt64(2);
                checkSum += zip + id + cnt;
            }
            return checkSum;
        }

        [Benchmark(Description = "LF: Multi-Col (Stream)")]
        public long LF_E_Stream()
        {
            var stream = _lfFrame.Lazy()
                                 .GroupBy("Zip", "Id")
                                 .Agg(F.Count().As("Cnt"))
                                 .CollectStream();

            long checkSum = 0;
            foreach (var row in stream)
            {
                int zip = row.Get<int>(0);
                int id = row.Get<int>(1);
                int cnt = row.Get<int>(2);
                checkSum += zip + id + cnt;
            }
            return checkSum;
        }

        [Benchmark(Description = "DuckDB: Multi-Col (Materialized)")]
        public List<object[]> DuckDB_E_Mat()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Zip, Id, COUNT(*) FROM BenchData GROUP BY Zip, Id";
            using var reader = cmd.ExecuteReader();

            var result = new List<object[]>();
            while (reader.Read())
            {
                result.Add(new object[] { reader.GetValue(0), reader.GetValue(1), reader.GetValue(2) });
            }
            return result;
        }

        [Benchmark(Description = "LF: Multi-Col (Materialized)")]
        public DataFrame LF_E_Mat()
        {
            return _lfFrame.Lazy()
                           .GroupBy("Zip", "Id")
                           .Agg(F.Count().As("Cnt"))
                           .Collect();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/JoinBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.Join;

namespace LeichtFrame.Benchmarks
{
    public class JoinBenchmarks : BenchmarkData
    {
        private LeichtFrame.Core.DataFrame _lfRight = null!;

        public override void GlobalSetup()
        {
            base.GlobalSetup();

            // ---------------------------------------------------------
            // SETUP LEICHTFRAME RIGHT SIDE
            // ---------------------------------------------------------
            var schemaRight = new DataFrameSchema(new[] {
                new ColumnDefinition("UniqueId", typeof(string)),
                new ColumnDefinition("RightVal", typeof(double))
            });

            // We purposefully create a smaller right dataset (50% of Left)
            // to ensure Left Join produces Nulls and Inner Join filters rows.
            int rightCount = N / 2;
            _lfRight = DataFrame.Create(schemaRight, rightCount);

            var colKey = (StringColumn)_lfRight["UniqueId"];
            var colVal = (DoubleColumn)_lfRight["RightVal"];

            // Insert every 2nd item -> 50% match rate
            for (int i = 0; i < N; i += 2)
            {
                colKey.Append(_pocoList[i].UniqueId);
                colVal.Append(_pocoList[i].Val * 2);
            }

            // ---------------------------------------------------------
            // SETUP DUCKDB RIGHT SIDE ü¶Ü
            // ---------------------------------------------------------
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "CREATE TABLE BenchDataRight (UniqueId VARCHAR, RightVal DOUBLE)";
            cmd.ExecuteNonQuery();

            using (var appender = _duckConnection.CreateAppender("BenchDataRight"))
            {
                for (int i = 0; i < N; i += 2)
                {
                    var row = appender.CreateRow();
                    row.AppendValue(_pocoList[i].UniqueId);
                    row.AppendValue(_pocoList[i].Val * 2);
                    row.EndRow();
                }
            }
        }

        public override void GlobalCleanup()
        {
            _lfRight?.Dispose();
            base.GlobalCleanup();
        }

        // =========================================================
        // INNER JOIN
        // =========================================================

        [Benchmark(Baseline = true, Description = "DuckDB Inner Join")]
        public long DuckDB_InnerJoin()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = @"
                SELECT COUNT(*) 
                FROM BenchData 
                INNER JOIN BenchDataRight ON BenchData.UniqueId = BenchDataRight.UniqueId";

            return (long)cmd.ExecuteScalar()!;
        }

        [Benchmark(Description = "LeichtFrame Inner Join")]
        public DataFrame LF_InnerJoin()
        {
            return _lfFrame.Join(_lfRight, "UniqueId", JoinType.Inner);
        }

        // =========================================================
        // LEFT JOIN
        // =========================================================

        [Benchmark(Description = "DuckDB Left Join")]
        public long DuckDB_LeftJoin()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = @"
                SELECT COUNT(*) 
                FROM BenchData 
                LEFT JOIN BenchDataRight ON BenchData.UniqueId = BenchDataRight.UniqueId";

            return (long)cmd.ExecuteScalar()!;
        }

        [Benchmark(Description = "LeichtFrame Left Join")]
        public DataFrame LF_LeftJoin()
        {
            return _lfFrame.Join(_lfRight, "UniqueId", JoinType.Left);
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/LazyAdvancedBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.Join;
using LeichtFrame.Core.Operations.GroupBy;
using LeichtFrame.Core.Operations.Aggregate;
using DuckDB.NET.Data;
using static LeichtFrame.Core.Expressions.F;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    public class LazyAdvancedBenchmarks : BenchmarkData
    {
        private DataFrame _products = null!;
        private DataFrame _orders = null!;

        public record ProductPoco(int ProductId, string Category);
        public record OrderPoco(int OrderId, int ProductId, double Amount);

        public override void GlobalSetup()
        {
            base.GlobalSetup();

            int productCount = N / 10;
            var prodList = new List<ProductPoco>(productCount);
            for (int i = 0; i < productCount; i++)
            {
                prodList.Add(new ProductPoco(i, $"Cat_{i % 5}"));
            }
            _products = DataFrame.FromObjects(prodList);

            var rnd = new Random(42);
            var orderList = new List<OrderPoco>(N);
            for (int i = 0; i < N; i++)
            {
                orderList.Add(new OrderPoco(i, rnd.Next(0, productCount), _pocoList[i].Val));
            }
            _orders = DataFrame.FromObjects(orderList);

            using var cmd = _duckConnection.CreateCommand();

            cmd.CommandText = "CREATE TABLE Products (ProductId INTEGER, Category VARCHAR)";
            cmd.ExecuteNonQuery();

            cmd.CommandText = "CREATE TABLE Orders (OrderId INTEGER, ProductId INTEGER, Amount DOUBLE)";
            cmd.ExecuteNonQuery();

            using (var appender = _duckConnection.CreateAppender("Products"))
            {
                foreach (var p in prodList)
                {
                    var row = appender.CreateRow();
                    row.AppendValue(p.ProductId);
                    row.AppendValue(p.Category);
                    row.EndRow();
                }
            }

            using (var appender = _duckConnection.CreateAppender("Orders"))
            {
                foreach (var o in orderList)
                {
                    var row = appender.CreateRow();
                    row.AppendValue(o.OrderId);
                    row.AppendValue(o.ProductId);
                    row.AppendValue(o.Amount);
                    row.EndRow();
                }
            }
        }

        [Benchmark(Baseline = true, Description = "DuckDB SQL")]
        public void DuckDB_Complex()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = @"
                SELECT p.Category, SUM(o.Amount) as Total, COUNT(*) as Cnt
                FROM Orders o
                JOIN Products p ON o.ProductId = p.ProductId
                WHERE o.Amount > 500.0
                GROUP BY p.Category
                ORDER BY Total DESC";

            using var reader = cmd.ExecuteReader();
            while (reader.Read()) { }
        }

        [Benchmark(Description = "LF Eager (Classic)")]
        public DataFrame LF_Eager()
        {
            var filtered = _orders.Where(r => r.Get<double>("Amount") > 500.0);

            var joined = filtered.Join(_products, "ProductId", JoinType.Inner);

            using var grouped = joined.GroupBy("Category");

            var agg = grouped.Aggregate(
                Agg.Sum("Amount", "Total"),
                Agg.Count("Cnt")
            );

            return agg.OrderByDescending("Total");
        }

        [Benchmark(Description = "LF Lazy (New Engine)")]
        public DataFrame LF_Lazy()
        {
            return _orders.Lazy()
                .Where(Col("Amount") > 500.0)
                .Join(_products.Lazy(), "ProductId")
                .GroupBy("Category")
                .Agg(
                    Sum(Col("Amount")).As("Total"),
                    Count().As("Cnt")
                )
                .OrderByDescending("Total")
                .Collect();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/LazyVsEagerBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.Transform;
using LeichtFrame.Core.Operations.Filter;
using static LeichtFrame.Core.Expressions.F;

namespace LeichtFrame.Benchmarks
{
    public class LazyVsEagerBenchmarks : BenchmarkData
    {
        [Benchmark(Baseline = true, Description = "DuckDB (SQL)")]
        public long DuckDB_Query()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT (Val * 2.0) FROM BenchData WHERE Val > 500.0";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LF Eager (Delegate)")]
        public DataFrame LF_Eager_Delegate()
        {
            var filtered = _lfFrame.Where(r => r.Get<double>("Val") > 500.0);

            var result = filtered.AddColumn("Result", r => r.Get<double>("Val") * 2.0);

            return result.Select("Result");
        }

        [Benchmark(Description = "LF Lazy (Expression)")]
        public DataFrame LF_Lazy_Expr()
        {
            return _lfFrame.Lazy()
                .Where(Col("Val") > 500.0)
                .Select(
                    (Col("Val") * 2.0).As("Result")
                )
                .Collect();
        }

        [Benchmark(Description = "LF Manual (Hardcoded SIMD)")]
        public DataFrame LF_Manual_Hardcoded()
        {
            var filtered = _lfFrame.WhereVec("Val", CompareOp.GreaterThan, 500.0);

            var srcCol = (DoubleColumn)filtered["Val"];
            var calcCol = srcCol * 2.0;

            var finalCol = new DoubleColumn("Result", calcCol.Length, calcCol.IsNullable);
            for (int i = 0; i < calcCol.Length; i++)
            {
                if (calcCol.IsNull(i)) finalCol.Append(null);
                else finalCol.Append(calcCol.Get(i));
            }

            return new DataFrame(new[] { finalCol });
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/LazyVsEagerGroupByBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.GroupBy;
using LeichtFrame.Core.Operations.Aggregate;
using static LeichtFrame.Core.Expressions.F;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    public class LazyVsEagerGroupByBenchmarks : BenchmarkData
    {
        [Benchmark(Baseline = true, Description = "DuckDB GroupBy")]
        public long DuckDB_Group()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Id, COUNT(*) FROM BenchData GROUP BY Id";

            using var reader = cmd.ExecuteReader();
            long count = 0;
            while (reader.Read()) count++;
            return count;
        }

        [Benchmark(Description = "LF Eager (Direct API)")]
        public DataFrame LF_Eager()
        {
            return _lfFrame.GroupBy("Id").Count();
        }

        [Benchmark(Description = "LF Lazy (Expression API)")]
        public DataFrame LF_Lazy()
        {
            // FIX: Updated to use .Agg() syntax
            return _lfFrame.Lazy()
                           .GroupBy("Id")
                           .Agg(Count().As("Count"))
                           .Collect();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/MultiColumnGroupByBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using static LeichtFrame.Core.Expressions.F;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    public class MultiColumnGroupByBenchmarks : BenchmarkData
    {
        [Benchmark(Baseline = true, Description = "DuckDB GroupBy (2 Cols)")]
        public void DuckDB_Group2()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT Category, (Id % 100) as SubGroup, COUNT(*) FROM BenchData GROUP BY Category, (Id % 100)";
            using var reader = cmd.ExecuteReader();
            while (reader.Read()) { }
        }

        [Benchmark(Description = "LF Lazy GroupBy (2 Cols)")]
        public DataFrame LF_Group2()
        {
            return _lfFrame.Lazy()
                .Aggregate(
                    new[] { Col("Category"), Col("Id") },
                    new[] { Count().As("Cnt") }
                )
                .Collect();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/Core/Sorting Benchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.Core.Operations.Sort;

namespace LeichtFrame.Benchmarks
{
    public class SortingBenchmarks : BenchmarkData
    {
        // =========================================================
        // FULL SORTING (OrderBy)
        // =========================================================

        [Benchmark(Baseline = true, Description = "DuckDB Sort (Int)")]
        public void DuckDB_Sort_Int()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT * FROM BenchData ORDER BY Id";
            using var reader = cmd.ExecuteReader();

            // Iterate to ensure sorting is actually executed and materialized
            while (reader.Read()) { }
        }

        [Benchmark(Description = "LeichtFrame Sort (Int)")]
        public DataFrame LF_Sort_Int()
        {
            return _lfFrame.OrderBy("Id");
        }

        [Benchmark(Description = "DuckDB Sort (String)")]
        public void DuckDB_Sort_String()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT * FROM BenchData ORDER BY UniqueId";
            using var reader = cmd.ExecuteReader();

            while (reader.Read()) { }
        }

        [Benchmark(Description = "LeichtFrame Sort (String)")]
        public DataFrame LF_Sort_String()
        {
            return _lfFrame.OrderBy("UniqueId");
        }

        // =========================================================
        // TOP-N (Smallest/Largest vs LIMIT)
        // =========================================================

        [Benchmark(Description = "DuckDB Top 10 (Int)")]
        public void DuckDB_TopN_Int()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT * FROM BenchData ORDER BY Id ASC LIMIT 10";
            using var reader = cmd.ExecuteReader();

            while (reader.Read()) { }
        }

        [Benchmark(Description = "LeichtFrame Top 10 (Int)")]
        public DataFrame LF_TopN_Int()
        {
            // Uses optimized PriorityQueue implementation
            return _lfFrame.Smallest(10, "Id");
        }

        [Benchmark(Description = "DuckDB Top 10 (String)")]
        public void DuckDB_TopN_String()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "SELECT * FROM BenchData ORDER BY UniqueId DESC LIMIT 10";
            using var reader = cmd.ExecuteReader();

            while (reader.Read()) { }
        }

        [Benchmark(Description = "LeichtFrame Top 10 (String)")]
        public DataFrame LF_TopN_String()
        {
            return _lfFrame.Largest(10, "UniqueId");
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/IO/IOBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Order;
using DuckDB.NET.Data;
using LeichtFrame.Core;
using LeichtFrame.IO;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    [Orderer(SummaryOrderPolicy.FastestToSlowest)]
    [RankColumn]
    public class IOBenchmarks
    {
        [Params(100_000)]
        public int N;

        private string _tempDir = null!;
        private string _sourceCsvPath = null!;
        private string _sourceParquetPath = null!;
        private string _destCsvPath = null!;
        private string _destParquetPath = null!;

        private DataFrame _lfDataFrame = null!;
        private DuckDBConnection _duckConnection = null!;
        private DataFrameSchema _schema = null!;

        [GlobalSetup]
        public void Setup()
        {
            // 1. Prepare Temp Directory
            _tempDir = Path.Combine(Path.GetTempPath(), "LeichtFrame_IO_Bench");
            if (!Directory.Exists(_tempDir)) Directory.CreateDirectory(_tempDir);

            _sourceCsvPath = Path.Combine(_tempDir, "source.csv");
            _sourceParquetPath = Path.Combine(_tempDir, "source.parquet");
            _destCsvPath = Path.Combine(_tempDir, "dest.csv");
            _destParquetPath = Path.Combine(_tempDir, "dest.parquet");

            // 2. Generate Test Data
            var data = new List<BenchmarkData.TestPoco>(N);
            var rnd = new Random(42);
            var categories = new[] { "A", "B", "C", "D", "E" };

            for (int i = 0; i < N; i++)
            {
                data.Add(new BenchmarkData.TestPoco(
                    i,
                    rnd.NextDouble() * 1000.0,
                    categories[rnd.Next(categories.Length)],
                    Guid.NewGuid().ToString()
                ));
            }

            _lfDataFrame = DataFrame.FromObjects(data);
            _schema = _lfDataFrame.Schema;

            // 3. Write Source Files on disk
            _lfDataFrame.WriteCsv(_sourceCsvPath, new CsvWriteOptions { WriteHeader = true });
            _lfDataFrame.WriteParquet(_sourceParquetPath);

            // 4. DuckDB Setup
            _duckConnection = new DuckDBConnection("DataSource=:memory:");
            _duckConnection.Open();

            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = "CREATE TABLE BenchData (Id INTEGER, Val DOUBLE, Category VARCHAR, UniqueId VARCHAR)";
            cmd.ExecuteNonQuery();

            using (var appender = _duckConnection.CreateAppender("BenchData"))
            {
                foreach (var item in data)
                {
                    var row = appender.CreateRow();
                    row.AppendValue(item.Id);
                    row.AppendValue(item.Val);
                    row.AppendValue(item.Category);
                    row.AppendValue(item.UniqueId);
                    row.EndRow();
                }
            }
        }

        [GlobalCleanup]
        public void Cleanup()
        {
            _lfDataFrame?.Dispose();
            _duckConnection?.Dispose();

            if (Directory.Exists(_tempDir)) Directory.Delete(_tempDir, true);
        }

        // =========================================================
        // CSV READ
        // =========================================================

        [Benchmark(Description = "DuckDB Read CSV")]
        public long DuckDB_Read_CSV()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = $"SELECT * FROM read_csv_auto('{_sourceCsvPath}')";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read())
            {
                count++;
                var id = reader.GetValue(0);
            }
            return count;
        }

        [Benchmark(Description = "LeichtFrame Read CSV")]
        public DataFrame LF_Read_CSV()
        {
            return CsvReader.Read(_sourceCsvPath, _schema, new CsvReadOptions { HasHeader = true });
        }

        // =========================================================
        // PARQUET READ
        // =========================================================

        [Benchmark(Description = "DuckDB Read Parquet")]
        public long DuckDB_Read_Parquet()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = $"SELECT * FROM read_parquet('{_sourceParquetPath}')";
            using var reader = cmd.ExecuteReader();

            long count = 0;
            while (reader.Read())
            {
                count++;
                var id = reader.GetValue(0);
            }
            return count;
        }

        [Benchmark(Description = "LeichtFrame Read Parquet")]
        public DataFrame LF_Read_Parquet()
        {
            // Nutzt intern Parquet.Net
            return ParquetReader.Read(_sourceParquetPath);
        }

        // =========================================================
        // CSV WRITE
        // =========================================================

        [Benchmark(Description = "DuckDB Write CSV")]
        public void DuckDB_Write_CSV()
        {
            using var cmd = _duckConnection.CreateCommand();
            // COPY (SELECT ...) TO ...
            cmd.CommandText = $"COPY BenchData TO '{_destCsvPath}' (FORMAT CSV, HEADER)";
            cmd.ExecuteNonQuery();
        }

        [Benchmark(Description = "LeichtFrame Write CSV")]
        public void LF_Write_CSV()
        {
            _lfDataFrame.WriteCsv(_destCsvPath);
        }

        // =========================================================
        // PARQUET WRITE
        // =========================================================

        [Benchmark(Description = "DuckDB Write Parquet")]
        public void DuckDB_Write_Parquet()
        {
            using var cmd = _duckConnection.CreateCommand();
            cmd.CommandText = $"COPY BenchData TO '{_destParquetPath}' (FORMAT PARQUET)";
            cmd.ExecuteNonQuery();
        }

        [Benchmark(Description = "LeichtFrame Write Parquet")]
        public void LF_Write_Parquet()
        {
            _lfDataFrame.WriteParquet(_destParquetPath);
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/IO/InteropBenchmarks.cs =====
using Apache.Arrow;
using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Order;
using LeichtFrame.Core;
using LeichtFrame.IO;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    [Orderer(SummaryOrderPolicy.FastestToSlowest)]
    public class InteropBenchmarks : BenchmarkData
    {
        private RecordBatch _arrowBatch = null!;

        public override void GlobalSetup()
        {
            base.GlobalSetup();
            // Pre-calculate an Arrow Batch to measure Import speed
            _arrowBatch = _lfFrame.ToArrow();
        }

        // =========================================================
        // EXPORT TO ARROW
        // =========================================================

        [Benchmark(Description = "LeichtFrame ToArrow (Export)")]
        public RecordBatch LF_ToArrow()
        {
            // Measures how fast we can map internal columns to Arrow arrays
            return _lfFrame.ToArrow();
        }

        // =========================================================
        // IMPORT FROM ARROW
        // =========================================================

        [Benchmark(Description = "LeichtFrame FromArrow (Import)")]
        public DataFrame LF_FromArrow()
        {
            // Measures how fast we can ingest Arrow data into LeichtFrame columns
            return _arrowBatch.ToDataFrame();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/LeichtFrame.Benchmarks.csproj =====
Ôªø<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net8.0</TargetFramework>
    <ImplicitUsings>enable</ImplicitUsings>
    <Nullable>enable</Nullable>
    <IsPackable>false</IsPackable>

    <GenerateDocumentationFile>false</GenerateDocumentationFile>
    <NoWarn>$(NoWarn);CS1591</NoWarn>
  </PropertyGroup>

  <ItemGroup>
    <PackageReference Include="BenchmarkDotNet" Version="0.15.8" />
    <PackageReference Include="DuckDB.NET.Data.Full" Version="1.4.3" />
    <PackageReference Include="Microsoft.Data.Analysis" Version="0.23.0" />
  </ItemGroup>

  <ItemGroup>
    <ProjectReference Include="..\LeichtFrame.Core\LeichtFrame.Core.csproj" />
    <ProjectReference Include="..\LeichtFrame.IO\LeichtFrame.IO.csproj" />
  </ItemGroup>

</Project>

===== FILE: src/LeichtFrame.Benchmarks/Program.cs =====
using BenchmarkDotNet.Columns;
using BenchmarkDotNet.Configs;
using BenchmarkDotNet.Environments;
using BenchmarkDotNet.Exporters;
using BenchmarkDotNet.Jobs;
using BenchmarkDotNet.Loggers;
using BenchmarkDotNet.Running;
using BenchmarkDotNet.Toolchains.CsProj;

namespace LeichtFrame.Benchmarks
{
    public class Program
    {
        public static void Main(string[] args)
        {
            // --- 1. Custom Arguments Parsing ---           
            bool isFastMode = args.Any(a => a.Equals("fast", StringComparison.OrdinalIgnoreCase) ||
                                            a.Equals("short", StringComparison.OrdinalIgnoreCase));

            // Clean BDN Arguments
            var bdnArgs = args.Where(a => !a.Equals("fast", StringComparison.OrdinalIgnoreCase) &&
                                          !a.Equals("short", StringComparison.OrdinalIgnoreCase)).ToList();

            // "Magic" Argument: "all" -> "--filter *"
            if (bdnArgs.Count > 0 && bdnArgs[0].Equals("all", StringComparison.OrdinalIgnoreCase))
            {
                bdnArgs.Clear();
                bdnArgs.Add("--filter");
                bdnArgs.Add("*");
            }

            // --- 2. Job Configuration ---
            var job = Job.Default
                .WithRuntime(CoreRuntime.Core80)
                .WithPlatform(Platform.X64)
                .WithToolchain(CsProjCoreToolchain.NetCoreApp80);

            if (isFastMode)
            {
                job = job
                    .WithWarmupCount(1)
                    .WithIterationCount(3)
                    .WithLaunchCount(1)
                    .WithInvocationCount(16);
            }
            else
            {
                // STABLE MODE
                job = job
                    .WithLaunchCount(3)
                    .WithWarmupCount(4)
                    .WithIterationCount(10);
            }

            var config = ManualConfig.Create(DefaultConfig.Instance)
                .AddJob(job)
                .AddExporter(MarkdownExporter.GitHub)
                .WithOptions(ConfigOptions.JoinSummary)
                .AddLogger(ConsoleLogger.Default)
                .AddColumnProvider(DefaultColumnProviders.Instance);

            // --- 3. Header ---
            Console.ForegroundColor = ConsoleColor.Cyan;
            Console.WriteLine("=================================================");
            Console.WriteLine("   üöÄ LeichtFrame Benchmark Suite");
            Console.WriteLine("=================================================");
            Console.ResetColor();

            Console.WriteLine($"Mode:    {(isFastMode ? "‚ö° FAST / DEV (Low Precision)" : "üõ°Ô∏è  STABLE (High Precision)")}");
            Console.WriteLine("Target:  Comparison against DuckDB.NET");
            Console.WriteLine("Dataset: 1,000,000 Rows (configured via Params)");
            Console.WriteLine();

            // --- 4. Help-Text ---
            if (bdnArgs.Count == 0)
            {
                Console.ForegroundColor = ConsoleColor.Yellow;
                Console.WriteLine("Usage Examples:");
                Console.WriteLine("  1. Interactive Menu:   dotnet run -c Release");
                Console.WriteLine("  2. Run All (Stable):   dotnet run -c Release -- all");
                Console.WriteLine("  3. Run All (Fast):     dotnet run -c Release -- all fast");
                Console.WriteLine("  4. Filter (Fast):      dotnet run -c Release -- fast --filter \"*GroupBy*\"");
                Console.ResetColor();
                Console.WriteLine();
                Console.WriteLine("Select benchmarks from the list below:");
            }

            // --- 5. Start ---
            BenchmarkSwitcher.FromAssembly(typeof(Program).Assembly).Run(bdnArgs.ToArray(), config);
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/TPCH/GroupByBreakdownBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.IO;
using static LeichtFrame.Core.Expressions.F;
using LeichtFrame.Core.Operations.Transform;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    [Orderer(BenchmarkDotNet.Order.SummaryOrderPolicy.FastestToSlowest)]
    public class GroupByBreakdownBenchmarks
    {
        private DataFrame _filteredDf = null!;
        private readonly DateTime _targetDate = new DateTime(1998, 9, 2);
        private const string FilePath = "/home/dennis/source/repos/dbgen/tpch-dbgen/lineitem.tbl";

        [GlobalSetup]
        public void Setup()
        {
            if (!File.Exists(FilePath)) throw new FileNotFoundException(FilePath);

            var schema = new DataFrameSchema(new[]
            {
                new ColumnDefinition("l_returnflag", typeof(string), SourceIndex: 8),
                new ColumnDefinition("l_linestatus", typeof(string), SourceIndex: 9),
                new ColumnDefinition("l_quantity", typeof(double), SourceIndex: 4),
            });

            using var raw = CsvReader.Read(FilePath, schema, new CsvReadOptions { Separator = "|", HasHeader = false, HasTrailingDelimiter = true });

            _filteredDf = raw.Where(row => true);
        }

        [GlobalCleanup]
        public void Cleanup()
        {
            _filteredDf?.Dispose();
        }

        // =========================================================
        // TEST A: Single Column Grouping
        // =========================================================
        [Benchmark(Description = "A: GroupBy Single Col (Flag)")]
        public void GroupSingle()
        {
            using var res = _filteredDf.Lazy()
                .GroupBy("l_returnflag")
                .Agg(Count().As("Cnt"))
                .Collect();
        }

        // =========================================================
        // TEST B: Multi-Column Grouping
        // =========================================================
        [Benchmark(Description = "B: GroupBy Multi Col (Flag, Status)")]
        public void GroupMulti()
        {
            using var res = _filteredDf.Lazy()
                .GroupBy("l_returnflag", "l_linestatus")
                .Agg(Count().As("Cnt"))
                .Collect();
        }

        // =========================================================
        // TEST C: Manual String Concatenation (Simulation)
        // =========================================================
        [Benchmark(Description = "C: Manual String Concat Grouping")]
        public void GroupManualConcat()
        {
            var withKey = _filteredDf.AddColumn("ManualKey", row =>
                row.Get<string>("l_returnflag") + "_" + row.Get<string>("l_linestatus"));

            using var res = withKey.Lazy()
                .GroupBy("ManualKey")
                .Agg(Count().As("Cnt"))
                .Collect();
        }

        // =========================================================
        // TEST D: Multi-Column with Payload (Full Aggregation)
        // =========================================================
        [Benchmark(Description = "D: GroupBy Multi + Sum(Qty)")]
        public void GroupMultiWithAgg()
        {
            using var res = _filteredDf.Lazy()
                .GroupBy("l_returnflag", "l_linestatus")
                .Agg(
                    Count().As("Cnt"),
                    Sum(Col("l_quantity")).As("SumQty")
                )
                .Collect();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/TPCH/TPCHBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.IO;
using DuckDB.NET.Data;
using static LeichtFrame.Core.Expressions.F;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    public class TPCHBenchmarks
    {
        private DataFrame _lineItemDf = null!;
        private DuckDBConnection _duckDbConnection = null!;
        private readonly DateTime _targetDate = new DateTime(1998, 9, 2);

        private const string FilePath = "/home/dennis/source/repos/dbgen/tpch-dbgen/lineitem.tbl";

        private const string Q1Sql = @"
            SELECT
                l_returnflag,
                l_linestatus,
                sum(l_quantity) as sum_qty,
                sum(l_extendedprice) as sum_base_price,
                sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
                sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
                avg(l_quantity) as avg_qty,
                avg(l_extendedprice) as avg_price,
                avg(l_discount) as avg_disc,
                count(*) as count_order
            FROM
                lineitem
            WHERE
                l_shipdate <= CAST('1998-09-02' AS DATE)
            GROUP BY
                l_returnflag,
                l_linestatus
            ORDER BY
                l_returnflag,
                l_linestatus;";

        [GlobalSetup]
        public void Setup()
        {
            if (!File.Exists(FilePath))
                throw new FileNotFoundException($"File not found: {FilePath}");

            // 1. LeichtFrame Setup
            var schema = new DataFrameSchema(new[]
            {
                new ColumnDefinition("l_quantity", typeof(double), SourceIndex: 4),
                new ColumnDefinition("l_extendedprice", typeof(double), SourceIndex: 5),
                new ColumnDefinition("l_discount", typeof(double), SourceIndex: 6),
                new ColumnDefinition("l_tax", typeof(double), SourceIndex: 7),
                new ColumnDefinition("l_returnflag", typeof(string), SourceIndex: 8),
                new ColumnDefinition("l_linestatus", typeof(string), SourceIndex: 9),
                new ColumnDefinition("l_shipdate", typeof(DateTime), SourceIndex: 10)
            });

            _lineItemDf = CsvReader.Read(FilePath, schema, new CsvReadOptions
            {
                Separator = "|",
                HasHeader = false,
                HasTrailingDelimiter = true
            });

            // 2. DuckDB Setup
            _duckDbConnection = new DuckDBConnection("DataSource=:memory:");
            _duckDbConnection.Open();

            using var cmd = _duckDbConnection.CreateCommand();
            cmd.CommandText = $@"
                CREATE TABLE lineitem AS 
                SELECT * FROM read_csv('{FilePath}', 
                    header=false, 
                    sep='|', 
                    columns={{
                        'l_orderkey': 'INT', 
                        'l_partkey': 'INT', 
                        'l_suppkey': 'INT', 
                        'l_linenumber': 'INT',
                        'l_quantity': 'DOUBLE', 
                        'l_extendedprice': 'DOUBLE', 
                        'l_discount': 'DOUBLE', 
                        'l_tax': 'DOUBLE',
                        'l_returnflag': 'VARCHAR', 
                        'l_linestatus': 'VARCHAR', 
                        'l_shipdate': 'DATE',
                        'l_commitdate': 'DATE', 
                        'l_receiptdate': 'DATE', 
                        'l_shipinstruct': 'VARCHAR', 
                        'l_shipmode': 'VARCHAR', 
                        'l_comment': 'VARCHAR',
                        'dummy_end': 'VARCHAR' 
                    }});";
            cmd.ExecuteNonQuery();

            Console.WriteLine($"Benchmarks ready. Datasets: {_lineItemDf.RowCount:N0}");
        }

        [Benchmark(Description = "LF Q1: Lazy API")]
        public DataFrame RunQ1LeichtFrame()
        {
            return _lineItemDf.Lazy()
                .Where(Col("l_shipdate") <= _targetDate)
                .Select(
                    Col("l_returnflag"),
                    Col("l_linestatus"),
                    Col("l_quantity"),
                    Col("l_extendedprice"),
                    Col("l_discount"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount"))).As("disc_price"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount")) * (1.0 + Col("l_tax"))).As("charge")
                )
                .GroupBy("l_returnflag", "l_linestatus")
                .Agg(
                    Sum(Col("l_quantity")).As("sum_qty"),
                    Sum(Col("l_extendedprice")).As("sum_base_price"),
                    Sum(Col("disc_price")).As("sum_disc_price"),
                    Sum(Col("charge")).As("sum_charge"),
                    Mean(Col("l_quantity")).As("avg_qty"),
                    Mean(Col("l_extendedprice")).As("avg_price"),
                    Mean(Col("l_discount")).As("avg_disc"),
                    Count().As("count_order")
                )
                .OrderBy("l_returnflag", "l_linestatus")
                .Collect();
        }

        [Benchmark(Description = "DuckDB Q1: Pricing Summary (SQL)")]
        public int RunQ1DuckDb()
        {
            using var cmd = _duckDbConnection.CreateCommand();
            cmd.CommandText = Q1Sql;
            using var reader = cmd.ExecuteReader();

            int count = 0;
            while (reader.Read())
            {
                count++;
            }
            return count;
        }

        [GlobalCleanup]
        public void Cleanup()
        {
            _lineItemDf?.Dispose();
            _duckDbConnection?.Dispose();
        }
    }
}
===== FILE: src/LeichtFrame.Benchmarks/TPCH/TPCHBreakdownBenchmarks.cs =====
using BenchmarkDotNet.Attributes;
using LeichtFrame.Core;
using LeichtFrame.IO;
using static LeichtFrame.Core.Expressions.F;

namespace LeichtFrame.Benchmarks
{
    [MemoryDiagnoser]
    [Orderer(BenchmarkDotNet.Order.SummaryOrderPolicy.FastestToSlowest)]
    public class TPCHBreakdownBenchmarks
    {
        private DataFrame _lineItemDf = null!;
        private readonly DateTime _targetDate = new DateTime(1998, 9, 2);
        private const string FilePath = "/home/dennis/source/repos/dbgen/tpch-dbgen/lineitem.tbl";

        [GlobalSetup]
        public void Setup()
        {
            if (!File.Exists(FilePath)) throw new FileNotFoundException(FilePath);

            var schema = new DataFrameSchema(new[]
            {
                new ColumnDefinition("l_quantity", typeof(double), SourceIndex: 4),
                new ColumnDefinition("l_extendedprice", typeof(double), SourceIndex: 5),
                new ColumnDefinition("l_discount", typeof(double), SourceIndex: 6),
                new ColumnDefinition("l_tax", typeof(double), SourceIndex: 7),
                new ColumnDefinition("l_returnflag", typeof(string), SourceIndex: 8),
                new ColumnDefinition("l_linestatus", typeof(string), SourceIndex: 9),
                new ColumnDefinition("l_shipdate", typeof(DateTime), SourceIndex: 10)
            });

            _lineItemDf = CsvReader.Read(FilePath, schema, new CsvReadOptions { Separator = "|", HasHeader = false, HasTrailingDelimiter = true });
        }

        [GlobalCleanup]
        public void Cleanup()
        {
            _lineItemDf?.Dispose();
        }

        // =========================================================
        // STEP 1: Filter
        // Measures: WhereVec (SIMD) + CloneSubset (Raw Byte Copy)
        // =========================================================
        [Benchmark(Description = "1. Filter Only")]
        public void Step1_Filter()
        {
            using var res = _lineItemDf.Lazy()
                .Where(Col("l_shipdate") <= _targetDate)
                .Collect();
        }

        // =========================================================
        // STEP 2: Filter + Projection (Calculation)
        // Measures: Evaluator JIT Kernel Fusion + Result Allocations
        // =========================================================
        [Benchmark(Description = "2. Filter + Calc")]
        public void Step2_Filter_Calc()
        {
            using var res = _lineItemDf.Lazy()
                .Where(Col("l_shipdate") <= _targetDate)
                .Select(
                    Col("l_returnflag"),
                    Col("l_linestatus"),
                    Col("l_quantity"),
                    Col("l_extendedprice"),
                    Col("l_discount"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount"))).As("disc_price"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount")) * (1.0 + Col("l_tax"))).As("charge")
                )
                .Collect();
        }

        // =========================================================
        // STEP 3: Filter + Calc + GroupBy + Agg
        // Measures: MultiColumn Hashing + Aggregation (Sum/Mean)
        // =========================================================
        [Benchmark(Description = "3. Filter + Calc + GroupAgg")]
        public void Step3_Filter_Calc_GroupAgg()
        {
            using var res = _lineItemDf.Lazy()
                .Where(Col("l_shipdate") <= _targetDate)
                .Select(
                    Col("l_returnflag"),
                    Col("l_linestatus"),
                    Col("l_quantity"),
                    Col("l_extendedprice"),
                    Col("l_discount"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount"))).As("disc_price"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount")) * (1.0 + Col("l_tax"))).As("charge")
                )
                .GroupBy("l_returnflag", "l_linestatus")
                .Agg(
                    Sum(Col("l_quantity")).As("sum_qty"),
                    Sum(Col("l_extendedprice")).As("sum_base_price"),
                    Sum(Col("disc_price")).As("sum_disc_price"),
                    Sum(Col("charge")).As("sum_charge"),
                    Mean(Col("l_quantity")).As("avg_qty"),
                    Mean(Col("l_extendedprice")).As("avg_price"),
                    Mean(Col("l_discount")).As("avg_disc"),
                    Count().As("count_order")
                )
                .Collect();
        }

        // =========================================================
        // STEP 4: Full Query (incl. Sort)
        // Measures: Multi-Column Sort on Result
        // =========================================================
        [Benchmark(Description = "4. Full Query (Sort)")]
        public void Step4_Full()
        {
            using var res = _lineItemDf.Lazy()
                .Where(Col("l_shipdate") <= _targetDate)
                .Select(
                    Col("l_returnflag"),
                    Col("l_linestatus"),
                    Col("l_quantity"),
                    Col("l_extendedprice"),
                    Col("l_discount"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount"))).As("disc_price"),
                    (Col("l_extendedprice") * (1.0 - Col("l_discount")) * (1.0 + Col("l_tax"))).As("charge")
                )
                .GroupBy("l_returnflag", "l_linestatus")
                .Agg(
                    Sum(Col("l_quantity")).As("sum_qty"),
                    Sum(Col("l_extendedprice")).As("sum_base_price"),
                    Sum(Col("disc_price")).As("sum_disc_price"),
                    Sum(Col("charge")).As("sum_charge"),
                    Mean(Col("l_quantity")).As("avg_qty"),
                    Mean(Col("l_extendedprice")).As("avg_price"),
                    Mean(Col("l_discount")).As("avg_disc"),
                    Count().As("count_order")
                )
                .OrderBy("l_returnflag", "l_linestatus")
                .Collect();
        }
    }
}
===== FILE: src/LeichtFrame.Core/Columns/BoolColumn.cs =====
using System.Buffers;
using System.Runtime.CompilerServices;
using LeichtFrame.Core.Engine.Memory;

namespace LeichtFrame.Core
{
    /// <summary>
    /// A high-performance column for storing boolean values.
    /// Uses internal bit-packing (1 bit per boolean) to reduce memory usage by 87.5% compared to bool[].
    /// </summary>
    public class BoolColumn : Column<bool>, IDisposable
    {
        private byte[] _data; // 8 bools per byte
        private NullBitmap? _nulls;
        private int _length;

        /// <summary>
        /// Initializes a new instance of the <see cref="BoolColumn"/> class.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        /// <param name="capacity">The initial capacity (number of rows).</param>
        /// <param name="isNullable">Whether the column supports null values.</param>
        public BoolColumn(string name, int capacity = 16, bool isNullable = false)
            : base(name, isNullable)
        {
            _length = 0;
            // Calculate required bytes: (capacity + 7) / 8
            int byteCount = (capacity + 7) >> 3;
            _data = ArrayPool<byte>.Shared.Rent(byteCount);
            Array.Clear(_data, 0, byteCount);

            if (isNullable)
            {
                _nulls = new NullBitmap(capacity);
            }
        }

        /// <inheritdoc />
        public override int Length => _length;

        /// <summary>
        /// Gets the raw values as memory.             
        /// <para>
        /// **Not Supported for BoolColumn:** Because booleans are bit-packed, they cannot be represented as a contiguous <see cref="ReadOnlyMemory{Boolean}"/>.
        /// Use <see cref="Get(int)"/> or specialized bulk operations instead.
        /// </para>
        /// </summary>
        /// <exception cref="NotSupportedException">Always thrown.</exception>
        public override ReadOnlyMemory<bool> Values => throw new NotSupportedException(
            "BoolColumn uses bit-packed storage. Cannot return ReadOnlyMemory<bool>. Use GetValue or dedicated bulk methods.");

        // --- Core Data Access ---

        /// <inheritdoc />
        public override bool Get(int index)
        {
            CheckBounds(index);
            return (_data[index >> 3] & (1 << (index & 7))) != 0;
        }

        /// <inheritdoc />
        public override void SetValue(int index, bool value)
        {
            CheckBounds(index);
            SetBit(index, value);
            _nulls?.SetNotNull(index);
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private void SetBit(int index, bool value)
        {
            int byteIndex = index >> 3;
            int bitMask = 1 << (index & 7);

            if (value)
                _data[byteIndex] |= (byte)bitMask;
            else
                _data[byteIndex] &= (byte)~bitMask;
        }

        /// <inheritdoc />
        public override void Append(bool value)
        {
            EnsureCapacity(_length + 1);
            SetBit(_length, value);
            _nulls?.SetNotNull(_length);
            _length++;
        }

        /// <summary>
        /// Appends a nullable boolean to the column.
        /// </summary>
        /// <param name="value">The value to append, or null.</param>
        public void Append(bool? value)
        {
            EnsureCapacity(_length + 1);
            if (value.HasValue)
            {
                SetBit(_length, value.Value);
                _nulls?.SetNotNull(_length);
            }
            else
            {
                if (_nulls == null) throw new InvalidOperationException("Cannot append null to non-nullable column.");
                SetBit(_length, false);
                _nulls.SetNull(_length);
            }
            _length++;
        }

        // --- Null Handling ---

        /// <inheritdoc />
        public override bool IsNull(int index)
        {
            CheckBounds(index);
            return _nulls != null && _nulls.IsNull(index);
        }

        /// <inheritdoc />
        public override void SetNull(int index)
        {
            CheckBounds(index);
            if (_nulls == null) throw new InvalidOperationException("Cannot set null on non-nullable column.");
            SetBit(index, false);
            _nulls.SetNull(index);
        }

        /// <summary>
        /// Marks the value at the specified index as not null.
        /// </summary>
        public override void SetNotNull(int index)
        {
            CheckBounds(index);
            _nulls?.SetNotNull(index);
        }

        // --- Bulk Operations ---

        /// <summary>
        /// Checks if any value in the column is true (optimized bit-scan).
        /// </summary>
        public bool AnyTrue()
        {
            if (_nulls == null)
            {
                int fullBytes = _length >> 3;
                for (int i = 0; i < fullBytes; i++)
                {
                    if (_data[i] != 0) return true;
                }
                for (int i = fullBytes * 8; i < _length; i++)
                {
                    if (Get(i)) return true;
                }
                return false;
            }
            else
            {
                for (int i = 0; i < _length; i++)
                {
                    if (!IsNull(i) && Get(i)) return true;
                }
                return false;
            }
        }

        /// <summary>
        /// Checks if all values in the column are true (optimized bit-scan).
        /// </summary>
        public bool AllTrue()
        {
            if (_length == 0) return true;

            if (_nulls == null)
            {
                int fullBytes = _length >> 3;
                for (int i = 0; i < fullBytes; i++)
                {
                    if (_data[i] != 0xFF) return false;
                }
                for (int i = fullBytes * 8; i < _length; i++)
                {
                    if (!Get(i)) return false;
                }
                return true;
            }
            else
            {
                for (int i = 0; i < _length; i++)
                {
                    if (!IsNull(i) && !Get(i)) return false;
                }
                return true;
            }
        }

        // --- Memory ---

        /// <inheritdoc />
        public override void EnsureCapacity(int minCapacity)
        {
            int currentByteCap = _data.Length;
            int requiredByteCap = (minCapacity + 7) >> 3;

            if (currentByteCap >= requiredByteCap) return;

            int newByteCap = Math.Max(currentByteCap * 2, requiredByteCap);

            var newBuffer = ArrayPool<byte>.Shared.Rent(newByteCap);

            Array.Copy(_data, newBuffer, (Length + 7) >> 3);
            Array.Clear(newBuffer, (Length + 7) >> 3, newByteCap - ((Length + 7) >> 3));

            ArrayPool<byte>.Shared.Return(_data);
            _data = newBuffer;

            _nulls?.Resize(minCapacity);
        }

        private void CheckBounds(int index)
        {
            if ((uint)index >= (uint)_length) throw new IndexOutOfRangeException();
        }

        /// <inheritdoc />
        public override IColumn CloneSubset(IReadOnlyList<int> indices)
        {
            var newCol = new BoolColumn(Name, indices.Count, IsNullable);

            for (int i = 0; i < indices.Count; i++)
            {
                int sourceIndex = indices[i];
                if (IsNullable && IsNull(sourceIndex))
                {
                    newCol.Append(null);
                }
                else
                {
                    newCol.Append(Get(sourceIndex));
                }
            }
            return newCol;
        }

        /// <inheritdoc />
        public void Dispose()
        {
            if (_data != null)
            {
                ArrayPool<byte>.Shared.Return(_data);
                _data = null!;
            }
            _nulls?.Dispose();
            _nulls = null;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Columns/Column.cs =====
Ôªønamespace LeichtFrame.Core;

/// <summary>
/// Non-generic base class for all columns. 
/// Allows storing columns of different types in a single collection.
/// </summary>
public abstract class Column : IColumn
{
    /// <inheritdoc />
    public string Name { get; }

    /// <inheritdoc />
    public Type DataType { get; }

    /// <inheritdoc />
    public bool IsNullable { get; }

    /// <summary>
    /// Initializes a new instance of the <see cref="Column"/> class.
    /// </summary>
    /// <param name="name">The unique name of the column.</param>
    /// <param name="dataType">The CLR type of the data stored.</param>
    /// <param name="isNullable">Whether the column allows null values.</param>
    /// <exception cref="ArgumentException">Thrown if name is null or whitespace.</exception>
    /// <exception cref="ArgumentNullException">Thrown if dataType is null.</exception>
    protected Column(string name, Type dataType, bool isNullable)
    {
        if (string.IsNullOrWhiteSpace(name))
            throw new ArgumentException("Column name cannot be null or empty.", nameof(name));

        Name = name;
        DataType = dataType ?? throw new ArgumentNullException(nameof(dataType));
        IsNullable = isNullable;
    }

    /// <inheritdoc />
    public abstract int Length { get; }

    /// <inheritdoc />
    public abstract void EnsureCapacity(int capacity);

    /// <inheritdoc />
    public abstract object? GetValue(int index);

    /// <inheritdoc />
    public abstract void AppendObject(object? value);

    /// <inheritdoc />
    public abstract IColumn CloneSubset(IReadOnlyList<int> indices);

    /// <inheritdoc />
    public abstract bool IsNull(int index);

    /// <inheritdoc />
    public abstract void SetNull(int index);

    /// <summary>
    /// Creates a shallow copy of the column (shares the data buffers).
    /// Used by the Engine for efficient renaming.
    /// </summary>
    internal Column ShallowClone()
    {
        return (Column)this.MemberwiseClone();
    }

    // =======================================================================
    // DEFAULT AGGREGATION IMPLEMENTATION (Fallback)
    // =======================================================================

    /// <inheritdoc />
    public virtual object? ComputeSum(int[] indices, int start, int end)
    {
        // Standard: Nicht unterst√ºtzt (z.B. f√ºr StringColumn oder SlicedColumn ohne Optimierung)
        throw new NotSupportedException($"Sum aggregation is not supported or optimized for column type '{GetType().Name}'.");
    }

    /// <inheritdoc />
    public virtual object? ComputeMean(int[] indices, int start, int end)
    {
        throw new NotSupportedException($"Mean aggregation is not supported or optimized for column type '{GetType().Name}'.");
    }

    /// <inheritdoc />
    public virtual object? ComputeMin(int[] indices, int start, int end)
    {
        // Min/Max k√∂nnten theoretisch generisch implementiert werden, 
        // aber f√ºr Performance verlassen wir uns auf die konkreten Klassen.
        throw new NotSupportedException($"Min aggregation is not supported or optimized for column type '{GetType().Name}'.");
    }

    /// <inheritdoc />
    public virtual object? ComputeMax(int[] indices, int start, int end)
    {
        throw new NotSupportedException($"Max aggregation is not supported or optimized for column type '{GetType().Name}'.");
    }
}
===== FILE: src/LeichtFrame.Core/Columns/ColumnFactory.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// Factory class to create concrete column instances based on runtime types.
    /// Acts as the central registry for supported column types.
    /// </summary>
    public static class ColumnFactory
    {
        /// <summary>
        /// Creates a concrete column instance (e.g. <see cref="IntColumn"/>) based on the provided CLR type.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        /// <param name="type">The data type (e.g. typeof(int)). Supported: int, double, bool, string, DateTime.</param>
        /// <param name="capacity">The initial capacity (number of rows) to allocate.</param>
        /// <param name="isNullable">Whether the column should support null values.</param>
        /// <returns>An <see cref="IColumn"/> instance containing the specific implementation.</returns>
        /// <exception cref="NotSupportedException">Thrown if the provided type is not supported by LeichtFrame.</exception>
        public static IColumn Create(string name, Type type, int capacity = 16, bool isNullable = false)
        {
            // WICHTIG: Nullable Typen auspacken (z.B. int? -> int)
            Type underlyingType = Nullable.GetUnderlyingType(type) ?? type;

            if (underlyingType == typeof(int))
                return new IntColumn(name, capacity, isNullable);

            if (underlyingType == typeof(double))
                return new DoubleColumn(name, capacity, isNullable);

            if (underlyingType == typeof(bool))
                return new BoolColumn(name, capacity, isNullable);

            if (underlyingType == typeof(string))
                return new StringColumn(name, capacity, isNullable);

            if (underlyingType == typeof(DateTime))
                return new DateTimeColumn(name, capacity, isNullable);

            throw new NotSupportedException($"Type {type.Name} is not supported yet.");
        }

        /// <summary>
        /// Generic convenience overload to create a strongly-typed column.
        /// </summary>
        /// <typeparam name="T">The data type of the column.</typeparam>
        /// <param name="name">The name of the column.</param>
        /// <param name="capacity">The initial capacity to allocate.</param>
        /// <param name="isNullable">Whether the column should support null values.</param>
        /// <returns>A typed <see cref="IColumn{T}"/> instance.</returns>
        public static IColumn<T> Create<T>(string name, int capacity = 16, bool isNullable = false)
        {
            return (IColumn<T>)Create(name, typeof(T), capacity, isNullable);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Columns/ColumnT.cs =====
using System.Globalization;

namespace LeichtFrame.Core;

/// <summary>
/// Typed base class for columns storing specific data types (int, double, string, etc.).
/// </summary>
/// <typeparam name="T">The type of data stored in this column.</typeparam>
public abstract class Column<T> : Column, IColumn<T>
{
    /// <summary>
    /// Initializes a new instance of the <see cref="Column{T}"/> class.
    /// </summary>
    /// <param name="name">The name of the column.</param>
    /// <param name="isNullable">Whether the column supports null values.</param>
    protected Column(string name, bool isNullable = false) : base(name, typeof(T), isNullable)
    {
    }

    /// <summary>
    /// Gets the underlying memory storage of the column.
    /// </summary>
    public abstract ReadOnlyMemory<T> Values { get; }

    /// <summary>
    /// Gets the strongly-typed value at the specified index.
    /// </summary>
    /// <param name="index">The zero-based row index.</param>
    /// <returns>The value of type T.</returns>
    public abstract T Get(int index);

    /// <inheritdoc />
    public abstract void SetValue(int index, T value);

    // --- Interface Implementations ---

    /// <exclude />
    T IColumn<T>.GetValue(int index) => Get(index);

    /// <inheritdoc />
    public override object? GetValue(int index)
    {
        if (IsNullable && IsNull(index)) return null;
        return Get(index);
    }

    // --- Appending ---

    /// <inheritdoc />
    public abstract void Append(T value);

    // WICHTIG: Hier muss 'override' stehen, da es in 'Column' abstract ist.
    /// <inheritdoc />
    public override void AppendObject(object? value)
    {
        if (value is T typedVal)
        {
            Append(typedVal);
        }
        else if (value is null)
        {
            if (!IsNullable)
                throw new ArgumentException($"Cannot append null to non-nullable column '{Name}'.");

            Append(default!);
            SetNull(Length - 1);
        }
        else
        {
            try
            {
                var converted = (T)Convert.ChangeType(value, typeof(T), CultureInfo.InvariantCulture);
                Append(converted);
            }
            catch
            {
                throw new ArgumentException($"Cannot convert '{value}' to {typeof(T).Name}");
            }
        }
    }

    // --- Null Handling ---

    /// <inheritdoc />
    public abstract override bool IsNull(int index);

    /// <inheritdoc />
    public abstract override void SetNull(int index);

    /// <summary>
    /// Marks the value at the specified index as not null.
    /// </summary>
    /// <param name="index">The zero-based row index.</param>
    public abstract void SetNotNull(int index);

    // --- Slicing ---

    /// <inheritdoc />
    public virtual ReadOnlyMemory<T> Slice(int start, int length)
    {
        if ((uint)start > (uint)Length || (uint)length > (uint)(Length - start))
        {
            throw new ArgumentOutOfRangeException(nameof(start),
                $"Slice range {start}..{start + length} is out of bounds (Length: {Length}).");
        }

        return Values.Slice(start, length);
    }

    /// <inheritdoc />
    public virtual ReadOnlySpan<T> AsSpan() => Values.Span;
}
===== FILE: src/LeichtFrame.Core/Columns/DateTimeColumn.cs =====
using System.Buffers;
using LeichtFrame.Core.Engine.Memory;

namespace LeichtFrame.Core
{
    /// <summary>
    /// A high-performance column for storing <see cref="DateTime"/> values.
    /// Uses pooled arrays for zero-allocation data management.
    /// </summary>
    public class DateTimeColumn : Column<DateTime>, IDisposable
    {
        private DateTime[] _data;
        private NullBitmap? _nulls;
        private int _length;

        /// <summary>
        /// Initializes a new instance of the <see cref="DateTimeColumn"/> class.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        /// <param name="capacity">The initial capacity (number of rows).</param>
        /// <param name="isNullable">Whether the column supports null values.</param>
        public DateTimeColumn(string name, int capacity = 16, bool isNullable = false)
            : base(name, isNullable)
        {
            _length = 0;
            _data = ArrayPool<DateTime>.Shared.Rent(capacity);

            if (isNullable)
            {
                _nulls = new NullBitmap(capacity);
            }
        }

        /// <inheritdoc />
        public override int Length => _length;

        /// <inheritdoc />
        public override ReadOnlyMemory<DateTime> Values => new ReadOnlyMemory<DateTime>(_data, 0, _length);

        // --- Core Access ---

        /// <inheritdoc />
        public override DateTime Get(int index)
        {
            CheckBounds(index);
            return _data[index];
        }

        /// <inheritdoc />
        public override void SetValue(int index, DateTime value)
        {
            CheckBounds(index);
            _data[index] = value;
            _nulls?.SetNotNull(index);
        }

        /// <inheritdoc />
        public override void Append(DateTime value)
        {
            EnsureCapacity(_length + 1);
            _data[_length] = value;
            _nulls?.SetNotNull(_length);
            _length++;
        }

        /// <summary>
        /// Appends a nullable DateTime value.
        /// </summary>
        /// <param name="value">The value to append, or null.</param>
        /// <exception cref="InvalidOperationException">Thrown if null is passed to a non-nullable column.</exception>
        public void Append(DateTime? value)
        {
            EnsureCapacity(_length + 1);
            if (value.HasValue)
            {
                _data[_length] = value.Value;
                _nulls?.SetNotNull(_length);
            }
            else
            {
                if (_nulls == null)
                    throw new InvalidOperationException("Cannot append null to non-nullable column.");

                _data[_length] = default;
                _nulls.SetNull(_length);
            }
            _length++;
        }

        // --- Null Handling ---

        /// <inheritdoc />
        public override bool IsNull(int index)
        {
            CheckBounds(index);
            return _nulls != null && _nulls.IsNull(index);
        }

        /// <inheritdoc />
        public override void SetNull(int index)
        {
            CheckBounds(index);
            if (_nulls == null)
                throw new InvalidOperationException("Cannot set null on non-nullable column.");

            _data[index] = default;
            _nulls.SetNull(index);
        }

        /// <inheritdoc />
        public override void SetNotNull(int index)
        {
            CheckBounds(index);
            _nulls?.SetNotNull(index);
        }

        // --- Memory Management ---

        /// <inheritdoc />
        public override void EnsureCapacity(int minCapacity)
        {
            if (_data.Length >= minCapacity) return;

            int newCapacity = Math.Max(_data.Length * 2, minCapacity);

            var newBuffer = ArrayPool<DateTime>.Shared.Rent(newCapacity);
            Array.Copy(_data, newBuffer, _length);

            ArrayPool<DateTime>.Shared.Return(_data);
            _data = newBuffer;

            _nulls?.Resize(newCapacity);
        }

        private void CheckBounds(int index)
        {
            if ((uint)index >= (uint)_length)
                throw new IndexOutOfRangeException($"Index {index} is out of range.");
        }

        /// <inheritdoc />
        public override IColumn CloneSubset(IReadOnlyList<int> indices)
        {
            var newCol = new DateTimeColumn(Name, indices.Count, IsNullable);

            for (int i = 0; i < indices.Count; i++)
            {
                int sourceIndex = indices[i];
                if (IsNullable && IsNull(sourceIndex))
                {
                    newCol.Append(null);
                }
                else
                {
                    newCol.Append(Get(sourceIndex));
                }
            }
            return newCol;
        }

        /// <inheritdoc />
        public void Dispose()
        {
            if (_data != null)
            {
                ArrayPool<DateTime>.Shared.Return(_data);
                _data = null!;
            }
            _nulls?.Dispose();
            _nulls = null;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Columns/DoubleColumn.cs =====
using System.Buffers;
using System.Numerics;
using System.Runtime.InteropServices;
using System.Runtime.CompilerServices;
using LeichtFrame.Core.Engine;
using LeichtFrame.Core.Engine.Memory;

namespace LeichtFrame.Core
{
    /// <summary>
    /// A high-performance column for storing <see cref="double"/> values.
    /// Supports optimized statistical operations like Sum, Min, and Max using contiguous memory and SIMD.
    /// </summary>
    public class DoubleColumn : Column<double>, IDisposable
    {
        private double[] _data;
        private NullBitmap? _nulls;
        private int _length;

        /// <summary>
        /// Initializes a new instance of the <see cref="DoubleColumn"/> class.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        /// <param name="capacity">The initial capacity (number of rows).</param>
        /// <param name="isNullable">Whether the column supports null values.</param>
        public DoubleColumn(string name, int capacity = 16, bool isNullable = false)
            : base(name, isNullable)
        {
            _length = 0;
            _data = ArrayPool<double>.Shared.Rent(capacity);
            if (isNullable) _nulls = new NullBitmap(capacity);
        }

        /// <inheritdoc />
        public override int Length => _length;

        /// <inheritdoc />
        public override ReadOnlyMemory<double> Values => new ReadOnlyMemory<double>(_data, 0, _length);

        // --- Core Data Access ---

        /// <inheritdoc />
        public override double Get(int index)
        {
            CheckBounds(index);
            return _data[index];
        }

        /// <inheritdoc />
        public override void SetValue(int index, double value)
        {
            CheckBounds(index);
            _data[index] = value;
            _nulls?.SetNotNull(index);
        }

        /// <inheritdoc />
        public override void Append(double value)
        {
            EnsureCapacity(_length + 1);
            _data[_length] = value;
            _nulls?.SetNotNull(_length);
            _length++;
        }

        /// <summary>
        /// Appends a nullable double value to the column.
        /// </summary>
        /// <param name="value">The value to append, or null.</param>
        /// <exception cref="InvalidOperationException">Thrown if null is passed to a non-nullable column.</exception>
        public void Append(double? value)
        {
            EnsureCapacity(_length + 1);
            if (value.HasValue)
            {
                _data[_length] = value.Value;
                _nulls?.SetNotNull(_length);
            }
            else
            {
                if (_nulls == null) throw new InvalidOperationException("Cannot append null to non-nullable column.");
                _data[_length] = double.NaN;
                _nulls.SetNull(_length);
            }
            _length++;
        }

        // --- Null Handling ---

        /// <inheritdoc />
        public override bool IsNull(int index)
        {
            CheckBounds(index);
            return _nulls != null && _nulls.IsNull(index);
        }

        /// <inheritdoc />
        public override void SetNull(int index)
        {
            CheckBounds(index);
            if (_nulls == null) throw new InvalidOperationException("Cannot set null on non-nullable column.");
            _data[index] = double.NaN;
            _nulls.SetNull(index);
        }

        /// <inheritdoc />
        public override void SetNotNull(int index)
        {
            CheckBounds(index);
            _nulls?.SetNotNull(index);
        }

        // --- Statistical Helpers (SIMD Optimized) ---

        /// <summary>
        /// Calculates the sum of the column. Uses SIMD for non-nullable columns.
        /// </summary>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public double Sum()
        {
            // Optimization: If non-nullable, we can use SIMD and ignore null checks.
            // If nullable, we cannot blindly use SIMD because NaN + Value = NaN.
            if (IsNullable)
            {
                return SumNullable();
            }

            var span = Values.Span;
            double sum = 0;

            if (Vector.IsHardwareAccelerated && span.Length >= Vector<double>.Count)
            {
                var vectors = MemoryMarshal.Cast<double, Vector<double>>(span);
                var accVector = Vector<double>.Zero;

                foreach (var v in vectors)
                {
                    accVector += v;
                }

                sum += Vector.Sum(accVector);

                int processed = vectors.Length * Vector<double>.Count;
                span = span.Slice(processed);
            }

            // Tail loop
            foreach (var val in span)
            {
                sum += val;
            }

            return sum;
        }

        private double SumNullable()
        {
            double sum = 0;
            for (int i = 0; i < _length; i++)
            {
                if (!IsNull(i)) sum += _data[i];
            }
            return sum;
        }

        /// <summary>
        /// Finds the minimum value. Optimized for non-nullable.
        /// </summary>
        public double Min()
        {
            if (_length == 0) return 0;
            if (IsNullable) return MinNullable();

            // Non-Nullable Scalar Optimization (Fastest for Doubles due to NaN checks in SIMD being complex)
            var span = Values.Span;
            double min = double.MaxValue;
            foreach (var val in span)
            {
                if (val < min) min = val;
            }
            return min;
        }

        private double MinNullable()
        {
            double min = double.MaxValue;
            bool hasValue = false;
            for (int i = 0; i < _length; i++)
            {
                if (!IsNull(i))
                {
                    double val = _data[i];
                    if (val < min) min = val;
                    hasValue = true;
                }
            }
            return hasValue ? min : 0;
        }

        /// <summary>
        /// Finds the maximum value. Optimized for non-nullable.
        /// </summary>
        public double Max()
        {
            if (_length == 0) return 0;
            if (IsNullable) return MaxNullable();

            // Non-Nullable Scalar Optimization
            var span = Values.Span;
            double max = double.MinValue;
            foreach (var val in span)
            {
                if (val > max) max = val;
            }
            return max;
        }

        private double MaxNullable()
        {
            double max = double.MinValue;
            bool hasValue = false;
            for (int i = 0; i < _length; i++)
            {
                if (!IsNull(i))
                {
                    double val = _data[i];
                    if (val > max) max = val;
                    hasValue = true;
                }
            }
            return hasValue ? max : 0;
        }

        // --- Memory ---

        /// <inheritdoc />
        public override void EnsureCapacity(int minCapacity)
        {
            if (_data.Length >= minCapacity) return;
            int newCapacity = Math.Max(_data.Length * 2, minCapacity);

            var newBuffer = ArrayPool<double>.Shared.Rent(newCapacity);
            Array.Copy(_data, newBuffer, _length);
            ArrayPool<double>.Shared.Return(_data);
            _data = newBuffer;

            _nulls?.Resize(newCapacity);
        }

        private void CheckBounds(int index)
        {
            if ((uint)index >= (uint)_length) throw new IndexOutOfRangeException();
        }

        /// <inheritdoc />
        public override IColumn CloneSubset(IReadOnlyList<int> indices)
        {
            var newCol = new DoubleColumn(Name, indices.Count, IsNullable);

            for (int i = 0; i < indices.Count; i++)
            {
                int sourceIndex = indices[i];
                if (IsNullable && IsNull(sourceIndex))
                {
                    newCol.Append(null);
                }
                else
                {
                    newCol.Append(Get(sourceIndex));
                }
            }
            return newCol;
        }

        /// <inheritdoc />
        public void Dispose()
        {
            if (_data != null)
            {
                ArrayPool<double>.Shared.Return(_data);
                _data = null!;
            }
            _nulls?.Dispose();
            _nulls = null;
        }

        // --- Arithmetic Operators ---

        /// <summary>
        /// Adds two double columns element-wise.
        /// </summary>
        public static DoubleColumn operator +(DoubleColumn a, DoubleColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Add);

        /// <summary>
        /// Subtracts the second double column from the first element-wise.
        /// </summary>
        public static DoubleColumn operator -(DoubleColumn a, DoubleColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Subtract);

        /// <summary>
        /// Multiplies two double columns element-wise.
        /// </summary>
        public static DoubleColumn operator *(DoubleColumn a, DoubleColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Multiply);

        /// <summary>
        /// Divides the first double column by the second element-wise.
        /// </summary>
        public static DoubleColumn operator /(DoubleColumn a, DoubleColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Divide);

        /// <summary>
        /// Adds a scalar value to every element in the column.
        /// </summary>
        public static DoubleColumn operator +(DoubleColumn a, double b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Add);

        /// <summary>
        /// Subtracts a scalar value from every element in the column.
        /// </summary>
        public static DoubleColumn operator -(DoubleColumn a, double b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Subtract);

        /// <summary>
        /// Multiplies every element in the column by a scalar value.
        /// </summary>
        public static DoubleColumn operator *(DoubleColumn a, double b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Multiply);

        /// <summary>
        /// Divides every element in the column by a scalar value.
        /// </summary>
        public static DoubleColumn operator /(DoubleColumn a, double b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Divide);

        private static DoubleColumn ExecuteOp(DoubleColumn a, DoubleColumn b, VectorizedMathOps.MathOp op)
        {
            if (a.Length != b.Length) throw new ArgumentException("Column lengths mismatch");

            bool resultNullable = a.IsNullable || b.IsNullable;
            var result = new DoubleColumn($"{a.Name}_op_{b.Name}", a.Length, resultNullable);

            result._length = a.Length;
            VectorizedMathOps.Calculate<double>(a._data.AsSpan(0, a.Length), b._data.AsSpan(0, b.Length), result._data.AsSpan(0, a.Length), op);

            if (resultNullable)
            {
                result._nulls?.Dispose();
                result._nulls = NullBitmap.MergeOr(a._nulls, b._nulls, a.Length);
            }
            return result;
        }

        private static DoubleColumn ExecuteOpScalar(DoubleColumn a, double scalar, VectorizedMathOps.MathOp op)
        {
            var result = new DoubleColumn($"{a.Name}_op_scalar", a.Length, a.IsNullable);
            result._length = a.Length;

            VectorizedMathOps.CalculateScalar<double>(a._data.AsSpan(0, a.Length), scalar, result._data.AsSpan(0, a.Length), op);

            if (a.IsNullable && a._nulls != null)
            {
                result._nulls?.Dispose();
                result._nulls = NullBitmap.MergeOr(a._nulls, null, a.Length);
            }
            return result;
        }

        /// <summary>
        /// Computes the sum over a subset of indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        public override object? ComputeSum(int[] indices, int start, int end)
        {
            double sum = 0;
            var data = _data;

            if (!IsNullable)
            {
                for (int i = start; i < end; i++) sum += data[indices[i]];
            }
            else
            {
                var nulls = _nulls!;
                for (int i = start; i < end; i++)
                {
                    int idx = indices[i];
                    if (!nulls.IsNull(idx)) sum += data[idx];
                }
            }
            return sum;
        }

        /// <summary>
        /// Computes the mean over a subset of indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        public override object? ComputeMean(int[] indices, int start, int end)
        {
            int count = end - start;
            if (count == 0) return null;

            // Summe berechnen (wir rufen intern die Logik von oben ab oder kopieren sie f√ºr Speed)
            double sum = (double)(ComputeSum(indices, start, end) ?? 0.0);
            return sum / count;
        }

        /// <summary>
        /// Computes the minimum over a subset of indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        public override object? ComputeMin(int[] indices, int start, int end)
        {
            if (start == end) return null;
            double min = double.MaxValue;
            bool hasVal = false;
            var data = _data;

            if (!IsNullable)
            {
                for (int i = start; i < end; i++)
                {
                    double v = data[indices[i]];
                    if (v < min) min = v;
                }
                hasVal = true;
            }
            else
            {
                var nulls = _nulls!;
                for (int i = start; i < end; i++)
                {
                    int idx = indices[i];
                    if (!nulls.IsNull(idx))
                    {
                        double v = data[idx];
                        if (v < min) min = v;
                        hasVal = true;
                    }
                }
            }
            return hasVal ? min : null;
        }


        /// <summary>
        /// Computes the maximum over a subset of indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        public override object? ComputeMax(int[] indices, int start, int end)
        {
            if (start == end) return null;
            double max = double.MinValue;
            bool hasVal = false;
            var data = _data;

            if (!IsNullable)
            {
                for (int i = start; i < end; i++)
                {
                    double v = data[indices[i]];
                    if (v > max) max = v;
                }
                hasVal = true;
            }
            else
            {
                var nulls = _nulls!;
                for (int i = start; i < end; i++)
                {
                    int idx = indices[i];
                    if (!nulls.IsNull(idx))
                    {
                        double v = data[idx];
                        if (v > max) max = v;
                        hasVal = true;
                    }
                }
            }
            return hasVal ? max : null;
        }

        /// <summary>
        /// Internal Move-Constructor for Kernel Fusion.
        /// Adopts the provided array as internal storage without copying.
        /// </summary>
        internal DoubleColumn(string name, double[] adoptedData, int length)
            : base(name, isNullable: false)
        {
            _data = adoptedData;
            _length = length;
            _nulls = null;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Columns/Extensions/ColumnExtensions.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// Provides helper extension methods for interacting with <see cref="IColumn"/> instances.
    /// </summary>
    public static class ColumnExtensions
    {
        /// <summary>
        /// Helper extension to get a typed value from a generic <see cref="IColumn"/>.
        /// Performs a cast and calls the typed GetValue method if possible.
        /// </summary>
        /// <typeparam name="T">The expected return type.</typeparam>
        /// <param name="column">The column instance.</param>
        /// <param name="index">The row index to retrieve.</param>
        /// <returns>The value of type T.</returns>
        /// <exception cref="InvalidCastException">Thrown if the column type does not match T.</exception>
        public static T Get<T>(this IColumn column, int index)
        {
            // Fast Path: If it is already the correct typed interface
            if (column is IColumn<T> typedCol)
            {
                return typedCol.GetValue(index);
            }

            // Slow Path: Type does not match or is unknown -> Exception or Convert
            throw new InvalidCastException(
                $"Column '{column.Name}' is of type {column.DataType.Name}, but '{typeof(T).Name}' was requested.");
        }

        /// <summary>
        /// Appends an untyped object value to the column. 
        /// Handles type checking and dispatching to the concrete Append method.
        /// </summary>
        /// <param name="column">The target column.</param>
        /// <param name="value">The value to append (can be null if supported).</param>
        /// <exception cref="NotSupportedException">Thrown if the value type is incompatible or the column type is unknown.</exception>
        public static void AppendObject(this IColumn column, object? value)
        {
            if (value == null)
            {
                // We must unfortunately know which concrete types support Append(null).
                // Since IColumn does not have Append (only the concrete classes), we cast.
                if (column is IntColumn ic) ic.Append(null);
                else if (column is DoubleColumn dc) dc.Append(null);
                else if (column is StringColumn sc) sc.Append(null);
                else if (column is BoolColumn bc) bc.Append(null);
                else if (column is DateTimeColumn dtc) dtc.Append(null);
                else throw new NotSupportedException($"Column '{column.Name}' does not support null values or type is unknown.");
                return;
            }

            // Type dispatch
            if (column is IntColumn i) i.Append((int)value);
            else if (column is DoubleColumn d) d.Append(Convert.ToDouble(value)); // Convert allows int->double
            else if (column is StringColumn s) s.Append(value.ToString());
            else if (column is BoolColumn b) b.Append((bool)value);
            else if (column is DateTimeColumn dt) dt.Append((DateTime)value);
            else
                throw new NotSupportedException($"Cannot append object of type {value.GetType().Name} to column {column.GetType().Name}");
        }
    }
}
===== FILE: src/LeichtFrame.Core/Columns/IColumn.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// Represents a generic column in a DataFrame containing metadata and operations.
    /// </summary>
    public interface IColumn
    {
        /// <summary>
        /// Gets the unique name of the column.
        /// </summary>
        string Name { get; }

        /// <summary>
        /// Gets the CLR type of the data stored in this column.
        /// </summary>
        Type DataType { get; }

        /// <summary>
        /// Gets the number of rows in this column.
        /// </summary>
        int Length { get; }

        /// <summary>
        /// Indicates whether the column supports null values.
        /// </summary>
        bool IsNullable { get; }

        /// <summary>
        /// Ensures the column has space for at least the specified number of elements.
        /// If the capacity is increased, the underlying buffer is swapped.           
        /// <para>
        /// **SAFETY WARNING:** Because this library uses array pooling, 
        /// calling this method may return the old buffer to the pool.  
        /// Existing Spans pointing to the old buffer will become invalid.
        /// </para>
        /// </summary>
        /// <param name="capacity">The minimum required capacity.</param>
        void EnsureCapacity(int capacity);

        /// <summary>
        /// Gets the value at the specified index as an object (boxed).
        /// For high performance, use the typed interface <see cref="IColumn{T}"/>.
        /// </summary>
        /// <param name="index">The zero-based row index.</param>
        /// <returns>The value at the index, or null.</returns>
        object? GetValue(int index);

        /// <summary>
        /// Checks if the value at the specified index is null.
        /// </summary>
        bool IsNull(int index);

        /// <summary>
        /// Sets the value at the specified index to null.
        /// </summary>
        void SetNull(int index);

        /// <summary>
        /// Appends an untyped value to the end of the column.
        /// </summary>
        void AppendObject(object? value);

        /// <summary>
        /// Creates a deep copy of the column containing only the rows at the specified indices.
        /// </summary>
        /// <param name="indices">The list of row indices to copy.</param>
        /// <returns>A new column containing the subset of data.</returns>
        IColumn CloneSubset(IReadOnlyList<int> indices);

        /// <summary>
        /// Computes the sum for a subset of rows defined by indices.
        /// </summary>
        /// <param name="indices">The global index array.</param>
        /// <param name="start">Start offset in the index array.</param>
        /// <param name="end">End offset in the index array.</param>
        object? ComputeSum(int[] indices, int start, int end);

        /// <summary>
        /// Computes the mean for a subset of rows defined by indices.
        /// </summary>
        object? ComputeMean(int[] indices, int start, int end);

        /// <summary>
        /// Finds the minimum value for a subset of rows defined by indices.
        /// </summary>
        object? ComputeMin(int[] indices, int start, int end);

        /// <summary>
        /// Finds the maximum value for a subset of rows defined by indices.
        /// </summary>
        object? ComputeMax(int[] indices, int start, int end);
    }

    /// <summary>
    /// Typed interface for high-performance, zero-boxing data access.
    /// </summary>
    /// <typeparam name="T">The type of data stored in the column.</typeparam>
    public interface IColumn<T> : IColumn
    {
        /// <summary>
        /// Gets the strongly-typed value at the specified index.
        /// </summary>
        new T GetValue(int index);

        /// <summary>
        /// Sets the strongly-typed value at the specified index.
        /// </summary>
        void SetValue(int index, T value);


        /// <summary>
        /// Appends a strongly-typed value to the end of the column.
        /// </summary>
        void Append(T value);

        /// <summary>
        /// Returns a zero-copy view of the column data as a Memory region.
        /// </summary>
        ReadOnlyMemory<T> Slice(int start, int length);

        /// <summary>
        /// Returns the underlying data as a ReadOnlySpan for high-performance processing.
        /// </summary>
        ReadOnlySpan<T> AsSpan();
    }
}
===== FILE: src/LeichtFrame.Core/Columns/IndirectColumn.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// A zero-copy view over specific rows of another column.
    /// Uses an index map (indirection array) to point to the original data.
    /// <para>
    /// ‚ö†Ô∏è Limitations: 
    /// 1. Access is slightly slower due to double lookup.
    /// 2. Does NOT support contiguous Span/Memory access (.Values throws).
    /// </para>
    /// </summary>
    /// <typeparam name="T">The type of data stored in the column.</typeparam>
    public class IndirectColumn<T> : IColumn<T>, IDisposable
    {
        private readonly IColumn<T> _source;
        private readonly int[] _indices;

        /// <summary>
        /// Initializes a new instance of the <see cref="IndirectColumn{T}"/> class.
        /// </summary>
        /// <param name="source">The underlying source column.</param>
        /// <param name="indices">The indices map representing the view.</param>
        public IndirectColumn(IColumn<T> source, int[] indices)
        {
            _source = source ?? throw new ArgumentNullException(nameof(source));
            _indices = indices ?? throw new ArgumentNullException(nameof(indices));
        }

        /// <inheritdoc />
        public string Name => _source.Name;

        /// <inheritdoc />
        public Type DataType => _source.DataType;

        /// <inheritdoc />
        public int Length => _indices.Length;

        /// <inheritdoc />
        public bool IsNullable => _source.IsNullable;

        /// <summary>
        /// Not supported for IndirectColumn as data is scattered.
        /// </summary>
        public ReadOnlyMemory<T> Values => throw new NotSupportedException(
            "IndirectColumn does not support contiguous memory access. Materialize this column first.");

        /// <inheritdoc />
        public ReadOnlySpan<T> AsSpan() => throw new NotSupportedException(
            "IndirectColumn does not support Span access.");

        // --- Data Access ---

        /// <summary>
        /// Gets the strongly-typed value at the specified view index.
        /// </summary>
        public T Get(int index)
        {
            int realIndex = _indices[index];
            return _source.GetValue(realIndex);
        }

        // Explicit Interface Implementation to satisfy IColumn<T>
        T IColumn<T>.GetValue(int index) => Get(index);

        /// <inheritdoc />
        public object? GetValue(int index)
        {
            int realIndex = _indices[index];
            return _source.GetValue(realIndex);
        }

        /// <inheritdoc />
        public void SetValue(int index, T value)
        {
            int realIndex = _indices[index];
            _source.SetValue(realIndex, value);
        }

        // --- Null Handling ---

        /// <inheritdoc />
        public bool IsNull(int index)
        {
            int realIndex = _indices[index];
            return _source.IsNull(realIndex);
        }

        /// <inheritdoc />
        public void SetNull(int index)
        {
            int realIndex = _indices[index];
            _source.SetNull(realIndex);
        }

        // --- Mutation (Not Supported) ---

        /// <summary>
        /// Not supported for Indirect View.
        /// </summary>
        public void Append(T value) => throw new NotSupportedException("Cannot append to an Indirect View.");

        /// <summary>
        /// Not supported for Indirect View.
        /// </summary>
        public void AppendObject(object? value) => throw new NotSupportedException("Cannot append to an Indirect View.");

        /// <summary>
        /// Not supported for Indirect View.
        /// </summary>
        public void EnsureCapacity(int capacity) => throw new NotSupportedException("Cannot resize an Indirect View.");

        // --- Slicing & Cloning ---

        /// <inheritdoc />
        public ReadOnlyMemory<T> Slice(int start, int length)
        {
            throw new NotSupportedException("Cannot slice an IndirectColumn safely to Memory.");
        }

        /// <inheritdoc />
        public IColumn CloneSubset(IReadOnlyList<int> indices)
        {
            // Deep Clone: Materialize the subset
            var newCol = ColumnFactory.Create<T>(Name, indices.Count, IsNullable);

            for (int i = 0; i < indices.Count; i++)
            {
                int viewIndex = indices[i];
                if (viewIndex < 0 || viewIndex >= _indices.Length) throw new IndexOutOfRangeException();

                int realIndex = _indices[viewIndex];

                if (IsNullable && _source.IsNull(realIndex))
                {
                    newCol.Append(default!);
                    newCol.SetNull(i);
                }
                else
                {
                    newCol.Append(_source.GetValue(realIndex));
                }
            }
            return newCol;
        }

        /// <inheritdoc />
        public void Dispose()
        {
            // We do not own the source, so we do NOT dispose it.
        }

        // --- Aggregation Interface Implementation (Not Supported yet) ---

        /// <summary>
        /// Not supported for IndirectColumn.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeSum(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");

        /// <summary>
        /// Not supported for IndirectColumn.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeMean(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");

        /// <summary>
        /// Not supported for IndirectColumn.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeMin(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");

        /// <summary>
        /// Not supported for IndirectColumn.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeMax(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");
    }
}
===== FILE: src/LeichtFrame.Core/Columns/IntColumn.cs =====
using System.Buffers;
using System.Numerics;
using System.Runtime.InteropServices;
using System.Runtime.CompilerServices;
using LeichtFrame.Core.Engine;
using LeichtFrame.Core.Engine.Memory;

namespace LeichtFrame.Core
{
    /// <summary>
    /// A high-performance column for storing <see cref="int"/> values.
    /// Uses pooled arrays for zero-allocation data management.
    /// </summary>
    public class IntColumn : Column<int>, IDisposable
    {
        private int[] _data;
        private NullBitmap? _nulls;
        private int _length;

        /// <summary>
        /// Initializes a new instance of the <see cref="IntColumn"/> class.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        /// <param name="capacity">The initial capacity (number of rows).</param>
        /// <param name="isNullable">Whether the column supports null values.</param>
        public IntColumn(string name, int capacity = 16, bool isNullable = false)
            : base(name, isNullable)
        {
            _length = 0;
            _data = ArrayPool<int>.Shared.Rent(capacity);

            if (isNullable)
            {
                _nulls = new NullBitmap(capacity);
            }
        }

        /// <inheritdoc />
        public override int Length => _length;

        /// <inheritdoc />
        public override ReadOnlyMemory<int> Values => new ReadOnlyMemory<int>(_data, 0, _length);

        // --- Core Get/Set ---

        /// <inheritdoc />
        public override int Get(int index)
        {
            CheckBounds(index);
            return _data[index];
        }

        /// <inheritdoc />
        public override void SetValue(int index, int value)
        {
            CheckBounds(index);
            _data[index] = value;
            _nulls?.SetNotNull(index);
        }

        // --- Append ---

        /// <inheritdoc />
        public override void Append(int value)
        {
            EnsureCapacity(_length + 1);
            _data[_length] = value;
            _nulls?.SetNotNull(_length);
            _length++;
        }

        /// <summary>
        /// Appends a nullable integer value to the column.
        /// </summary>
        /// <param name="value">The value to append, or null.</param>
        /// <exception cref="InvalidOperationException">Thrown if null is passed to a non-nullable column.</exception>
        public void Append(int? value)
        {
            EnsureCapacity(_length + 1);

            if (value.HasValue)
            {
                _data[_length] = value.Value;
                _nulls?.SetNotNull(_length);
            }
            else
            {
                if (_nulls == null)
                    throw new InvalidOperationException("Cannot append null to a non-nullable column.");

                _data[_length] = default;
                _nulls.SetNull(_length);
            }
            _length++;
        }

        // --- Null Handling ---

        /// <inheritdoc />
        public override bool IsNull(int index)
        {
            CheckBounds(index);
            return _nulls != null && _nulls.IsNull(index);
        }

        /// <inheritdoc />
        public override void SetNull(int index)
        {
            CheckBounds(index);
            if (_nulls == null)
                throw new InvalidOperationException("Cannot set null on a non-nullable column.");

            _data[index] = default;
            _nulls.SetNull(index);
        }

        /// <inheritdoc />
        public override void SetNotNull(int index)
        {
            CheckBounds(index);
            _nulls?.SetNotNull(index);
        }

        // --- SIMD Aggregations ---

        /// <summary>
        /// Calculates the sum of the column using SIMD with overflow protection (extends to long).
        /// </summary>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public long Sum()
        {
            var span = Values.Span;
            long sum = 0;

            if (Vector.IsHardwareAccelerated && span.Length >= Vector<int>.Count)
            {
                var vectors = MemoryMarshal.Cast<int, Vector<int>>(span);

                // We utilize Vector<long> to accumulate sums to prevent 32-bit overflow.
                var accVectorLow = Vector<long>.Zero;
                var accVectorHigh = Vector<long>.Zero;

                foreach (var v in vectors)
                {
                    // Widen: Split Vector<int> into two Vector<long>
                    Vector.Widen(v, out var low, out var high);
                    accVectorLow += low;
                    accVectorHigh += high;
                }

                // Sum up the lanes of the long accumulators
                sum += Vector.Sum(accVectorLow);
                sum += Vector.Sum(accVectorHigh);

                // Handle remaining elements (Tail)
                int processed = vectors.Length * Vector<int>.Count;
                span = span.Slice(processed);
            }

            // Scalar fallback / Tail loop
            foreach (var val in span)
            {
                sum += val;
            }

            return sum;
        }

        /// <summary>
        /// Calculates the minimum value using SIMD (only for non-nullable).
        /// </summary>
        public int Min()
        {
            if (_length == 0) return 0;

            // SIMD is only safe for non-nullable columns because '0' (null representation) 
            // would falsify the Min calculation.
            if (IsNullable)
            {
                return MinNullable();
            }

            var span = Values.Span;
            int min = int.MaxValue;

            if (Vector.IsHardwareAccelerated && span.Length >= Vector<int>.Count)
            {
                var vectors = MemoryMarshal.Cast<int, Vector<int>>(span);
                var minVector = new Vector<int>(int.MaxValue);

                foreach (var v in vectors)
                {
                    minVector = Vector.Min(minVector, v);
                }

                // Reduce vector lanes
                for (int i = 0; i < Vector<int>.Count; i++)
                {
                    if (minVector[i] < min) min = minVector[i];
                }

                int processed = vectors.Length * Vector<int>.Count;
                span = span.Slice(processed);
            }

            foreach (var val in span)
            {
                if (val < min) min = val;
            }

            return min;
        }

        /// <summary>
        /// Calculates the maximum value using SIMD (only for non-nullable).
        /// </summary>
        public int Max()
        {
            if (_length == 0) return 0;

            if (IsNullable)
            {
                return MaxNullable();
            }

            var span = Values.Span;
            int max = int.MinValue;

            if (Vector.IsHardwareAccelerated && span.Length >= Vector<int>.Count)
            {
                var vectors = MemoryMarshal.Cast<int, Vector<int>>(span);
                var maxVector = new Vector<int>(int.MinValue);

                foreach (var v in vectors)
                {
                    maxVector = Vector.Max(maxVector, v);
                }

                for (int i = 0; i < Vector<int>.Count; i++)
                {
                    if (maxVector[i] > max) max = maxVector[i];
                }

                int processed = vectors.Length * Vector<int>.Count;
                span = span.Slice(processed);
            }

            foreach (var val in span)
            {
                if (val > max) max = val;
            }

            return max;
        }

        // Fallback helpers for Nullable types (Scalar loop with null checks)
        private int MinNullable()
        {
            int min = int.MaxValue;
            bool hasValue = false;
            for (int i = 0; i < _length; i++)
            {
                if (!IsNull(i))
                {
                    int val = _data[i];
                    if (val < min) min = val;
                    hasValue = true;
                }
            }
            return hasValue ? min : 0;
        }

        private int MaxNullable()
        {
            int max = int.MinValue;
            bool hasValue = false;
            for (int i = 0; i < _length; i++)
            {
                if (!IsNull(i))
                {
                    int val = _data[i];
                    if (val > max) max = val;
                    hasValue = true;
                }
            }
            return hasValue ? max : 0;
        }

        // --- Memory Management ---

        /// <inheritdoc />
        public override void EnsureCapacity(int minCapacity)
        {
            if (_data.Length >= minCapacity) return;

            int newCapacity = Math.Max(_data.Length * 2, minCapacity);

            var newBuffer = ArrayPool<int>.Shared.Rent(newCapacity);
            Array.Copy(_data, newBuffer, _length);
            ArrayPool<int>.Shared.Return(_data);
            _data = newBuffer;

            _nulls?.Resize(newCapacity);
        }

        private void CheckBounds(int index)
        {
            if ((uint)index >= (uint)_length)
                throw new IndexOutOfRangeException($"Index {index} is out of range (Length: {_length})");
        }

        /// <inheritdoc />
        public override IColumn CloneSubset(IReadOnlyList<int> indices)
        {
            var newCol = new IntColumn(Name, indices.Count, IsNullable);

            for (int i = 0; i < indices.Count; i++)
            {
                int sourceIndex = indices[i];
                if (IsNullable && IsNull(sourceIndex))
                {
                    newCol.Append(null);
                }
                else
                {
                    newCol.Append(Get(sourceIndex));
                }
            }
            return newCol;
        }

        /// <inheritdoc />
        public void Dispose()
        {
            if (_data != null)
            {
                ArrayPool<int>.Shared.Return(_data);
                _data = null!;
            }

            _nulls?.Dispose();
            _nulls = null;
        }

        // --- Arithmetic Operators ---

        /// <summary>
        /// Adds two integer columns element-wise.
        /// </summary>
        public static IntColumn operator +(IntColumn a, IntColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Add);

        /// <summary>
        /// Subtracts the second integer column from the first element-wise.
        /// </summary>
        public static IntColumn operator -(IntColumn a, IntColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Subtract);

        /// <summary>
        /// Multiplies two integer columns element-wise.
        /// </summary>
        public static IntColumn operator *(IntColumn a, IntColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Multiply);

        /// <summary>
        /// Divides the first integer column by the second element-wise.
        /// </summary>
        public static IntColumn operator /(IntColumn a, IntColumn b) => ExecuteOp(a, b, VectorizedMathOps.MathOp.Divide);

        /// <summary>
        /// Adds a scalar value to every element in the column.
        /// </summary>
        public static IntColumn operator +(IntColumn a, int b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Add);

        /// <summary>
        /// Subtracts a scalar value from every element in the column.
        /// </summary>
        public static IntColumn operator -(IntColumn a, int b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Subtract);

        /// <summary>
        /// Multiplies every element in the column by a scalar value.
        /// </summary>
        public static IntColumn operator *(IntColumn a, int b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Multiply);

        /// <summary>
        /// Divides every element in the column by a scalar value.
        /// </summary>
        public static IntColumn operator /(IntColumn a, int b) => ExecuteOpScalar(a, b, VectorizedMathOps.MathOp.Divide);

        private static IntColumn ExecuteOp(IntColumn a, IntColumn b, VectorizedMathOps.MathOp op)
        {
            if (a.Length != b.Length) throw new ArgumentException("Column lengths mismatch");

            bool resultNullable = a.IsNullable || b.IsNullable;
            var result = new IntColumn($"{a.Name}_op_{b.Name}", a.Length, resultNullable);

            result._length = a.Length;
            VectorizedMathOps.Calculate<int>(a._data.AsSpan(0, a.Length), b._data.AsSpan(0, b.Length), result._data.AsSpan(0, a.Length), op);

            if (resultNullable)
            {
                result._nulls?.Dispose();
                result._nulls = NullBitmap.MergeOr(a._nulls, b._nulls, a.Length);
            }
            return result;
        }

        private static IntColumn ExecuteOpScalar(IntColumn a, int scalar, VectorizedMathOps.MathOp op)
        {
            var result = new IntColumn($"{a.Name}_op_scalar", a.Length, a.IsNullable);
            result._length = a.Length;

            VectorizedMathOps.CalculateScalar<int>(a._data.AsSpan(0, a.Length), scalar, result._data.AsSpan(0, a.Length), op);

            if (a.IsNullable && a._nulls != null)
            {
                result._nulls?.Dispose();
                result._nulls = NullBitmap.MergeOr(a._nulls, null, a.Length);
            }
            return result;
        }

        // =======================================================================
        // AGGREGATION ENGINE IMPLEMENTATION
        // =======================================================================

        /// <inheritdoc />
        public override object? ComputeSum(int[] indices, int start, int end)
        {
            long sum = 0;
            var data = _data; // Local ref for performance optimization

            if (!IsNullable)
            {
                // Non-nullable path: tight loop
                for (int i = start; i < end; i++)
                {
                    sum += data[indices[i]];
                }
            }
            else
            {
                // Nullable path: check null bitmap
                var nulls = _nulls!;
                for (int i = start; i < end; i++)
                {
                    int idx = indices[i];
                    if (!nulls.IsNull(idx))
                    {
                        sum += data[idx];
                    }
                }
            }
            return sum;
        }

        /// <inheritdoc />
        public override object? ComputeMean(int[] indices, int start, int end)
        {
            long sum = 0;
            int count = 0;
            var data = _data;

            if (!IsNullable)
            {
                count = end - start;
                if (count == 0) return null;

                for (int i = start; i < end; i++)
                {
                    sum += data[indices[i]];
                }
            }
            else
            {
                var nulls = _nulls!;
                for (int i = start; i < end; i++)
                {
                    int idx = indices[i];
                    if (!nulls.IsNull(idx))
                    {
                        sum += data[idx];
                        count++;
                    }
                }
                if (count == 0) return null;
            }

            return (double)sum / count;
        }

        /// <inheritdoc />
        public override object? ComputeMin(int[] indices, int start, int end)
        {
            if (start == end) return null;

            int min = int.MaxValue;
            bool hasValue = false;
            var data = _data;

            if (!IsNullable)
            {
                for (int i = start; i < end; i++)
                {
                    int val = data[indices[i]];
                    if (val < min) min = val;
                }
                hasValue = true;
            }
            else
            {
                var nulls = _nulls!;
                for (int i = start; i < end; i++)
                {
                    int idx = indices[i];
                    if (!nulls.IsNull(idx))
                    {
                        int val = data[idx];
                        if (val < min) min = val;
                        hasValue = true;
                    }
                }
            }

            return hasValue ? min : null;
        }

        /// <inheritdoc />
        public override object? ComputeMax(int[] indices, int start, int end)
        {
            if (start == end) return null;

            int max = int.MinValue;
            bool hasValue = false;
            var data = _data;

            if (!IsNullable)
            {
                for (int i = start; i < end; i++)
                {
                    int val = data[indices[i]];
                    if (val > max) max = val;
                }
                hasValue = true;
            }
            else
            {
                var nulls = _nulls!;
                for (int i = start; i < end; i++)
                {
                    int idx = indices[i];
                    if (!nulls.IsNull(idx))
                    {
                        int val = data[idx];
                        if (val > max) max = val;
                        hasValue = true;
                    }
                }
            }

            return hasValue ? max : null;
        }

        /// <summary>
        /// Internal Move-Constructor.
        /// </summary>
        internal IntColumn(string name, int[] adoptedData, int length)
            : base(name, isNullable: false)
        {
            _data = adoptedData;
            _length = length;
            _nulls = null;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Columns/SlicedColumn.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// A zero-copy view over a subset of another column.
    /// Delegates calls to the source column with an index offset without allocating new memory for data.
    /// </summary>
    /// <typeparam name="T">The type of data stored in the column.</typeparam>
    public class SlicedColumn<T> : IColumn<T>, IDisposable
    {
        private readonly IColumn<T> _source;
        private readonly int _offset;
        private readonly int _length;

        /// <summary>
        /// Initializes a new instance of the <see cref="SlicedColumn{T}"/> class.
        /// </summary>
        /// <param name="source">The underlying source column.</param>
        /// <param name="offset">The zero-based starting index in the source column.</param>
        /// <param name="length">The number of rows in the slice.</param>
        /// <exception cref="ArgumentOutOfRangeException">Thrown if offset or length are negative.</exception>
        /// <exception cref="ArgumentException">Thrown if the slice range exceeds the source column bounds.</exception>
        public SlicedColumn(IColumn<T> source, int offset, int length)
        {
            if (offset < 0) throw new ArgumentOutOfRangeException(nameof(offset));
            if (length < 0) throw new ArgumentOutOfRangeException(nameof(length));

            if (offset + length > source.Length)
                throw new ArgumentException($"Slice range ({offset}..{offset + length}) exceeds source column bounds (Length: {source.Length}).");

            _source = source;
            _offset = offset;
            _length = length;
        }

        /// <inheritdoc />
        public string Name => _source.Name;

        /// <inheritdoc />
        public Type DataType => _source.DataType;

        /// <inheritdoc />
        public int Length => _length;

        /// <inheritdoc />
        public bool IsNullable => _source.IsNullable;

        /// <summary>
        /// Gets the underlying memory storage of the slice.
        /// </summary>
        public ReadOnlyMemory<T> Values => _source.Slice(_offset, _length);

        /// <inheritdoc />
        public ReadOnlySpan<T> AsSpan() => Values.Span;

        // --- Data Access ---

        /// <inheritdoc cref="IColumn{T}.GetValue(int)" />
        public T Get(int index)
        {
            CheckBounds(index);
            return _source.GetValue(index + _offset);
        }

        // Interface Implementation
        /// <exclude />
        T IColumn<T>.GetValue(int index) => Get(index);

        /// <inheritdoc />
        public object? GetValue(int index)
        {
            CheckBounds(index);
            return _source.GetValue(index + _offset);
        }

        /// <inheritdoc />
        public void SetValue(int index, T value)
        {
            CheckBounds(index);
            _source.SetValue(index + _offset, value);
        }

        // --- Appending (Not Supported for Views) ---
        // Views cannot grow, so we explicitly forbid appending.

        /// <summary>
        /// Not supported for SlicedColumn. Slices have a fixed size.
        /// </summary>
        /// <exception cref="NotSupportedException">Always thrown.</exception>
        public void Append(T value)
        {
            throw new NotSupportedException("Cannot append to a SlicedColumn view. Append to the source column instead.");
        }

        /// <summary>
        /// Not supported for SlicedColumn. Slices have a fixed size.
        /// </summary>
        /// <exception cref="NotSupportedException">Always thrown.</exception>
        public void AppendObject(object? value)
        {
            throw new NotSupportedException("Cannot append to a SlicedColumn view. Append to the source column instead.");
        }

        /// <summary>
        /// Not supported for SlicedColumn.
        /// </summary>
        /// <exception cref="NotSupportedException">Always thrown.</exception>
        public void EnsureCapacity(int capacity)
        {
            throw new NotSupportedException("Cannot resize a SlicedColumn view.");
        }

        // --- Slicing ---

        /// <inheritdoc />
        public ReadOnlyMemory<T> Slice(int start, int length)
        {
            CheckBounds(start);
            if (start + length > _length) throw new ArgumentOutOfRangeException(nameof(length));

            // Delegate to source slice with accumulated offset
            return _source.Slice(start + _offset, length);
        }

        /// <inheritdoc />
        public IColumn CloneSubset(IReadOnlyList<int> indices)
        {
            var mappedIndices = new int[indices.Count];
            for (int i = 0; i < indices.Count; i++)
            {
                if (indices[i] < 0 || indices[i] >= _length)
                    throw new IndexOutOfRangeException();

                mappedIndices[i] = indices[i] + _offset;
            }
            return _source.CloneSubset(mappedIndices);
        }

        // --- Null Handling ---

        /// <inheritdoc />
        public bool IsNull(int index)
        {
            CheckBounds(index);
            return _source.IsNull(index + _offset);
        }

        /// <inheritdoc />
        public void SetNull(int index)
        {
            CheckBounds(index);
            _source.SetNull(index + _offset);
        }

        // --- Helpers ---

        /// <inheritdoc />
        public void Dispose()
        {
            // Do nothing. We do NOT own the underlying memory.
        }

        private void CheckBounds(int index)
        {
            if ((uint)index >= (uint)_length)
                throw new IndexOutOfRangeException($"Index {index} is out of slice bounds (Length {_length})");
        }

        // --- Aggregation Interface Implementation (Not Supported for Slices yet) ---

        /// <summary>
        /// Computes the sum for a subset of rows defined by indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeSum(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");

        /// <summary>
        /// Computes the mean for a subset of rows defined by indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeMean(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");

        /// <summary>
        /// Finds the minimum value for a subset of rows defined by indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeMin(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");

        /// <summary>
        /// Finds the maximum value for a subset of rows defined by indices.
        /// </summary>
        /// <param name="indices"></param>
        /// <param name="start"></param>
        /// <param name="end"></param>
        /// <returns></returns>
        /// <exception cref="NotSupportedException"></exception>
        public object? ComputeMax(int[] indices, int start, int end)
            => throw new NotSupportedException($"Aggregation not supported on {GetType().Name}");
    }
}
===== FILE: src/LeichtFrame.Core/Columns/StringColumn.cs =====
using System.Buffers;
using System.Runtime.CompilerServices;
using System.Text;
using LeichtFrame.Core.Engine.Memory;

namespace LeichtFrame.Core
{
    /// <summary>
    /// A high-performance column for storing variable-length strings using Arrow-style layout.
    /// Data is stored as contiguous UTF-8 bytes to minimize GC pressure and improve locality.
    /// </summary>
    public class StringColumn : Column<string?>, IDisposable
    {
        // 1. Offsets: Start position of each string in the byte buffer. Size: RowCount + 1
        private int[] _offsets;

        // 2. Values: The concatenated UTF-8 bytes of all strings.
        private byte[] _values;

        // 3. Nulls: Bitmap for null values.
        private NullBitmap? _nulls;

        private int _length;          // Number of rows
        private int _totalByteCount;  // Currently used bytes in _values

        /// <summary>
        /// Initializes a new instance of the <see cref="StringColumn"/> class.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        /// <param name="capacity">The initial row capacity.</param>
        /// <param name="isNullable">Whether nulls are allowed.</param>
        public StringColumn(string name, int capacity = 16, bool isNullable = false)
            : base(name, isNullable)
        {
            _length = 0;
            _totalByteCount = 0;

            // Offsets always need N+1 entries (Start of next is End of current)
            _offsets = ArrayPool<int>.Shared.Rent(capacity + 1);
            _offsets[0] = 0;

            // Heuristic: Assume average string length is 32 bytes.
            // This prevents frequent resizing at the beginning.
            int initialByteCapacity = capacity * 32;
            if (initialByteCapacity < 64) initialByteCapacity = 64;

            _values = ArrayPool<byte>.Shared.Rent(initialByteCapacity);

            if (isNullable)
            {
                _nulls = new NullBitmap(capacity);
            }
        }

        /// <inheritdoc />
        public override int Length => _length;

        /// <summary>
        /// Not supported for variable length layout directly as contiguous memory.
        /// Use <see cref="Get(int)"/> or specialized Span accessors.
        /// </summary>
        /// <exception cref="NotSupportedException">Always thrown.</exception>
        public override ReadOnlyMemory<string?> Values => throw new NotSupportedException(
            "StringColumn uses Arrow-style byte storage. Contiguous string references are not available.");

        // --- Core Access ---

        /// <inheritdoc />
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public override string? Get(int index)
        {
            CheckBounds(index);

            if (_nulls != null && _nulls.IsNull(index))
                return null;

            int start = _offsets[index];
            int end = _offsets[index + 1]; // Length = Offset[i+1] - Offset[i]
            int byteLen = end - start;

            if (byteLen == 0) return string.Empty;

            // Allocates a new string (Lazy Decoding)
            return Encoding.UTF8.GetString(_values, start, byteLen);
        }

        /// <summary>
        /// Sets the value at the specified index.
        /// <para>
        /// ‚ö†Ô∏è **PERFORMANCE WARNING:** This operation is **O(N)** because it requires shifting all subsequent bytes 
        /// in the underlying buffer to accommodate the new string length. 
        /// Use this method only for corrections, not for bulk updates.
        /// </para>
        /// </summary>
        /// <param name="index">The zero-based row index.</param>
        /// <param name="value">The new string value.</param>
        public override void SetValue(int index, string? value)
        {
            CheckBounds(index);

            // 1. Determine old length
            int oldStart = _offsets[index];
            int oldLen = _offsets[index + 1] - oldStart;

            // 2. Calculate new length
            int newLen = 0;
            byte[]? newBytes = null;

            if (value != null)
            {
                // Optimization: Encoding.UTF8.GetByteCount would be faster for alloc check,
                // but we need the bytes anyway.
                newBytes = Encoding.UTF8.GetBytes(value);
                newLen = newBytes.Length;
            }

            // 3. Calculate shift difference
            int diff = newLen - oldLen;

            // Make space or close gap?
            if (diff > 0)
            {
                EnsureByteCapacity(_totalByteCount + diff);
                // Shift Right: Move everything after the old string to the right
                Array.Copy(_values, oldStart + oldLen,
                           _values, oldStart + newLen,
                           _totalByteCount - (oldStart + oldLen));
            }
            else if (diff < 0)
            {
                // Shift Left: Pull everything forward
                Array.Copy(_values, oldStart + oldLen,
                           _values, oldStart + newLen,
                           _totalByteCount - (oldStart + oldLen));
            }

            // 4. Write data
            if (newBytes != null)
            {
                Array.Copy(newBytes, 0, _values, oldStart, newLen);
                if (IsNullable) SetNotNull(index);
            }
            else
            {
                if (!IsNullable) throw new ArgumentNullException(nameof(value), "Column is not nullable");
                _nulls?.SetNull(index);
            }

            // 5. Update offsets (for ALL subsequent rows!)
            for (int i = index + 1; i <= _length; i++)
            {
                _offsets[i] += diff;
            }

            _totalByteCount += diff;
        }

        /// <inheritdoc />
        public override void Append(string? value)
        {
            EnsureCapacity(_length + 1);

            int byteLen = 0;
            if (value != null)
            {
                // 1. Calculate bytes
                byteLen = Encoding.UTF8.GetByteCount(value);

                // 2. Ensure byte capacity
                EnsureByteCapacity(_totalByteCount + byteLen);

                // 3. Write bytes
                Encoding.UTF8.GetBytes(value, 0, value.Length, _values, _totalByteCount);

                if (_nulls != null) _nulls.SetNotNull(_length);
            }
            else
            {
                if (_nulls == null)
                    throw new InvalidOperationException("Cannot append null to non-nullable column.");

                _nulls.SetNull(_length);
            }

            // 4. Update pointers
            _totalByteCount += byteLen;
            _offsets[_length + 1] = _totalByteCount;

            _length++;
        }

        /// <summary>
        /// Compares the string at indexA with the string at indexB directly on the byte buffer.
        /// Returns -1, 0, or 1.
        /// Zero-Allocation implementation.
        /// </summary>
        public int CompareRaw(int indexA, int indexB)
        {
            // 1. Null Handling
            bool nullA = IsNull(indexA);
            bool nullB = IsNull(indexB);

            if (nullA && nullB) return 0;
            if (nullA) return -1; // Null is smaller
            if (nullB) return 1;

            // 2. Get Spans (Zero-Copy pointers)
            int startA = _offsets[indexA];
            int lenA = _offsets[indexA + 1] - startA;
            ReadOnlySpan<byte> spanA = _values.AsSpan(startA, lenA);

            int startB = _offsets[indexB];
            int lenB = _offsets[indexB + 1] - startB;
            ReadOnlySpan<byte> spanB = _values.AsSpan(startB, lenB);

            // 3. Compare Bytes directly
            // SequenceCompareTo is an optimized .NET intrinsic method
            return spanA.SequenceCompareTo(spanB);
        }

        /// <summary>
        /// Computes the hash code of the string at the specified index directly from the byte buffer.
        /// Zero-Allocation.
        /// </summary>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public int GetHashCodeRaw(int index)
        {
            // 1. Null Check
            if (_nulls != null && _nulls.IsNull(index)) return 0;

            // 2. Locate Bytes
            int start = _offsets[index];
            int length = _offsets[index + 1] - start;

            if (length == 0) return string.Empty.GetHashCode();

            // 3. Compute Hash (FNV-1a or similar fast hash)            
            int hash = unchecked((int)2166136261);

            var span = _values.AsSpan(start, length);

            for (int i = 0; i < span.Length; i++)
            {
                hash ^= span[i];
                hash *= 16777619;
            }

            return hash;
        }

        // --- INTERNAL ACCESS FOR OPTIMIZED OPS ---

        /// <summary>Internal access to raw offsets for high-performance grouping/sorting.</summary>
        internal int[] Offsets => _offsets;

        /// <summary>Internal access to raw bytes for high-performance grouping/sorting.</summary>
        internal byte[] RawBytes => _values;

        // --- Null Handling ---

        /// <inheritdoc />
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public override bool IsNull(int index)
        {
            CheckBounds(index);
            return _nulls != null && _nulls.IsNull(index);
        }

        /// <inheritdoc />
        public override void SetNull(int index)
        {
            // We use SetValue to correctly shift bytes (length 0).
            SetValue(index, null);
        }

        /// <inheritdoc />
        public override void SetNotNull(int index)
        {
            CheckBounds(index);
            _nulls?.SetNotNull(index);
        }

        // --- Memory ---

        /// <inheritdoc />
        public override void EnsureCapacity(int minCapacity)
        {
            // 1. Resize Offsets Array (Rows)
            if (_offsets.Length < minCapacity + 1)
            {
                int newCap = Math.Max(_offsets.Length * 2, minCapacity + 1);
                var newBuf = ArrayPool<int>.Shared.Rent(newCap);
                Array.Copy(_offsets, newBuf, _length + 1);
                ArrayPool<int>.Shared.Return(_offsets);
                _offsets = newBuf;

                _nulls?.Resize(newCap);
            }
        }

        internal void EnsureByteCapacity(int minBytes)
        {
            if (_values.Length < minBytes)
            {
                int newCap = Math.Max(_values.Length * 2, minBytes);
                var newBuf = ArrayPool<byte>.Shared.Rent(newCap);

                Array.Copy(_values, newBuf, _totalByteCount);

                ArrayPool<byte>.Shared.Return(_values);
                _values = newBuf;
            }
        }

        private void CheckBounds(int index)
        {
            if ((uint)index >= (uint)_length) throw new IndexOutOfRangeException();
        }

        /// <inheritdoc />
        public override IColumn CloneSubset(IReadOnlyList<int> indices)
        {
            int newRowCount = indices.Count;
            var newCol = new StringColumn(Name, newRowCount, IsNullable);

            int totalBytesNeeded = 0;

            if (indices is int[] arrIndices)
            {
                for (int i = 0; i < newRowCount; i++)
                {
                    int idx = arrIndices[i];
                    if (!IsNullable || !IsNull(idx))
                    {
                        totalBytesNeeded += _offsets[idx + 1] - _offsets[idx];
                    }
                }
            }
            else
            {
                for (int i = 0; i < newRowCount; i++)
                {
                    int idx = indices[i];
                    if (!IsNullable || !IsNull(idx))
                    {
                        totalBytesNeeded += _offsets[idx + 1] - _offsets[idx];
                    }
                }
            }

            newCol.EnsureByteCapacity(totalBytesNeeded);

            int currentDestOffset = 0;

            var destOffsets = newCol._offsets;
            var destBytes = newCol._values;
            var destNulls = newCol._nulls;

            destOffsets[0] = 0;

            for (int i = 0; i < newRowCount; i++)
            {
                int srcIdx = indices[i];

                if (IsNullable && IsNull(srcIdx))
                {
                    destNulls!.SetNull(i);
                    destOffsets[i + 1] = currentDestOffset;
                    continue;
                }

                int start = _offsets[srcIdx];
                int length = _offsets[srcIdx + 1] - start;

                if (length > 0)
                {
                    Array.Copy(_values, start, destBytes, currentDestOffset, length);
                    currentDestOffset += length;
                }

                destOffsets[i + 1] = currentDestOffset;
            }

            newCol._length = newRowCount;
            newCol._totalByteCount = currentDestOffset;

            return newCol;
        }

        /// <inheritdoc />
        public void Dispose()
        {
            if (_offsets != null)
            {
                ArrayPool<int>.Shared.Return(_offsets);
                _offsets = null!;
            }
            if (_values != null)
            {
                ArrayPool<byte>.Shared.Return(_values);
                _values = null!;
            }
            _nulls?.Dispose();
            _nulls = null;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Common/CompareOp.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// Defines the comparison operations available for vectorized filtering.
    /// </summary>
    public enum CompareOp
    {
        /// <summary>
        /// Checks if the value is equal to the target.
        /// </summary>
        Equal,

        /// <summary>
        /// Checks if the value is not equal to the target.
        /// </summary>
        NotEqual,

        /// <summary>
        /// Checks if the value is strictly greater than the target.
        /// </summary>
        GreaterThan,

        /// <summary>
        /// Checks if the value is greater than or equal to the target.
        /// </summary>
        GreaterThanOrEqual,

        /// <summary>
        /// Checks if the value is strictly less than the target.
        /// </summary>
        LessThan,

        /// <summary>
        /// Checks if the value is less than or equal to the target.
        /// </summary>
        LessThanOrEqual
    }
}
===== FILE: src/LeichtFrame.Core/Common/JoinType.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// Specifies the type of join operation to perform when combining two DataFrames.
    /// </summary>
    public enum JoinType
    {
        /// <summary>
        /// Returns records that have matching values in both tables.
        /// </summary>
        Inner,

        /// <summary>
        /// Returns all records from the left table, and the matched records from the right table. 
        /// Unmatched records from the right side are null.
        /// </summary>
        Left
    }
}
===== FILE: src/LeichtFrame.Core/DataFrame/DataFrame.cs =====
using System.Reflection;
using System.Text;
using LeichtFrame.Core.Expressions;
using LeichtFrame.Core.Operations.Delegates;

namespace LeichtFrame.Core
{
    /// <summary>
    /// Represents a high-performance, column-oriented in-memory data table.
    /// Optimized for low memory allocation and fast analytical queries using SIMD and <see cref="Span{T}"/>.
    /// </summary>
    public class DataFrame : IDisposable
    {
        private readonly List<IColumn> _columns;
        private bool _isDisposed;

        /// <summary>
        /// Gets the schema definition of this DataFrame.
        /// </summary>
        public DataFrameSchema Schema { get; }

        /// <summary>
        /// Gets the internal list of columns.
        /// </summary>
        public IReadOnlyList<IColumn> Columns => _columns;

        /// <summary>
        /// Gets the number of rows in the DataFrame.
        /// </summary>
        public int RowCount => _columns.Count > 0 ? _columns[0].Length : 0;

        /// <summary>
        /// Gets the number of columns in the DataFrame.
        /// </summary>
        public int ColumnCount => _columns.Count;

        /// <summary>
        /// Creates a new DataFrame from the provided columns.
        /// Validates that all columns share the same length.
        /// </summary>
        public DataFrame(IEnumerable<IColumn> columns)
        {
            if (columns == null) throw new ArgumentNullException(nameof(columns));

            _columns = columns.ToList();

            // 1. Validation: Row Count Consistency
            if (_columns.Count > 0)
            {
                int expectedLength = _columns[0].Length;
                if (_columns.Any(c => c.Length != expectedLength))
                {
                    throw new ArgumentException(
                       $"Column length mismatch. Expected {expectedLength} rows based on first column.");
                }
            }

            // 2. Build Schema automatically from column metadata
            var definitions = _columns.Select(c => new ColumnDefinition(c.Name, c.DataType, c.IsNullable));
            Schema = new DataFrameSchema(definitions);
        }

        /// <summary>
        /// Creates a new, empty DataFrame based on the provided schema.
        /// Pre-allocates memory for the specified capacity to minimize resize operations.
        /// </summary>
        public static DataFrame Create(DataFrameSchema schema, int capacity = 16)
        {
            if (schema == null) throw new ArgumentNullException(nameof(schema));
            if (capacity < 0) throw new ArgumentOutOfRangeException(nameof(capacity));

            var columns = new List<IColumn>(schema.Columns.Count);

            foreach (var colDef in schema.Columns)
            {
                var col = ColumnFactory.Create(colDef.Name, colDef.DataType, capacity, colDef.IsNullable);
                columns.Add(col);
            }

            return new DataFrame(columns);
        }

        /// <summary>
        /// Creates a DataFrame from a collection of objects (POCOs) using Reflection.
        /// </summary>
        public static DataFrame FromObjects<T>(IEnumerable<T> objects)
        {
            if (objects == null) throw new ArgumentNullException(nameof(objects));

            // 1. Get Schema via centralized logic
            var schema = DataFrameSchema.FromType<T>();

            // 2. Prepare for data population
            int estimatedCount = objects is ICollection<T> coll ? coll.Count : 16;
            var df = DataFrame.Create(schema, estimatedCount);

            // Cache PropertyInfos for speed
            var type = typeof(T);
            var propMap = new Dictionary<string, PropertyInfo>();
            foreach (var col in df.Columns)
            {
                propMap[col.Name] = type.GetProperty(col.Name)!;
            }

            // 3. Populate Data
            foreach (var item in objects)
            {
                foreach (var col in df.Columns)
                {
                    var prop = propMap[col.Name];
                    object? val = prop.GetValue(item);
                    col.AppendObject(val);
                }
            }

            return df;
        }

        // --- INDEXERS & ACCESSORS ---

        /// <summary>
        /// Gets the column at the specified index.
        /// </summary>
        public IColumn this[int index] => _columns[index];

        /// <summary>
        /// Gets the column with the specified name.
        /// </summary>
        public IColumn this[string name] => _columns[Schema.GetColumnIndex(name)];

        /// <summary>
        /// Tries to get the column with the specified name.
        /// </summary>
        public bool TryGetColumn(string name, out IColumn? column)
        {
            if (Schema.HasColumn(name))
            {
                column = this[name];
                return true;
            }
            column = null;
            return false;
        }

        /// <summary>
        /// Returns the names of all columns in the DataFrame.
        /// </summary>
        public IEnumerable<string> GetColumnNames() => _columns.Select(c => c.Name);

        /// <summary>
        /// Returns the .NET Type of the data stored in the specified column.
        /// </summary>
        public Type GetColumnType(string name) => this[name].DataType;

        /// <summary>
        /// Checks if a column with the given name exists in the DataFrame.
        /// </summary>
        public bool HasColumn(string name) => Schema.HasColumn(name);

        // =========================================================
        // EAGER OPERATIONS (Direct Execution)
        // =========================================================

        /// <summary>
        /// Filters rows using a C# delegate. Executed immediately via row-by-row iteration.
        /// Use only if logic cannot be expressed via <see cref="Lazy"/>.
        /// </summary>
        /// <param name="predicate">The filter condition.</param>
        /// <returns>A new DataFrame containing matching rows.</returns>
        public DataFrame Where(Func<RowView, bool> predicate)
        {
            // Delegates to the specialized Eager implementation in Operations/Delegates
            return FilterDelegateOps.Execute(this, predicate);
        }

        /// <summary>
        /// Selects columns immediately. Uses the high-performance Lazy Engine internally.
        /// </summary>
        /// <param name="columnNames">The names of the columns to keep.</param>
        public DataFrame Select(params string[] columnNames)
        {
            return this.Lazy().Select(columnNames).Collect();
        }

        /// <summary>
        /// Sorts the DataFrame immediately in ascending order.
        /// </summary>
        public DataFrame OrderBy(string columnName)
        {
            return this.Lazy().OrderBy(columnName).Collect();
        }

        /// <summary>
        /// Sorts the DataFrame immediately in descending order.
        /// </summary>
        public DataFrame OrderByDescending(string columnName)
        {
            return this.Lazy().OrderByDescending(columnName).Collect();
        }

        // =========================================================
        // LAZY ENTRY POINT
        // =========================================================

        /// <summary>
        /// Converts this DataFrame into a <see cref="LazyDataFrame"/> to enable 
        /// optimization and lazy evaluation of subsequent operations.
        /// </summary>
        /// <returns>A lazy wrapper around this DataFrame.</returns>
        public LazyDataFrame Lazy()
        {
            return LazyDataFrame.From(this);
        }

        // --- UTILS & DISPOSE ---

        /// <summary>
        /// Returns a short summary of the DataFrame.
        /// </summary>
        public override string ToString()
        {
            return $"DataFrame ({RowCount} rows, {ColumnCount} columns)";
        }

        /// <summary>
        /// Generates a formatted string representing the first N rows of the DataFrame.
        /// </summary>
        public string Inspect(int limit = 10)
        {
            if (ColumnCount == 0) return "Empty DataFrame";

            var sb = new StringBuilder();
            sb.AppendLine(ToString());
            sb.AppendLine(new string('-', 30));

            int rowsToShow = Math.Min(RowCount, limit);
            int[] widths = new int[ColumnCount];

            // Calculate widths
            for (int c = 0; c < ColumnCount; c++)
            {
                var col = _columns[c];
                int maxWidth = Math.Max(col.Name.Length, col.DataType.Name.Length + 2);
                for (int r = 0; r < rowsToShow; r++)
                {
                    int len = col.GetValue(r)?.ToString()?.Length ?? 4;
                    if (len > maxWidth) maxWidth = len;
                }
                widths[c] = Math.Min(maxWidth, 50) + 2;
            }

            // Header
            for (int c = 0; c < ColumnCount; c++) sb.Append(_columns[c].Name.PadRight(widths[c]));
            sb.AppendLine();
            for (int c = 0; c < ColumnCount; c++) sb.Append($"<{_columns[c].DataType.Name}>".PadRight(widths[c]));
            sb.AppendLine();
            sb.AppendLine(new string('-', widths.Sum()));

            // Rows
            for (int r = 0; r < rowsToShow; r++)
            {
                for (int c = 0; c < ColumnCount; c++)
                {
                    object? val = _columns[c].GetValue(r);
                    string valStr = val is null ? "null" : val.ToString() ?? "";
                    if (valStr.Length > widths[c] - 1) valStr = valStr.Substring(0, widths[c] - 4) + "...";
                    sb.Append(valStr.PadRight(widths[c]));
                }
                sb.AppendLine();
            }

            if (RowCount > limit)
            {
                sb.AppendLine(new string('-', 20));
                sb.AppendLine($"... ({RowCount - limit} more rows)");
            }

            return sb.ToString();
        }

        /// <summary>
        /// Disposes all contained columns, returning their memory to the pool.
        /// </summary>
        public void Dispose()
        {
            Dispose(true);
            GC.SuppressFinalize(this);
        }

        /// <summary>
        /// Releases unmanaged and - optionally - managed resources.
        /// </summary>
        protected virtual void Dispose(bool disposing)
        {
            if (_isDisposed) return;

            if (disposing)
            {
                foreach (var col in _columns)
                {
                    if (col is IDisposable disposableCol)
                    {
                        disposableCol.Dispose();
                    }
                }
            }
            _isDisposed = true;
        }
    }
}
===== FILE: src/LeichtFrame.Core/DataFrame/DataFrameSchema.cs =====
using System.Text.Json;

namespace LeichtFrame.Core;

/// <summary>
/// Defines the metadata for a single column within a DataFrame schema.
/// </summary>
/// <param name="Name">The unique name of the column.</param>
/// <param name="DataType">The CLR type of the data stored in the column.</param>
/// <param name="IsNullable">Indicates if the column supports null values.</param>
/// <param name="SourceIndex">The index of the column in the original data source (if applicable).</param>
public record ColumnDefinition(string Name, Type DataType, bool IsNullable = false, int? SourceIndex = null);

/// <summary>
/// Represents the structure of a DataFrame, consisting of a collection of column definitions.
/// Provides lookup methods for column indices and types.
/// </summary>
public class DataFrameSchema
{
    private readonly List<ColumnDefinition> _columns;
    private readonly Dictionary<string, int> _nameMap;

    /// <summary>
    /// Gets the list of column definitions in this schema.
    /// </summary>
    public IReadOnlyList<ColumnDefinition> Columns => _columns;

    /// <summary>
    /// Initializes a new instance of the <see cref="DataFrameSchema"/> class.
    /// </summary>
    /// <param name="columns">The collection of column definitions.</param>
    /// <exception cref="ArgumentNullException">Thrown if columns is null.</exception>
    /// <exception cref="ArgumentException">Thrown if duplicate column names are detected.</exception>
    public DataFrameSchema(IEnumerable<ColumnDefinition> columns)
    {
        _columns = columns?.ToList() ?? throw new ArgumentNullException(nameof(columns));
        _nameMap = new Dictionary<string, int>();

        // Build lookup dictionary for fast access
        for (int i = 0; i < _columns.Count; i++)
        {
            var col = _columns[i];
            if (_nameMap.ContainsKey(col.Name))
                throw new ArgumentException($"Duplicate column name '{col.Name}' is not allowed.");

            _nameMap[col.Name] = i;
        }
    }

    /// <summary>
    /// Checks if a column with the given name exists in the schema.
    /// </summary>
    /// <param name="name">The name of the column to check.</param>
    /// <returns><c>true</c> if the column exists; otherwise, <c>false</c>.</returns>
    public bool HasColumn(string name) => _nameMap.ContainsKey(name);

    /// <summary>
    /// Gets the zero-based index of the column with the specified name.
    /// </summary>
    /// <param name="name">The name of the column.</param>
    /// <returns>The index of the column.</returns>
    /// <exception cref="ArgumentException">Thrown if the column does not exist.</exception>
    public int GetColumnIndex(string name)
    {
        if (_nameMap.TryGetValue(name, out int index))
            return index;

        throw new ArgumentException($"Column '{name}' does not exist in the schema.");
    }

    // --- JSON Serialization Logic ---

    /// <summary>
    /// Serializes the schema to a JSON string representation.
    /// Useful for persisting metadata or transferring schemas between processes.
    /// </summary>
    /// <returns>A JSON string defining the schema.</returns>
    public string ToJson()
    {
        // Convert to DTO because System.Type implies security risks and complexity in raw JSON. We store the Type Name as a string.
        var dto = new SchemaDto
        {
            Columns = _columns.Select(c => new ColumnDto
            {
                Name = c.Name,
                DataTypeName = c.DataType.AssemblyQualifiedName ?? c.DataType.FullName ?? c.DataType.Name,
                IsNullable = c.IsNullable
            }).ToList()
        };

        return JsonSerializer.Serialize(dto, new JsonSerializerOptions { WriteIndented = true });
    }

    /// <summary>
    /// Creates a <see cref="DataFrameSchema"/> from a JSON string.
    /// </summary>
    /// <param name="json">The JSON string containing the schema definition.</param>
    /// <returns>The deserialized schema.</returns>
    /// <exception cref="ArgumentException">Thrown if the JSON is invalid.</exception>
    public static DataFrameSchema FromJson(string json)
    {
        var dto = JsonSerializer.Deserialize<SchemaDto>(json);
        if (dto == null || dto.Columns == null)
            throw new ArgumentException("Invalid JSON schema.");

        var definitions = dto.Columns.Select(c => new ColumnDefinition(
            c.Name,
            Type.GetType(c.DataTypeName) ?? throw new InvalidOperationException($"Type '{c.DataTypeName}' not found."),
            c.IsNullable
        ));

        return new DataFrameSchema(definitions);
    }

    /// <summary>
    /// Helper to get the <see cref="Type"/> of a column by name.
    /// </summary>
    /// <param name="name">The column name.</param>
    /// <returns>The data type of the column.</returns>
    /// <exception cref="ArgumentException">Thrown if the column does not exist.</exception>
    public Type GetColumnType(string name)
    {
        int index = GetColumnIndex(name); // Wirft Fehler, wenn nicht gefunden
        return _columns[index].DataType;
    }

    // --- Private Helper Classes for JSON ---
    private class SchemaDto
    {
        public List<ColumnDto> Columns { get; set; } = new();
    }

    private class ColumnDto
    {
        public string Name { get; set; } = string.Empty;
        public string DataTypeName { get; set; } = string.Empty;
        public bool IsNullable { get; set; }
    }

    /// <summary>
    /// Creates a schema definition automatically from a C# class (POCO) using Reflection.
    /// Only supported primitive types (int, double, bool, string, DateTime) are mapped.
    /// </summary>
    /// <typeparam name="T">The POCO type to analyze.</typeparam>
    /// <returns>A derived <see cref="DataFrameSchema"/>.</returns>
    /// <exception cref="ArgumentException">Thrown if the type has no supported public properties.</exception>
    public static DataFrameSchema FromType<T>()
    {
        var properties = typeof(T).GetProperties(System.Reflection.BindingFlags.Public | System.Reflection.BindingFlags.Instance);
        var colDefs = new List<ColumnDefinition>();

        foreach (var prop in properties)
        {
            Type type = prop.PropertyType;
            // Unbox Nullable<T> -> T
            Type coreType = Nullable.GetUnderlyingType(type) ?? type;
            bool isNullable = !type.IsValueType || Nullable.GetUnderlyingType(type) != null;

            // Supported Types Check
            if (coreType != typeof(int) && coreType != typeof(double) &&
                coreType != typeof(string) && coreType != typeof(bool) &&
                coreType != typeof(DateTime))
            {
                continue; // Skip unsupported types
            }

            colDefs.Add(new ColumnDefinition(prop.Name, coreType, isNullable));
        }

        if (colDefs.Count == 0)
            throw new ArgumentException($"Type '{typeof(T).Name}' has no supported public properties.");

        return new DataFrameSchema(colDefs);
    }
}
===== FILE: src/LeichtFrame.Core/DataFrame/Enumerators/NativeGroupCountEnumerator.cs =====
using System.Runtime.CompilerServices;
using LeichtFrame.Core.Engine;

namespace LeichtFrame.Core
{
    /// <summary>
    /// Reader for fast streaming of Group Key / Count pairs from NativeGroupedData.
    /// </summary>
    public unsafe ref struct NativeGroupCountEnumerator
    {
        private int* _pKey;
        private int* _pNextOffset;
        private readonly int* _pEnd;
        private int _cachedStartOffset;

        internal NativeGroupCountEnumerator(NativeGroupedData data)
        {
            _pKey = data.Keys.Ptr;
            _pEnd = data.Keys.Ptr + data.GroupCount;
            // Caching Logik:
            _cachedStartOffset = data.Offsets.Ptr[0];
            _pNextOffset = data.Offsets.Ptr + 1;
        }

        /// <summary>
        /// Reads Key and Count in a single CPU step.
        /// Returns false if end is reached.
        /// </summary>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public bool Read(out int key, out int count)
        {
            // 1. Boundary Check
            if (_pKey >= _pEnd)
            {
                key = 0; count = 0;
                return false;
            }

            // 2. Fetch Data (Pointer Increment)
            key = *_pKey;
            _pKey++;

            int nextOffset = *_pNextOffset;
            _pNextOffset++;

            // 3. Calc Count (Register Math)
            count = nextOffset - _cachedStartOffset;
            _cachedStartOffset = nextOffset;

            return true;
        }
    }

    /// <summary>
    /// Extension Methods for GroupedDataFrame to access NativeGroupCountReader
    /// </summary>
    public static class GroupStreamingExtensions
    {
        /// <summary>
        /// Gets a NativeGroupCountReader for fast streaming of Group Key / Count pairs.
        /// </summary>
        /// <param name="gdf"></param>
        /// <returns></returns>
        /// <exception cref="InvalidOperationException"></exception>
        public static NativeGroupCountEnumerator GetCountReader(this GroupedDataFrame gdf)
        {
            if (gdf.NativeData == null) throw new InvalidOperationException("Slow Path!");
            return new NativeGroupCountEnumerator(gdf.NativeData);
        }
    }
}
===== FILE: src/LeichtFrame.Core/DataFrame/GroupedDataFrame.cs =====
using System.Runtime.InteropServices;
using LeichtFrame.Core.Engine;

namespace LeichtFrame.Core
{
    /// <summary>
    /// Represents the result of a GroupBy operation.
    /// This abstract base class holds references to the source data and manages the lifecycle of native resources.
    /// </summary>
    public abstract class GroupedDataFrame : IDisposable
    {
        /// <summary>
        /// Gets the source DataFrame that was grouped.
        /// </summary>
        public DataFrame Source { get; }

        /// <summary>
        /// Gets the names of the columns used for grouping.
        /// </summary>
        public string[] GroupColumnNames { get; }

        /// <summary>
        /// Internal reference to unmanaged memory containing the grouping results (CSR format).
        /// If this is not null, the "Fast Path" is active.
        /// </summary>
        internal NativeGroupedData? NativeData { get; set; }

        internal bool KeysAreRowIndices { get; set; } = false;

        /// <summary>
        /// Initializes a new instance of the <see cref="GroupedDataFrame"/> class.
        /// </summary>
        /// <param name="source">The source DataFrame.</param>
        /// <param name="groupColumnNames">The names of the grouping columns.</param>
        /// <param name="nativeData">Optional pointer to native data.</param>
        internal GroupedDataFrame(DataFrame source, string[] groupColumnNames, NativeGroupedData? nativeData)
        {
            Source = source;
            GroupColumnNames = groupColumnNames;
            NativeData = nativeData;
        }

        /// <summary>
        /// Gets the total number of groups found.
        /// </summary>
        public abstract int GroupCount { get; }

        /// <summary>
        /// Gets the group offsets for Compressed Sparse Row (CSR) iteration.
        /// </summary>
        public abstract int[] GroupOffsets { get; }

        /// <summary>
        /// Gets the row indices sorted by group (CSR Values).
        /// </summary>
        public abstract int[] RowIndices { get; }

        /// <summary>
        /// Gets the indices of rows belonging to the 'null' group, if any.
        /// </summary>
        public abstract int[]? NullGroupIndices { get; }

        /// <summary>
        /// Gets the unique keys of the groups as a generic Array.
        /// </summary>
        /// <returns>An array containing the group keys.</returns>
        public abstract Array GetKeys();

        /// <summary>
        /// Creates a shallow copy of this GroupedDataFrame attached to a new Source DataFrame.
        /// Used internally when injecting computed columns.
        /// </summary>
        internal GroupedDataFrame WithSource(DataFrame newSource)
        {
            // Case 1: Integer Keys (Fast Path or Slow Path)
            if (this is GroupedDataFrame<int> gInt)
            {
                if (NativeData != null)
                {
                    return new GroupedDataFrame<int>(
                        newSource,
                        GroupColumnNames,
                        NativeData,
                        gInt.NullGroupIndices
                    );
                }

                return new GroupedDataFrame<int>(
                    newSource,
                    GroupColumnNames,
                    (int[])gInt.GetKeys(),
                    gInt.GroupOffsets,
                    gInt.RowIndices,
                    gInt.NullGroupIndices
                );
            }

            // Case 2: String Keys
            if (this is GroupedDataFrame<string> gStr)
            {
                return new GroupedDataFrame<string>(
                    newSource,
                    GroupColumnNames,
                    (string[])gStr.GetKeys(),
                    gStr.GroupOffsets,
                    gStr.RowIndices,
                    gStr.NullGroupIndices
                );
            }

            throw new NotSupportedException($"Re-binding source not supported for key type of {this.GetType().Name}");
        }

        /// <summary>
        /// Releases unmanaged resources (NativeGroupedData) if they exist.
        /// </summary>
        public virtual void Dispose()
        {
            NativeData?.Dispose();
            NativeData = null;
            GC.SuppressFinalize(this);
        }
    }

    /// <summary>
    /// A strongly-typed implementation of <see cref="GroupedDataFrame"/> for a specific key type.
    /// Primarily used for single-column grouping strategies.
    /// </summary>
    /// <typeparam name="TKey">The type of the grouping key (e.g., int, string).</typeparam>
    public class GroupedDataFrame<TKey> : GroupedDataFrame
    {
        private TKey[]? _keys;
        private int[]? _groupOffsets;
        private int[]? _rowIndices;
        private readonly int[]? _nullGroupIndices;

        /// <summary>
        /// Initializes a new instance using unmanaged memory (Fast Path).
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="colNames">The column names.</param>
        /// <param name="nativeData">The unmanaged data structure.</param>
        /// <param name="nullIndices">Optional managed array for null indices.</param>
        internal GroupedDataFrame(
            DataFrame df,
            string[] colNames,
            NativeGroupedData nativeData,
            int[]? nullIndices = null)
            : base(df, colNames, nativeData)
        {
            // [FIX] Wir speichern die Null-Indizes auch im Fast Path
            _nullGroupIndices = nullIndices;
        }

        /// <summary>
        /// Initializes a new instance using managed arrays (Slow Path / Fallback).
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="colNames">The column names.</param>
        /// <param name="keys">The group keys.</param>
        /// <param name="offsets">The CSR offsets.</param>
        /// <param name="indices">The CSR row indices.</param>
        /// <param name="nullIndices">Optional indices for null values.</param>
        public GroupedDataFrame(DataFrame df, string[] colNames, TKey[] keys, int[] offsets, int[] indices, int[]? nullIndices)
            : base(df, colNames, null)
        {
            _keys = keys;
            _groupOffsets = offsets;
            _rowIndices = indices;
            _nullGroupIndices = nullIndices;
        }

        /// <inheritdoc />
        public override int GroupCount => NativeData?.GroupCount ?? _keys!.Length;

        /// <inheritdoc />
        public override int[] GroupOffsets
        {
            get
            {
                // Lazy loading: Copy from native memory only if requested as managed array.
                if (_groupOffsets == null && NativeData != null)
                {
                    int count = NativeData.GroupCount + 1;
                    _groupOffsets = new int[count];
                    unsafe
                    {
                        Marshal.Copy((nint)NativeData.Offsets.Ptr, _groupOffsets, 0, count);
                    }
                }
                return _groupOffsets!;
            }
        }

        /// <inheritdoc />
        public override int[] RowIndices
        {
            get
            {
                // Lazy loading: Copy from native memory only if requested.
                if (_rowIndices == null && NativeData != null)
                {
                    int count = NativeData.RowCount;
                    _rowIndices = new int[count];
                    unsafe
                    {
                        Marshal.Copy((nint)NativeData.Indices.Ptr, _rowIndices, 0, count);
                    }
                }
                return _rowIndices!;
            }
        }

        /// <inheritdoc />
        public override int[]? NullGroupIndices => _nullGroupIndices;

        /// <inheritdoc />
        public unsafe override Array GetKeys()
        {
            if (_keys == null && NativeData != null)
            {
                bool isMultiColumnRowIndex = KeysAreRowIndices && GroupColumnNames.Length > 1;

                if (isMultiColumnRowIndex || (typeof(TKey) == typeof(int) && !KeysAreRowIndices))
                {
                    if (typeof(TKey) == typeof(int))
                    {
                        int count = NativeData.GroupCount;
                        int[] intKeys = new int[count];
                        unsafe
                        {
                            Marshal.Copy((nint)NativeData.Keys.Ptr, intKeys, 0, count);
                        }
                        _keys = (TKey[])(object)intKeys;
                    }
                    else
                    {
                        throw new InvalidOperationException("Multi-Column Grouping with NativeData requires GroupedDataFrame<int>.");
                    }
                }
                else if (KeysAreRowIndices)
                {
                    int count = NativeData.GroupCount;
                    int* ptrKeys = NativeData.Keys.Ptr;

                    var resultKeys = new TKey[count];
                    var sourceCol = (IColumn<TKey>)Source[GroupColumnNames[0]];

                    unsafe
                    {
                        for (int i = 0; i < count; i++)
                        {
                            resultKeys[i] = sourceCol.GetValue(ptrKeys[i]);
                        }
                    }
                    _keys = resultKeys;
                }
                else
                {
                    throw new InvalidOperationException($"NativeData present but Type {typeof(TKey).Name} logic undefined.");
                }
            }
            return _keys!;
        }
    }
}
===== FILE: src/LeichtFrame.Core/DataFrame/GroupedLazyFrame.cs =====
using LeichtFrame.Core.Expressions;
using LeichtFrame.Core.Plans;

namespace LeichtFrame.Core
{
    /// <summary>
    /// Represents an intermediate state of a LazyDataFrame after a GroupBy operation.
    /// Allows defining aggregations to finish the logical plan node.
    /// </summary>
    public class GroupedLazyFrame
    {
        private readonly LazyDataFrame _parent;
        private readonly List<Expr> _groupKeys;

        internal GroupedLazyFrame(LazyDataFrame parent, IEnumerable<Expr> groupKeys)
        {
            _parent = parent;
            _groupKeys = groupKeys.ToList();
        }

        /// <summary>
        /// Defines the aggregations to perform on the grouped data.
        /// </summary>
        /// <param name="aggregations">A list of aggregation expressions (e.g. Sum, Count).</param>
        /// <returns>A new LazyDataFrame containing the result.</returns>
        public LazyDataFrame Agg(params Expr[] aggregations)
        {
            var node = new Aggregate(_parent.Plan, _groupKeys, aggregations.ToList());
            return new LazyDataFrame(node);
        }
    }
}
===== FILE: src/LeichtFrame.Core/DataFrame/LazyDataFrame.cs =====
using LeichtFrame.Core.Expressions;
using LeichtFrame.Core.Plans;
using LeichtFrame.Core.Execution;
using LeichtFrame.Core.Optimizer;

namespace LeichtFrame.Core
{
    /// <summary>
    /// Represents a lazy DataFrame builder. 
    /// Operations recorded on this object are not executed until <see cref="Collect"/> is called.
    /// </summary>
    public class LazyDataFrame
    {
        /// <summary>
        /// Gets the current logical plan of transformations.
        /// </summary>
        public LogicalPlan Plan { get; }

        internal LazyDataFrame(LogicalPlan plan)
        {
            Plan = plan;
        }

        /// <summary>
        /// Creates a LazyDataFrame starting from an existing materialized DataFrame.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <returns>A new LazyDataFrame instance.</returns>
        public static LazyDataFrame From(DataFrame df)
        {
            return new LazyDataFrame(new DataFrameScan(df));
        }

        // --- Lazy Operations ---

        /// <summary>
        /// Filters the rows based on the provided boolean expression.
        /// </summary>
        public LazyDataFrame Where(Expr predicate)
        {
            return new LazyDataFrame(new Filter(Plan, predicate));
        }

        /// <summary>
        /// Projects the DataFrame to the specified expressions.
        /// </summary>
        public LazyDataFrame Select(params Expr[] exprs)
        {
            return new LazyDataFrame(new Projection(Plan, exprs.ToList()));
        }

        /// <summary>
        /// Projects the DataFrame to the specified columns by name.
        /// </summary>
        public LazyDataFrame Select(params string[] cols)
        {
            var exprs = cols.Select(c => new ColExpr(c)).Cast<Expr>().ToArray();
            return Select(exprs);
        }

        /// <summary>
        /// Internal method to create an Aggregate node directly.
        /// </summary>
        public LazyDataFrame Aggregate(Expr[] groupByCols, Expr[] aggregations)
        {
            return new LazyDataFrame(new Aggregate(Plan, groupByCols.ToList(), aggregations.ToList()));
        }

        /// <summary>
        /// Joins this DataFrame with another LazyDataFrame.
        /// </summary>
        public LazyDataFrame Join(LazyDataFrame other, string on, JoinType type = JoinType.Inner)
        {
            return new LazyDataFrame(new Join(Plan, other.Plan, on, on, type));
        }

        /// <summary>
        /// Sorts by one or more columns (Ascending).
        /// </summary>
        public LazyDataFrame OrderBy(params string[] columns)
        {
            var sortDefs = columns.Select(name => (name, true)).ToList();
            return new LazyDataFrame(new Sort(Plan, sortDefs));
        }

        /// <summary>
        /// Sorts by one or more columns (Descending).
        /// </summary>
        public LazyDataFrame OrderByDescending(params string[] columns)
        {
            var sortDefs = columns.Select(name => (name, false)).ToList();
            return new LazyDataFrame(new Sort(Plan, sortDefs));
        }

        // --- Materialization ---

        /// <summary>
        /// Optimizes and executes the logical plan, returning the result as a materialized DataFrame.
        /// </summary>
        public DataFrame Collect()
        {
            var optimizer = new OptimizerEngine();
            var optimizedPlan = optimizer.Optimize(Plan);

            var physicalPlanner = new PhysicalPlanner();
            return physicalPlanner.Execute(optimizedPlan);
        }

        // --- Aggregation ---

        /// <summary>
        /// Groups the DataFrame by the specified columns.
        /// Returns a <see cref="GroupedLazyFrame"/> to define aggregations via .Agg(...).
        /// </summary>
        public GroupedLazyFrame GroupBy(params Expr[] cols)
        {
            return new GroupedLazyFrame(this, cols);
        }

        /// <summary>
        /// Groups the DataFrame by the specified column names.
        /// Returns a <see cref="GroupedLazyFrame"/> to define aggregations via .Agg(...).
        /// </summary>
        public GroupedLazyFrame GroupBy(params string[] cols)
        {
            var exprs = cols.Select(c => new ColExpr(c)).Cast<Expr>();
            return new GroupedLazyFrame(this, exprs);
        }

        // --- Streaming (API) ---

        /// <summary>
        /// Executes the plan and returns a streaming iterator over the results.
        /// This avoids materializing the full result DataFrame in memory, which is ideal for large aggregations.
        /// </summary>
        public IEnumerable<RowView> CollectStream()
        {
            var optimizer = new OptimizerEngine();
            var optimizedPlan = optimizer.Optimize(Plan);

            return PhysicalStreamer.Execute(optimizedPlan);
        }
    }
}
===== FILE: src/LeichtFrame.Core/DataFrame/RowView.cs =====
namespace LeichtFrame.Core
{
    /// <summary>
    /// Represents a lightweight, read-only view of a single row in a <see cref="DataFrame"/>.
    /// Acts as a zero-copy cursor enabling row-based operations without materializing objects.
    /// </summary>
    public readonly struct RowView
    {
        private readonly int _rowIndex;
        private readonly IReadOnlyList<IColumn> _columns;
        private readonly DataFrameSchema _schema;

        /// <summary>
        /// Initializes a new instance of the <see cref="RowView"/> struct.
        /// </summary>
        /// <param name="rowIndex">The zero-based index of the row.</param>
        /// <param name="columns">The list of columns backing the data.</param>
        /// <param name="schema">The schema definition for column name lookups.</param>
        /// <exception cref="ArgumentOutOfRangeException">Thrown if rowIndex is negative.</exception>
        /// <exception cref="ArgumentNullException">Thrown if columns or schema are null.</exception>
        public RowView(int rowIndex, IReadOnlyList<IColumn> columns, DataFrameSchema schema)
        {
            if (rowIndex < 0) throw new ArgumentOutOfRangeException(nameof(rowIndex));
            _rowIndex = rowIndex;
            _columns = columns ?? throw new ArgumentNullException(nameof(columns));
            _schema = schema ?? throw new ArgumentNullException(nameof(schema));
        }

        /// <summary>
        /// Gets the strongly-typed value from the specified column index.
        /// This is the fastest way to access data within a row.
        /// </summary>
        /// <typeparam name="T">The expected type of the value.</typeparam>
        /// <param name="columnIndex">The zero-based index of the column.</param>
        /// <returns>The value of type T.</returns>
        /// <exception cref="InvalidCastException">Thrown if the column type does not match T.</exception>
        public T Get<T>(int columnIndex)
        {
            var col = _columns[columnIndex];

            // Pattern matching on generic interface
            if (col is IColumn<T> typedCol)
            {
                return typedCol.GetValue(_rowIndex);
            }

            throw new InvalidCastException(
                $"Column '{col.Name}' is type {col.DataType.Name}, but '{typeof(T).Name}' was requested.");
        }

        /// <summary>
        /// Gets the strongly-typed value from the column with the specified name.
        /// </summary>
        /// <typeparam name="T">The expected type of the value.</typeparam>
        /// <param name="columnName">The name of the column.</param>
        /// <returns>The value of type T.</returns>
        /// <exception cref="ArgumentException">Thrown if the column name does not exist.</exception>
        public T Get<T>(string columnName)
        {
            int index = _schema.GetColumnIndex(columnName);
            return Get<T>(index);
        }

        /// <summary>
        /// Gets the value at the specified column index as an object (boxed).
        /// </summary>
        /// <param name="columnIndex">The zero-based index of the column.</param>
        /// <returns>The value as an object, or null.</returns>
        public object? GetValue(int columnIndex)
        {
            return _columns[columnIndex].GetValue(_rowIndex);
        }

        // Indexer for convenience

        /// <summary>
        /// Gets the value at the specified column index (untyped).
        /// </summary>
        /// <param name="index">The zero-based column index.</param>
        public object? this[int index] => GetValue(index);

        /// <summary>
        /// Gets the value of the column with the specified name (untyped).
        /// </summary>
        /// <param name="name">The name of the column.</param>
        public object? this[string name] => GetValue(_schema.GetColumnIndex(name));

        /// <summary>
        /// Checks if the value at the specified column index is null.
        /// </summary>
        public bool IsNull(int columnIndex)
        {
            return _columns[columnIndex].IsNull(_rowIndex);
        }

        /// <summary>
        /// Checks if the value at the specified column name is null.
        /// </summary>
        public bool IsNull(string columnName)
        {
            int index = _schema.GetColumnIndex(columnName);
            return IsNull(index);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Algorithms/Hashing/VectorizedHasher.cs =====
using System.Runtime.CompilerServices;
using System.Runtime.Intrinsics;
using System.Runtime.Intrinsics.X86;

namespace LeichtFrame.Core.Engine
{
    internal static unsafe class VectorizedHasher
    {
        private const int C1 = unchecked((int)0x85ebca6b);
        private const int C2 = unchecked((int)0xc2b2ae35);

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public static void HashIntegers(ReadOnlySpan<int> input, Span<int> hashes)
        {
            fixed (int* pInput = input)
            fixed (int* pHashes = hashes)
            {
                int len = input.Length;
                int i = 0;

                if (Avx2.IsSupported && len >= 8)
                {
                    Vector256<int> vC1 = Vector256.Create(C1);
                    Vector256<int> vC2 = Vector256.Create(C2);

                    int loopLimit = len - 8;

                    for (; i <= loopLimit; i += 8)
                    {
                        Vector256<int> k = Avx2.LoadVector256(pInput + i);

                        k = Avx2.Xor(k, Avx2.ShiftRightLogical(k, 16));

                        k = Avx2.MultiplyLow(k, vC1);

                        k = Avx2.Xor(k, Avx2.ShiftRightLogical(k, 13));

                        k = Avx2.MultiplyLow(k, vC2);

                        k = Avx2.Xor(k, Avx2.ShiftRightLogical(k, 16));

                        Avx2.Store(pHashes + i, k);
                    }
                }

                for (; i < len; i++)
                {
                    pHashes[i] = MixScalar(pInput[i]);
                }
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveOptimization)]
        public static void HashStrings(
            byte* globalBytes,
            int* offsets,
            int* hashes,
            int count)
        {
            const int FnvOffsetBasis = unchecked((int)2166136261);
            const int FnvPrime = 16777619;

            Parallel.For(0, count, i =>
            {
                int start = offsets[i];
                int end = offsets[i + 1];
                int len = end - start;

                int hash = FnvOffsetBasis;

                byte* pStr = globalBytes + start;

                for (int k = 0; k < len; k++)
                {
                    hash ^= pStr[k];
                    hash *= FnvPrime;
                }

                hashes[i] = hash;
            });
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private static int MixScalar(int k)
        {
            k ^= (int)((uint)k >> 16);
            k *= C1;
            k ^= (int)((uint)k >> 13);
            k *= C2;
            k ^= (int)((uint)k >> 16);
            return k;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Algorithms/Numeric/VectorizedMathOps.cs =====
using System.Numerics;
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Engine
{
    internal static class VectorizedMathOps
    {
        public enum MathOp { Add, Subtract, Multiply, Divide }

        public static void Calculate<T>(ReadOnlySpan<T> a, ReadOnlySpan<T> b, Span<T> result, MathOp op)
            where T : struct, INumber<T>
        {
            int i = 0;
            if (Vector.IsHardwareAccelerated)
            {
                int vecSize = Vector<T>.Count;
                var vecA = MemoryMarshal.Cast<T, Vector<T>>(a);
                var vecB = MemoryMarshal.Cast<T, Vector<T>>(b);
                var vecRes = MemoryMarshal.Cast<T, Vector<T>>(result);

                int limit = Math.Min(vecA.Length, vecB.Length);

                for (int v = 0; v < limit; v++)
                {
                    vecRes[v] = op switch
                    {
                        MathOp.Add => vecA[v] + vecB[v],
                        MathOp.Subtract => vecA[v] - vecB[v],
                        MathOp.Multiply => vecA[v] * vecB[v],
                        MathOp.Divide => vecA[v] / vecB[v],
                        _ => throw new NotSupportedException()
                    };
                }
                i = limit * vecSize;
            }

            // Tail Loop
            for (; i < a.Length; i++)
            {
                result[i] = op switch
                {
                    MathOp.Add => a[i] + b[i],
                    MathOp.Subtract => a[i] - b[i],
                    MathOp.Multiply => a[i] * b[i],
                    MathOp.Divide => a[i] / b[i],
                    _ => throw new NotSupportedException()
                };
            }
        }

        // Overload f√ºr Scalar (Column + 5)
        public static void CalculateScalar<T>(ReadOnlySpan<T> a, T scalar, Span<T> result, MathOp op)
            where T : struct, INumber<T>
        {
            int i = 0;
            if (Vector.IsHardwareAccelerated)
            {
                int vecSize = Vector<T>.Count;
                var vecA = MemoryMarshal.Cast<T, Vector<T>>(a);
                var vecRes = MemoryMarshal.Cast<T, Vector<T>>(result);
                var vecScalar = new Vector<T>(scalar);

                int limit = vecA.Length;

                for (int v = 0; v < limit; v++)
                {
                    vecRes[v] = op switch
                    {
                        MathOp.Add => vecA[v] + vecScalar,
                        MathOp.Subtract => vecA[v] - vecScalar,
                        MathOp.Multiply => vecA[v] * vecScalar,
                        MathOp.Divide => vecA[v] / vecScalar,
                        _ => throw new NotSupportedException()
                    };
                }
                i = limit * vecSize;
            }

            for (; i < a.Length; i++)
            {
                result[i] = op switch
                {
                    MathOp.Add => a[i] + scalar,
                    MathOp.Subtract => a[i] - scalar,
                    MathOp.Multiply => a[i] * scalar,
                    MathOp.Divide => a[i] / scalar,
                    _ => throw new NotSupportedException()
                };
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Algorithms/Packing/RowLayoutPacking.cs =====
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Engine.Algorithms.Packing
{
    internal static unsafe class RowLayoutPacking
    {
        /// <summary>
        /// Packs multiple columns into a contiguous byte buffer (Row Layout).
        /// Format per Column: [NullFlag (1 Byte)] [Data (N Bytes)]
        /// </summary>
        /// <returns>Tuple of (BufferPtr, RowWidth)</returns>
        public static (IntPtr Buffer, int Width) Pack(DataFrame df, string[] cols)
        {
            int rowCount = df.RowCount;
            int width = 0;
            var types = new Type[cols.Length];
            var columnData = new IColumn[cols.Length];

            for (int i = 0; i < cols.Length; i++)
            {
                var col = df[cols[i]];
                columnData[i] = col;
                types[i] = Nullable.GetUnderlyingType(col.DataType) ?? col.DataType;

                int dataSize = GetSize(types[i]);
                if (dataSize == 0)
                    throw new NotSupportedException($"Packing not supported for type {types[i].Name}");

                width += dataSize + 1;
            }

            byte* buffer = (byte*)NativeMemory.Alloc((nuint)(rowCount * width));

            int offset = 0;

            for (int c = 0; c < cols.Length; c++)
            {
                PackColumn(columnData[c], buffer, rowCount, width, offset);

                int size = GetSize(types[c]);
                offset += size + 1;
            }

            return ((IntPtr)buffer, width);
        }

        private static void PackColumn(IColumn col, byte* buffer, int rows, int stride, int offset)
        {
            // --- INT ---
            if (col is IntColumn ic)
            {
                var span = ic.Values.Span;
                for (int i = 0; i < rows; i++)
                {
                    byte* ptr = buffer + (i * stride) + offset;
                    if (ic.IsNull(i))
                    {
                        *ptr = 1;
                        *(int*)(ptr + 1) = 0;
                    }
                    else
                    {
                        *ptr = 0;
                        *(int*)(ptr + 1) = span[i];
                    }
                }
            }
            // --- DOUBLE ---
            else if (col is DoubleColumn dc)
            {
                var span = dc.Values.Span;
                for (int i = 0; i < rows; i++)
                {
                    byte* ptr = buffer + (i * stride) + offset;
                    if (dc.IsNull(i))
                    {
                        *ptr = 1;
                        *(double*)(ptr + 1) = 0;
                    }
                    else
                    {
                        *ptr = 0;
                        *(double*)(ptr + 1) = span[i];
                    }
                }
            }
            // --- BOOL ---
            else if (col is BoolColumn bc)
            {
                for (int i = 0; i < rows; i++)
                {
                    byte* ptr = buffer + (i * stride) + offset;
                    if (bc.IsNull(i))
                    {
                        *ptr = 1;
                        *(byte*)(ptr + 1) = 0;
                    }
                    else
                    {
                        *ptr = 0;
                        *(byte*)(ptr + 1) = bc.Get(i) ? (byte)1 : (byte)0;
                    }
                }
            }
            // --- DATE TIME ---
            else if (col is DateTimeColumn dtc)
            {
                var span = dtc.Values.Span;
                for (int i = 0; i < rows; i++)
                {
                    byte* ptr = buffer + (i * stride) + offset;
                    if (dtc.IsNull(i))
                    {
                        *ptr = 1;
                        *(long*)(ptr + 1) = 0;
                    }
                    else
                    {
                        *ptr = 0;
                        *(long*)(ptr + 1) = span[i].Ticks;
                    }
                }
            }
            // --- LONG (Generic Fallback) ---
            else if (col is IColumn<long> lc)
            {
                for (int i = 0; i < rows; i++)
                {
                    byte* ptr = buffer + (i * stride) + offset;
                    if (lc.IsNull(i))
                    {
                        *ptr = 1;
                        *(long*)(ptr + 1) = 0;
                    }
                    else
                    {
                        *ptr = 0;
                        *(long*)(ptr + 1) = lc.GetValue(i);
                    }
                }
            }
            else
            {
                throw new NotSupportedException($"PackColumn not implemented for {col.GetType().Name}");
            }
        }

        private static int GetSize(Type t)
        {
            if (t == typeof(int)) return 4;
            if (t == typeof(double)) return 8;
            if (t == typeof(bool)) return 1;
            if (t == typeof(long)) return 8;
            if (t == typeof(DateTime)) return 8;
            return 0;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Algorithms/Partitioning/PartitionedHistogram.cs =====
using System.Runtime.CompilerServices;

namespace LeichtFrame.Core.Engine
{
    internal static unsafe class PartitionedHistogram
    {
        [MethodImpl(MethodImplOptions.AggressiveOptimization)]
        public static void ComputeHistograms(
            int* pInput,
            int* pGlobalHistograms,
            int length,
            int min,
            int bucketCount,
            int numThreads)
        {
            int chunkSize = length / numThreads;

            Parallel.For(0, numThreads, t =>
            {
                int start = t * chunkSize;
                int end = (t == numThreads - 1) ? length : start + chunkSize;
                int* pLocalHist = pGlobalHistograms + (t * bucketCount);

                int i = start;

                // UNROLLING: 4x per Loop (reduces Branch Prediction Overhead)
                int endUnroll = end - 4;
                while (i < endUnroll)
                {
                    pLocalHist[pInput[i] - min]++;
                    pLocalHist[pInput[i + 1] - min]++;
                    pLocalHist[pInput[i + 2] - min]++;
                    pLocalHist[pInput[i + 3] - min]++;
                    i += 4;
                }

                // Tail
                while (i < end)
                {
                    pLocalHist[pInput[i] - min]++;
                    i++;
                }
            });
        }

        [MethodImpl(MethodImplOptions.AggressiveOptimization)]
        public static void ScatterIndices(
            int* pInput,
            int* pFinalIndices, // Goal
            int* pWriteOffsets, // Cache
            int length,
            int min,
            int bucketCount,
            int numThreads)
        {
            int chunkSize = length / numThreads;

            Parallel.For(0, numThreads, t =>
            {
                int start = t * chunkSize;
                int end = (t == numThreads - 1) ? length : start + chunkSize;
                int* pLocalWriteOffsets = pWriteOffsets + (t * bucketCount);

                int i = start;

                int endUnroll = end - 4;
                while (i < endUnroll)
                {
                    // Item 1
                    int val1 = pInput[i];
                    int b1 = val1 - min;
                    int dest1 = pLocalWriteOffsets[b1];
                    pFinalIndices[dest1] = i;
                    pLocalWriteOffsets[b1]++;

                    // Item 2
                    int val2 = pInput[i + 1];
                    int b2 = val2 - min;
                    int dest2 = pLocalWriteOffsets[b2];
                    pFinalIndices[dest2] = i + 1;
                    pLocalWriteOffsets[b2]++;

                    // Item 3
                    int val3 = pInput[i + 2];
                    int b3 = val3 - min;
                    int dest3 = pLocalWriteOffsets[b3];
                    pFinalIndices[dest3] = i + 2;
                    pLocalWriteOffsets[b3]++;

                    // Item 4
                    int val4 = pInput[i + 3];
                    int b4 = val4 - min;
                    int dest4 = pLocalWriteOffsets[b4];
                    pFinalIndices[dest4] = i + 3;
                    pLocalWriteOffsets[b4]++;

                    i += 4;
                }

                while (i < end)
                {
                    int val = pInput[i];
                    int bucketIdx = val - min;
                    int destIdx = pLocalWriteOffsets[bucketIdx];
                    pFinalIndices[destIdx] = i;
                    pLocalWriteOffsets[bucketIdx]++;
                    i++;
                }
            });
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Algorithms/Partitioning/RadixPartitioner.cs =====
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Engine.Algorithms.Partitioning
{
    internal static unsafe class RadixPartitioner
    {
        public static void Partition(
            int* pHashes,
            int rowCount,
            int partitionCount,
            int shift,
            out int* pOutHashes,
            out int* pOutRowIndices,
            int[] partitionOffsets)
        {
            // Alloc Output
            pOutHashes = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));
            pOutRowIndices = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));

            int numThreads = Environment.ProcessorCount;
            int chunkSize = (rowCount + numThreads - 1) / numThreads;

            // --- STEP 1: Thread-Local Histograms ---
            int* histograms = (int*)NativeMemory.AllocZeroed((nuint)(numThreads * partitionCount * sizeof(int)));

            int* localPtrHashes = pHashes;

            Parallel.For(0, numThreads, t =>
            {
                int start = t * chunkSize;
                int end = Math.Min(start + chunkSize, rowCount);
                int* localHist = histograms + (t * partitionCount);

                for (int i = start; i < end; i++)
                {
                    int p = (int)((uint)localPtrHashes[i] >> shift);
                    localHist[p]++;
                }
            });

            // --- STEP 2: Prefix Sum ---
            int* writeOffsets = (int*)NativeMemory.Alloc((nuint)(numThreads * partitionCount * sizeof(int)));
            int currentGlobal = 0;

            for (int p = 0; p < partitionCount; p++)
            {
                partitionOffsets[p] = currentGlobal;
                for (int t = 0; t < numThreads; t++)
                {
                    int count = histograms[t * partitionCount + p];
                    writeOffsets[t * partitionCount + p] = currentGlobal;
                    currentGlobal += count;
                }
            }
            partitionOffsets[partitionCount] = currentGlobal;

            // --- STEP 3: Scatter ---
            int* localPtrOutHashes = pOutHashes;
            int* localPtrOutIndices = pOutRowIndices;

            Parallel.For(0, numThreads, t =>
            {
                int start = t * chunkSize;
                int end = Math.Min(start + chunkSize, rowCount);
                int* localOffsets = writeOffsets + (t * partitionCount);

                for (int i = start; i < end; i++)
                {
                    int hash = localPtrHashes[i];
                    int p = (int)((uint)hash >> shift);

                    int dest = localOffsets[p];

                    localPtrOutHashes[dest] = hash;
                    localPtrOutIndices[dest] = i;

                    localOffsets[p]++;
                }
            });

            NativeMemory.Free(histograms);
            NativeMemory.Free(writeOffsets);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Algorithms/Sorting/SimdRadixSorter.cs =====
using System.Buffers;
using System.Runtime.Intrinsics;
using System.Runtime.Intrinsics.X86;

namespace LeichtFrame.Core.Engine
{
    internal static unsafe class SimdRadixSorter
    {
        private const int RadixBits = 11;
        private const int BucketCount = 1 << RadixBits; // 2048
        private const int Mask = BucketCount - 1;

        /// <summary>
        /// Sorts the indices based on the provided hash values using a SIMD-optimized Radix Sort algorithm.
        /// </summary>
        public static void Sort(int* pHashes, int* pIndices, int* pTempIndices, int length)
        {
            int parallelism = Environment.ProcessorCount;

            // 1. Init
            InitializeIndicesSIMD(pIndices, length);

            // 2. Histograms
            int[][] histograms = new int[parallelism][];
            for (int i = 0; i < parallelism; i++)
            {
                histograms[i] = ArrayPool<int>.Shared.Rent(BucketCount);
            }

            try
            {
                // Ranges
                var ranges = new Tuple<int, int>[parallelism];
                int chunkSize = length / parallelism;
                int remainder = length % parallelism;
                int currentStart = 0;

                for (int i = 0; i < parallelism; i++)
                {
                    int end = currentStart + chunkSize + (i < remainder ? 1 : 0);
                    ranges[i] = Tuple.Create(currentStart, end);
                    currentStart = end;
                }

                // 3. P√§sse (0, 11, 22 bit shift)
                ExecuteRadixPass(pHashes, pIndices, pTempIndices, histograms, ranges, 0);
                ExecuteRadixPass(pHashes, pTempIndices, pIndices, histograms, ranges, 11);
                ExecuteRadixPass(pHashes, pIndices, pTempIndices, histograms, ranges, 22);
            }
            finally
            {
                for (int i = 0; i < parallelism; i++)
                {
                    if (histograms[i] != null) ArrayPool<int>.Shared.Return(histograms[i]);
                }
            }
        }

        private static void InitializeIndicesSIMD(int* pIndices, int length)
        {
            if (Avx2.IsSupported)
            {
                int parallelism = Environment.ProcessorCount;
                int chunkSize = length / parallelism;

                Parallel.For(0, parallelism, p =>
                {
                    int start = p * chunkSize;
                    int end = (p == parallelism - 1) ? length : start + chunkSize;
                    int* ptr = pIndices + start;
                    int i = start;

                    Vector256<int> vIndex = Vector256.Create(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);
                    Vector256<int> vIncrement = Vector256.Create(8);
                    int simdEnd = start + ((end - start) & ~7);

                    while (i < simdEnd)
                    {
                        Avx2.Store(ptr, vIndex);
                        vIndex = Avx2.Add(vIndex, vIncrement);
                        ptr += 8;
                        i += 8;
                    }
                    while (i < end) *ptr++ = i++;
                });
            }
            else
            {
                Parallel.For(0, length, i => pIndices[i] = i);
            }
        }

        private static void ExecuteRadixPass(
            int* pHashes,
            int* pSourceIndices,
            int* pDestIndices,
            int[][] histograms,
            Tuple<int, int>[] ranges,
            int shift)
        {
            int parallelism = histograms.Length;

            // 1. Histogram
            Parallel.For(0, parallelism, p =>
            {
                int[] localCounts = histograms[p];
                Array.Clear(localCounts, 0, BucketCount);
                int start = ranges[p].Item1;
                int end = ranges[p].Item2;

                fixed (int* pCounts = localCounts)
                {
                    for (int i = start; i < end; i++)
                    {
                        int idx = pSourceIndices[i];
                        int hash = pHashes[idx];
                        int bucket = (int)((uint)hash >> shift) & Mask;
                        pCounts[bucket]++;
                    }
                }
            });

            // 2. Prefix Sum
            int globalOffset = 0;
            for (int b = 0; b < BucketCount; b++)
            {
                for (int p = 0; p < parallelism; p++)
                {
                    int count = histograms[p][b];
                    histograms[p][b] = globalOffset;
                    globalOffset += count;
                }
            }

            // 3. Scatter
            Parallel.For(0, parallelism, p =>
            {
                int[] localOffsets = histograms[p];
                int start = ranges[p].Item1;
                int end = ranges[p].Item2;

                fixed (int* pOffsets = localOffsets)
                {
                    for (int i = start; i < end; i++)
                    {
                        int idx = pSourceIndices[i];
                        int hash = pHashes[idx];
                        int bucket = (int)((uint)hash >> shift) & Mask;
                        int destPos = pOffsets[bucket]++;
                        pDestIndices[destPos] = idx;
                    }
                }
            });
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Collections/NativeIntMap.cs =====
using System.Numerics;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using System.Runtime.Intrinsics;
using System.Runtime.Intrinsics.X86;

namespace LeichtFrame.Core.Engine.Collections
{
    internal unsafe struct NativeIntMap : IDisposable
    {
        private byte* _ctrl;

        private int* _keys;
        private int* _groupIds;

        private int _capacity;
        private int _mask;
        private int _count;
        private int _resizeThreshold;

        private const byte Empty = 0;
        private const int GroupSize = 32;

        public int Count => _count;
        public int Capacity => _capacity;

        public NativeIntMap(int initialCapacity)
        {
            _capacity = NextPowerOfTwo(Math.Max(GroupSize, initialCapacity));
            _mask = _capacity - 1;
            _resizeThreshold = (int)(_capacity * 0.75f);
            _count = 0;

            nuint sizeCtrl = (nuint)(_capacity + GroupSize);
            nuint sizeData = (nuint)(_capacity * sizeof(int));

            _ctrl = (byte*)NativeMemory.AllocZeroed(sizeCtrl);

            _keys = (int*)NativeMemory.Alloc(sizeData);
            _groupIds = (int*)NativeMemory.Alloc(sizeData);
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public int GetOrAdd(int key)
        {
            if (_count >= _resizeThreshold) Resize();

            int hash = Hash(key);

            byte h2 = (byte)((hash & 0x7F) + 1);

            int idx = hash & _mask;

            while (true)
            {
                Vector256<byte> ctrlGroup = Vector256.Load(_ctrl + idx);

                Vector256<byte> matchH2 = Vector256.Equals(ctrlGroup, Vector256.Create(h2));

                Vector256<byte> matchEmpty = Vector256.Equals(ctrlGroup, Vector256.Create(Empty));

                int mask = Avx2.MoveMask(matchH2 | matchEmpty);

                while (mask != 0)
                {
                    int bitPos = BitOperations.TrailingZeroCount(mask);

                    int realIdx = (idx + bitPos) & _mask;

                    byte foundCtrl = _ctrl[realIdx];

                    if (foundCtrl == Empty)
                    {
                        _ctrl[realIdx] = h2;
                        _keys[realIdx] = key;

                        int newGroupId = _count;
                        _groupIds[realIdx] = newGroupId;
                        _count++;

                        if (realIdx < GroupSize)
                        {
                            _ctrl[realIdx + _capacity] = h2;
                        }

                        return newGroupId;
                    }
                    else if (foundCtrl == h2)
                    {
                        if (_keys[realIdx] == key)
                        {
                            return _groupIds[realIdx];
                        }
                    }

                    mask &= ~(1 << bitPos);
                }

                idx += GroupSize;

                if (idx >= _capacity)
                {
                    idx -= _capacity;
                }
            }
        }

        [MethodImpl(MethodImplOptions.NoInlining)]
        private void Resize()
        {
            int oldCap = _capacity;
            int newCap = oldCap * 2;
            int newMask = newCap - 1;

            byte* oldCtrl = _ctrl;
            int* oldKeys = _keys;
            int* oldIds = _groupIds;

            nuint sizeCtrl = (nuint)(newCap + GroupSize);
            nuint sizeData = (nuint)(newCap * sizeof(int));

            _ctrl = (byte*)NativeMemory.AllocZeroed(sizeCtrl);
            _keys = (int*)NativeMemory.Alloc(sizeData);
            _groupIds = (int*)NativeMemory.Alloc(sizeData);

            _capacity = newCap;
            _mask = newMask;
            _resizeThreshold = (int)(newCap * 0.75f);

            for (int i = 0; i < oldCap; i++)
            {
                if (oldCtrl[i] != Empty)
                {
                    int key = oldKeys[i];
                    int id = oldIds[i];

                    int hash = Hash(key);
                    byte h2 = (byte)((hash & 0x7F) + 1);
                    int idx = hash & newMask;

                    while (true)
                    {
                        if (_ctrl[idx] == Empty)
                        {
                            _ctrl[idx] = h2;
                            _keys[idx] = key;
                            _groupIds[idx] = id;

                            if (idx < GroupSize) _ctrl[idx + newCap] = h2;

                            break;
                        }
                        idx = (idx + 1) & newMask;
                    }
                }
            }

            NativeMemory.Free(oldCtrl);
            NativeMemory.Free(oldKeys);
            NativeMemory.Free(oldIds);
        }

        public int[] ExportKeys()
        {
            var result = new int[_count];
            fixed (int* pRes = result)
            {
                for (int i = 0; i < _capacity; i++)
                {
                    if (_ctrl[i] != Empty)
                    {
                        int id = _groupIds[i];
                        pRes[id] = _keys[i];
                    }
                }
            }
            return result;
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private static int Hash(int k)
        {
            k ^= k >> 16;
            k *= unchecked((int)0x85ebca6b);
            k ^= k >> 13;
            k *= unchecked((int)0xc2b2ae35);
            k ^= k >> 16;
            return k;
        }

        private static int NextPowerOfTwo(int x)
        {
            return (int)BitOperations.RoundUpToPowerOf2((uint)x);
        }

        public void Dispose()
        {
            if (_ctrl != null) { NativeMemory.Free(_ctrl); _ctrl = null; }
            if (_keys != null) { NativeMemory.Free(_keys); _keys = null; }
            if (_groupIds != null) { NativeMemory.Free(_groupIds); _groupIds = null; }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Collections/NativeRowMap.cs =====
using System.Numerics;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using System.Runtime.Intrinsics;
using System.Runtime.Intrinsics.X86;

namespace LeichtFrame.Core.Engine.Collections
{
    /// <summary>
    /// Specialized Swiss Table for Fixed-Width Byte Rows (Multi-Column Keys).
    /// </summary>
    internal unsafe struct NativeRowMap : IDisposable
    {
        private byte* _ctrl;
        private int* _rowIndices;
        private int* _groupIds;

        private int _capacity;
        private int _mask;
        private int _count;
        private int _resizeThreshold;

        private readonly byte* _packedRows;
        private readonly int _rowWidthBytes;

        private const byte Empty = 0;
        private const int GroupSize = 32;

        public int Count => _count;

        public NativeRowMap(int initialCapacity, byte* packedRows, int rowWidthBytes)
        {
            _capacity = NextPowerOfTwo(Math.Max(GroupSize, initialCapacity));
            _mask = _capacity - 1;
            _resizeThreshold = (int)(_capacity * 0.75f);
            _count = 0;
            _packedRows = packedRows;
            _rowWidthBytes = rowWidthBytes;

            AllocateMemory(_capacity);
        }

        private void AllocateMemory(int cap)
        {
            _ctrl = (byte*)NativeMemory.AllocZeroed((nuint)(cap + GroupSize));
            _rowIndices = (int*)NativeMemory.Alloc((nuint)(cap * sizeof(int)));
            _groupIds = (int*)NativeMemory.Alloc((nuint)(cap * sizeof(int)));
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public int GetOrAdd(int rowIndex, int hash)
        {
            if (_count >= _resizeThreshold) Resize();

            byte h2 = (byte)((hash & 0x7F) + 1);
            int idx = hash & _mask;

            byte* pKey = _packedRows + (rowIndex * _rowWidthBytes);

            while (true)
            {
                Vector256<byte> ctrlGroup = Vector256.Load(_ctrl + idx);
                int mask = Avx2.MoveMask(
                    Vector256.Equals(ctrlGroup, Vector256.Create(h2)) |
                    Vector256.Equals(ctrlGroup, Vector256.Create(Empty))
                );

                while (mask != 0)
                {
                    int bitPos = BitOperations.TrailingZeroCount(mask);
                    int realIdx = (idx + bitPos) & _mask;
                    byte foundCtrl = _ctrl[realIdx];

                    if (foundCtrl == Empty)
                    {
                        _ctrl[realIdx] = h2;
                        _rowIndices[realIdx] = rowIndex;
                        int gid = _count++;
                        _groupIds[realIdx] = gid;

                        if (realIdx < GroupSize) _ctrl[realIdx + _capacity] = h2;
                        return gid;
                    }
                    else
                    {
                        int existingRowIdx = _rowIndices[realIdx];
                        byte* pExisting = _packedRows + (existingRowIdx * _rowWidthBytes);

                        if (RowEquals(pKey, pExisting, _rowWidthBytes))
                        {
                            return _groupIds[realIdx];
                        }
                    }
                    mask &= ~(1 << bitPos);
                }
                idx += GroupSize;
                if (idx >= _capacity) idx -= _capacity;
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private bool RowEquals(byte* a, byte* b, int len)
        {
            return new ReadOnlySpan<byte>(a, len).SequenceEqual(new ReadOnlySpan<byte>(b, len));
        }

        [MethodImpl(MethodImplOptions.NoInlining)]
        private void Resize()
        {
            int oldCap = _capacity;
            byte* oldCtrl = _ctrl;
            int* oldRowIndices = _rowIndices;
            int* oldGroupIds = _groupIds;

            int newCap = oldCap * 2;
            AllocateMemory(newCap);
            _capacity = newCap;
            _mask = newCap - 1;
            _resizeThreshold = (int)(newCap * 0.75f);

            for (int i = 0; i < oldCap; i++)
            {
                if (oldCtrl[i] != Empty)
                {
                    int rowIdx = oldRowIndices[i];
                    int hash = RecomputeHash(rowIdx);
                    byte h2 = oldCtrl[i];
                    int idx = hash & _mask;

                    while (true)
                    {
                        if (_ctrl[idx] == Empty)
                        {
                            _ctrl[idx] = h2;
                            _rowIndices[idx] = rowIdx;
                            _groupIds[idx] = oldGroupIds[i];
                            if (idx < GroupSize) _ctrl[idx + newCap] = h2;
                            break;
                        }
                        idx = (idx + 1) & _mask;
                    }
                }
            }

            NativeMemory.Free(oldCtrl);
            NativeMemory.Free(oldRowIndices);
            NativeMemory.Free(oldGroupIds);
        }

        private int RecomputeHash(int rowIdx)
        {
            byte* pRow = _packedRows + (rowIdx * _rowWidthBytes);
            int hash = unchecked((int)2166136261);
            for (int i = 0; i < _rowWidthBytes; i++)
            {
                hash = (hash ^ pRow[i]) * 16777619;
            }
            return hash;
        }

        public int[] ExportKeysAsRowIndices()
        {
            var result = new int[_count];
            fixed (int* pRes = result)
            {
                for (int i = 0; i < _capacity; i++)
                {
                    if (_ctrl[i] != Empty)
                    {
                        pRes[_groupIds[i]] = _rowIndices[i];
                    }
                }
            }
            return result;
        }

        private static int NextPowerOfTwo(int x) => (int)BitOperations.RoundUpToPowerOf2((uint)x);

        public void Dispose()
        {
            if (_ctrl != null) { NativeMemory.Free(_ctrl); _ctrl = null; }
            if (_rowIndices != null) { NativeMemory.Free(_rowIndices); _rowIndices = null; }
            if (_groupIds != null) { NativeMemory.Free(_groupIds); _groupIds = null; }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Collections/NativeStringMap.cs =====
using System.Numerics;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using System.Runtime.Intrinsics;
using System.Runtime.Intrinsics.X86;

namespace LeichtFrame.Core.Engine.Collections
{
    /// <summary>
    /// Specialized Swiss Table for Strings with "German String" optimization (Prefix caching).
    /// </summary>
    internal unsafe struct NativeStringMap : IDisposable
    {
        private byte* _ctrl;

        // --- German String Cache (Hot Data) ---
        private int* _lengths;
        private int* _prefixes;

        // --- Payload ---
        private int* _rowIndices;
        private int* _groupIds;

        // --- Source Data Access ---
        private readonly byte* _sourceBytes;
        private readonly int* _sourceOffsets;

        private int _capacity;
        private int _mask;
        private int _count;
        private int _resizeThreshold;

        private const byte Empty = 0;
        private const int GroupSize = 32;

        public int Count => _count;

        public int Capacity => _capacity;

        public NativeStringMap(int initialCapacity, byte* sourceBytes, int* sourceOffsets)
        {
            _capacity = NextPowerOfTwo(Math.Max(GroupSize, initialCapacity));
            _mask = _capacity - 1;
            _resizeThreshold = (int)(_capacity * 0.75f);
            _count = 0;

            _sourceBytes = sourceBytes;
            _sourceOffsets = sourceOffsets;

            AllocateMemory(_capacity);
        }

        private void AllocateMemory(int cap)
        {
            nuint sizeCtrl = (nuint)(cap + GroupSize);
            nuint sizeInt = (nuint)(cap * sizeof(int));

            _ctrl = (byte*)NativeMemory.AllocZeroed(sizeCtrl);

            _lengths = (int*)NativeMemory.Alloc(sizeInt);
            _prefixes = (int*)NativeMemory.Alloc(sizeInt);

            _rowIndices = (int*)NativeMemory.Alloc(sizeInt);
            _groupIds = (int*)NativeMemory.Alloc(sizeInt);
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public int GetOrAdd(int rowIndex, int hash)
        {
            if (_count >= _resizeThreshold) Resize();

            // 1. Prepare Probe Data
            byte h2 = (byte)((hash & 0x7F) + 1);
            int idx = hash & _mask;

            // 2. Fetch Row Data (Source)
            int start = _sourceOffsets[rowIndex];
            int len = _sourceOffsets[rowIndex + 1] - start;

            int prefix = 0;
            if (len >= 4)
            {
                prefix = Unsafe.ReadUnaligned<int>(_sourceBytes + start);
            }
            else
            {
                byte* pStr = _sourceBytes + start;
                for (int i = 0; i < len; i++) prefix |= (pStr[i] << (i * 8));
            }

            // 3. Probing Loop
            while (true)
            {
                Vector256<byte> ctrlGroup = Vector256.Load(_ctrl + idx);
                Vector256<byte> matchH2 = Vector256.Equals(ctrlGroup, Vector256.Create(h2));
                Vector256<byte> matchEmpty = Vector256.Equals(ctrlGroup, Vector256.Create(Empty));
                int mask = Avx2.MoveMask(matchH2 | matchEmpty);

                while (mask != 0)
                {
                    int bitPos = BitOperations.TrailingZeroCount(mask);
                    int realIdx = (idx + bitPos) & _mask;
                    byte foundCtrl = _ctrl[realIdx];

                    if (foundCtrl == Empty)
                    {
                        // --- INSERT ---
                        _ctrl[realIdx] = h2;

                        // Fill German Cache
                        _lengths[realIdx] = len;
                        _prefixes[realIdx] = prefix;

                        // Fill Payload
                        _rowIndices[realIdx] = rowIndex;
                        int newGroupId = _count;
                        _groupIds[realIdx] = newGroupId;
                        _count++;

                        if (realIdx < GroupSize) _ctrl[realIdx + _capacity] = h2;

                        return newGroupId;
                    }
                    else if (foundCtrl == h2)
                    {
                        // --- MATCH CHECK ---
                        if (_lengths[realIdx] == len && _prefixes[realIdx] == prefix)
                        {
                            if (SeqEqual(rowIndex, _rowIndices[realIdx], len))
                            {
                                return _groupIds[realIdx];
                            }
                        }
                    }

                    mask &= ~(1 << bitPos);
                }

                idx += GroupSize;
                if (idx >= _capacity) idx -= _capacity;
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private bool SeqEqual(int rowA, int rowB, int len)
        {
            int startA = _sourceOffsets[rowA];
            int startB = _sourceOffsets[rowB];

            var spanA = new ReadOnlySpan<byte>(_sourceBytes + startA, len);
            var spanB = new ReadOnlySpan<byte>(_sourceBytes + startB, len);
            return spanA.SequenceEqual(spanB);
        }

        [MethodImpl(MethodImplOptions.NoInlining)]
        private void Resize()
        {
            int oldCap = _capacity;
            byte* oldCtrl = _ctrl;
            int* oldLengths = _lengths;
            int* oldPrefixes = _prefixes;
            int* oldRowIndices = _rowIndices;
            int* oldGroupIds = _groupIds;

            int newCap = oldCap * 2;
            AllocateMemory(newCap);

            _capacity = newCap;
            _mask = newCap - 1;
            _resizeThreshold = (int)(newCap * 0.75f);

            for (int i = 0; i < oldCap; i++)
            {
                if (oldCtrl[i] != Empty)
                {
                    int rowIdx = oldRowIndices[i];
                    int len = oldLengths[i];

                    int hash = ComputeHashForResize(rowIdx, len);
                    byte h2 = oldCtrl[i];

                    int idx = hash & _mask;

                    while (true)
                    {
                        if (_ctrl[idx] == Empty)
                        {
                            _ctrl[idx] = h2;
                            _lengths[idx] = len;
                            _prefixes[idx] = oldPrefixes[i];
                            _rowIndices[idx] = rowIdx;
                            _groupIds[idx] = oldGroupIds[i];

                            if (idx < GroupSize) _ctrl[idx + newCap] = h2;
                            break;
                        }
                        idx = (idx + 1) & _mask;
                    }
                }
            }

            NativeMemory.Free(oldCtrl);
            NativeMemory.Free(oldLengths);
            NativeMemory.Free(oldPrefixes);
            NativeMemory.Free(oldRowIndices);
            NativeMemory.Free(oldGroupIds);
        }

        private int ComputeHashForResize(int rowIndex, int len)
        {
            // Simple FNV-1a recalculation locally
            int start = _sourceOffsets[rowIndex];
            int hash = unchecked((int)2166136261);
            byte* pStr = _sourceBytes + start;
            for (int k = 0; k < len; k++)
            {
                hash ^= pStr[k];
                hash *= 16777619;
            }
            return hash;
        }

        public int[] ExportKeysAsRowIndices()
        {
            var result = new int[_count];
            fixed (int* pRes = result)
            {
                for (int i = 0; i < _capacity; i++)
                {
                    if (_ctrl[i] != Empty)
                    {
                        int id = _groupIds[i];
                        pRes[id] = _rowIndices[i];
                    }
                }
            }
            return result;
        }

        private static int NextPowerOfTwo(int x) => (int)BitOperations.RoundUpToPowerOf2((uint)x);

        public void Dispose()
        {
            if (_ctrl != null) { NativeMemory.Free(_ctrl); _ctrl = null; }
            if (_lengths != null) { NativeMemory.Free(_lengths); _lengths = null; }
            if (_prefixes != null) { NativeMemory.Free(_prefixes); _prefixes = null; }
            if (_rowIndices != null) { NativeMemory.Free(_rowIndices); _rowIndices = null; }
            if (_groupIds != null) { NativeMemory.Free(_groupIds); _groupIds = null; }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Collections/PrimitiveKeyMap.cs =====
using System.Runtime.CompilerServices;

namespace LeichtFrame.Core.Engine
{
    internal sealed class PrimitiveKeyMap<T> : IDisposable where T : unmanaged, IEquatable<T>
    {
        private int[] _buckets;
        private Entry[] _entries;
        private int _count;
        private int _freeList;

        private int[] _groupHeads;
        private int[] _groupTails;
        private int[] _groupCounts;

        public int[] RowNext;

        private struct Entry
        {
            public int HashCode;
            public int Next;
            public T Key;
        }

        public int Count => _count;

        public PrimitiveKeyMap(int capacity, int rowCount)
        {
            int size = GetPrime(capacity);
            _buckets = new int[size];
            Array.Fill(_buckets, -1);

            _entries = new Entry[size];
            _freeList = -1;
            _count = 0;

            _groupHeads = new int[size];
            Array.Fill(_groupHeads, -1);

            _groupTails = new int[size];
            Array.Fill(_groupTails, -1);

            _groupCounts = new int[size];

            RowNext = new int[rowCount];
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public void AddRow(T key, int rowIndex)
        {
            int groupIndex = GetOrAddGroup(key);

            int tail = _groupTails[groupIndex];
            if (tail == -1) _groupHeads[groupIndex] = rowIndex;
            else RowNext[tail] = rowIndex;

            _groupTails[groupIndex] = rowIndex;
            RowNext[rowIndex] = -1;
            _groupCounts[groupIndex]++;
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private int GetOrAddGroup(T key)
        {
            int hashCode = key.GetHashCode() & 0x7FFFFFFF;
            int bucket = hashCode % _buckets.Length;

            for (int i = _buckets[bucket]; i >= 0; i = _entries[i].Next)
            {
                if (_entries[i].HashCode == hashCode && _entries[i].Key.Equals(key)) return i;
            }

            int index;
            if (_freeList >= 0) { index = _freeList; _freeList = _entries[index].Next; }
            else
            {
                if (_count == _entries.Length) { Resize(); bucket = hashCode % _buckets.Length; }
                index = _count;
                _count++;
            }

            ref Entry entry = ref _entries[index];
            entry.HashCode = hashCode;
            entry.Next = _buckets[bucket];
            entry.Key = key;
            _buckets[bucket] = index;

            return index;
        }

        private void Resize()
        {
            int newSize = GetPrime(_count * 2);
            int[] newBuckets = new int[newSize];
            Array.Fill(newBuckets, -1);
            Entry[] newEntries = new Entry[newSize];
            Array.Copy(_entries, newEntries, _count);

            for (int i = 0; i < _count; i++)
            {
                int bucket = newEntries[i].HashCode % newSize;
                newEntries[i].Next = newBuckets[bucket];
                newBuckets[bucket] = i;
            }
            _buckets = newBuckets;
            _entries = newEntries;

            Array.Resize(ref _groupHeads, newSize);
            for (int i = _count; i < newSize; i++) _groupHeads[i] = -1;
            Array.Resize(ref _groupTails, newSize);
            for (int i = _count; i < newSize; i++) _groupTails[i] = -1;
            Array.Resize(ref _groupCounts, newSize);
        }

        public (T[] Keys, int[] GroupOffsets, int[] RowIndices) ToCSR()
        {
            var keys = new T[_count];
            var offsets = new int[_count + 1];
            int totalRows = 0;
            for (int i = 0; i < _count; i++) totalRows += _groupCounts[i];

            var indices = new int[totalRows];
            int currentOffset = 0;

            for (int i = 0; i < _count; i++)
            {
                keys[i] = _entries[i].Key;
                offsets[i] = currentOffset;
                int rowIdx = _groupHeads[i];
                int ptr = currentOffset;
                while (rowIdx != -1)
                {
                    indices[ptr++] = rowIdx;
                    rowIdx = RowNext[rowIdx];
                }
                currentOffset += _groupCounts[i];
            }
            offsets[_count] = currentOffset;
            return (keys, offsets, indices);
        }

        private static int GetPrime(int min)
        {
            int[] primes = { 101, 211, 431, 863, 1741, 3491, 6991, 13999, 28001, 56009, 112003, 224017, 448051, 896113, 1792241, 3584497, 7169003 };
            foreach (int p in primes) if (p >= min) return p;
            return min | 1;
        }

        public void Dispose()
        {
            _buckets = null!; _entries = null!; _groupHeads = null!; _groupTails = null!; _groupCounts = null!; RowNext = null!;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Collections/StringKeyMap.cs =====
using System.Runtime.CompilerServices;
using System.Text;

namespace LeichtFrame.Core.Engine
{
    internal sealed class StringKeyMap : IDisposable
    {
        private int[] _buckets;
        private Entry[] _entries;
        private int _count;
        private int _freeList;

        private readonly byte[] _bytes;
        private readonly int[] _offsets;

        private int[] _groupHeads;
        private int[] _groupTails;
        private int[] _groupCounts;

        public int[] RowNext;

        private struct Entry
        {
            public int HashCode;
            public int Next;
            public int FirstGlobalRowIdx;
        }

        public int Count => _count;

        // capacity: Gesch√§tzte Anzahl Gruppen
        // localRowCount: Gr√∂√üe des lokalen RowNext Arrays (Partition Size)
        public StringKeyMap(byte[] bytes, int[] offsets, int capacity, int localRowCount)
        {
            _bytes = bytes;
            _offsets = offsets;

            int size = GetPrime(capacity);
            _buckets = new int[size];
            Array.Fill(_buckets, -1);

            _entries = new Entry[size];
            _freeList = -1;
            _count = 0;

            _groupHeads = new int[size];
            Array.Fill(_groupHeads, -1);

            _groupTails = new int[size];
            Array.Fill(_groupTails, -1);

            _groupCounts = new int[size];

            RowNext = new int[localRowCount];
            Array.Fill(RowNext, -1);
        }

        // Sequential: Global Index == Local Index
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public void AddRow(int globalRowIndex)
        {
            AddRow(globalRowIndex, globalRowIndex);
        }

        // Parallel: Global Index (f√ºr Datenzugriff) != Local Index (f√ºr LinkedList Speicher)
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public void AddRow(int globalRowIndex, int localRowIndex)
        {
            // 1. Lookup (Hash Global Data)
            int start = _offsets[globalRowIndex];
            int len = _offsets[globalRowIndex + 1] - start;
            int hashCode = ComputeHash(start, len);
            int bucket = (hashCode & 0x7FFFFFFF) % _buckets.Length;

            int groupIndex = -1;
            for (int i = _buckets[bucket]; i >= 0; i = _entries[i].Next)
            {
                if (_entries[i].HashCode == hashCode)
                {
                    int repRow = _entries[i].FirstGlobalRowIdx;
                    int repStart = _offsets[repRow];
                    int repLen = _offsets[repRow + 1] - repStart;
                    if (len == repLen)
                    {
                        var span = new ReadOnlySpan<byte>(_bytes, start, len);
                        var repSpan = new ReadOnlySpan<byte>(_bytes, repStart, repLen);
                        if (span.SequenceEqual(repSpan))
                        {
                            groupIndex = i;
                            break;
                        }
                    }
                }
            }

            // 2. Insert new Group
            if (groupIndex == -1)
            {
                if (_freeList >= 0) { groupIndex = _freeList; _freeList = _entries[groupIndex].Next; }
                else
                {
                    if (_count == _entries.Length) { Resize(); bucket = (hashCode & 0x7FFFFFFF) % _buckets.Length; }
                    groupIndex = _count;
                    _count++;
                }

                ref Entry entry = ref _entries[groupIndex];
                entry.HashCode = hashCode;
                entry.Next = _buckets[bucket];
                entry.FirstGlobalRowIdx = globalRowIndex;
                _buckets[bucket] = groupIndex;
            }

            // 3. Append to Linked List (using Local Index)
            int tail = _groupTails[groupIndex];
            if (tail == -1)
            {
                _groupHeads[groupIndex] = localRowIndex;
            }
            else
            {
                RowNext[tail] = localRowIndex;
            }
            _groupTails[groupIndex] = localRowIndex;
            RowNext[localRowIndex] = -1;

            _groupCounts[groupIndex]++;
        }

        public (string[] Keys, int[] Offsets, int[] Indices) ToCSR()
        {
            var keys = new string[_count];
            var offsets = new int[_count + 1];

            int totalRows = 0;
            for (int i = 0; i < _count; i++) totalRows += _groupCounts[i];

            var indices = new int[totalRows];
            int currentOffset = 0;

            for (int i = 0; i < _count; i++)
            {
                // Materialize Key
                int repRow = _entries[i].FirstGlobalRowIdx;
                int start = _offsets[repRow];
                int len = _offsets[repRow + 1] - start;
                keys[i] = Encoding.UTF8.GetString(_bytes, start, len);

                offsets[i] = currentOffset;

                int rowIdx = _groupHeads[i];
                int ptr = currentOffset;
                while (rowIdx != -1)
                {
                    indices[ptr++] = rowIdx;
                    rowIdx = RowNext[rowIdx];
                }

                currentOffset += _groupCounts[i];
            }
            offsets[_count] = currentOffset;

            return (keys, offsets, indices);
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private int ComputeHash(int start, int length)
        {
            int hash = -2128831035;
            int end = start + length;
            for (int k = start; k < end; k++)
                hash = (hash ^ _bytes[k]) * 16777619;
            return hash;
        }

        private void Resize()
        {
            int newSize = GetPrime(_count * 2);
            int[] newBuckets = new int[newSize];
            Array.Fill(newBuckets, -1);

            Entry[] newEntries = new Entry[newSize];
            Array.Copy(_entries, newEntries, _count);

            for (int i = 0; i < _count; i++)
            {
                int bucket = (newEntries[i].HashCode & 0x7FFFFFFF) % newSize;
                newEntries[i].Next = newBuckets[bucket];
                newBuckets[bucket] = i;
            }
            _buckets = newBuckets;
            _entries = newEntries;

            Array.Resize(ref _groupHeads, newSize);
            for (int i = _count; i < newSize; i++) _groupHeads[i] = -1;

            Array.Resize(ref _groupTails, newSize);
            for (int i = _count; i < newSize; i++) _groupTails[i] = -1;

            Array.Resize(ref _groupCounts, newSize);
        }

        private static int GetPrime(int min)
        {
            int[] primes = { 101, 211, 431, 863, 1741, 3491, 6991, 13999, 28001, 56009, 112003, 224017, 448051, 896113, 1792241, 3584497, 7169003 };
            foreach (int p in primes) if (p >= min) return p;
            return min | 1;
        }

        public void Dispose()
        {
            _buckets = null!; _entries = null!; _groupHeads = null!; _groupTails = null!; _groupCounts = null!; RowNext = null!;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/Aggregate/AggregateDispatcher.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Aggregate/IAggregateStrategy.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Filter/FilterDispatcher.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Filter/IFilterStrategy.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/GroupByDispatcher.cs =====
using LeichtFrame.Core.Engine.Kernels.GroupBy.Strategies;

namespace LeichtFrame.Core.Engine.Kernels.GroupBy
{
    internal static class GroupByDispatcher
    {
        public static GroupedDataFrame DecideAndExecute(DataFrame df, string columnName)
        {
            var col = df[columnName];
            int rowCount = df.RowCount;

            if (rowCount == 0) return new GenericHashMapStrategy().Group(df, columnName);

            // --- INT ---
            if (col is IntColumn intCol)
            {
                int min = intCol.Min();
                int max = intCol.Max();
                long range = (long)max - min;

                // Dense -> Direct Addressing
                if (range >= 0 && range <= 1_000_000)
                {
                    return new DirectAddressingStrategy(min, max).Group(df, columnName);
                }
                // Sparse -> Int Swiss Map
                return new IntSwissMapStrategy().Group(df, columnName);
            }

            // --- STRING ---
            if (col is StringColumn strCol)
            {
                // Low Cardinality -> String Dictionary
                if (IsLikelyLowCardinality(strCol, rowCount))
                {
                    return new StringDictionaryStrategy().Group(df, columnName);
                }

                // High Cardinality -> String Swiss Map
                return new StringSwissMapStrategy().Group(df, columnName);
            }

            return new GenericHashMapStrategy().Group(df, columnName);
        }

        public static GroupedDataFrame DecideAndExecute(DataFrame df, string[] columnNames)
        {
            if (columnNames.Length == 1) return DecideAndExecute(df, columnNames[0]);

            bool allPackable = true;
            foreach (var c in columnNames)
            {
                var t = df[c].DataType;
                t = Nullable.GetUnderlyingType(t) ?? t;
                if (t != typeof(int) && t != typeof(double) && t != typeof(bool) && t != typeof(long) && t != typeof(DateTime))
                    allPackable = false;
            }

            if (allPackable)
            {
                // Fixed-Width Primitives -> Row Layout Packing
                return new RowLayoutHashStrategy().Group(df, columnNames);
            }

            // Fallback: Mixed Types / Variable Length -> Component-wise Hashing
            return new MultiColumnHashStrategy().Group(df, columnNames);
        }

        // Helper: Detect Low Cardinality via Sampling
        private static bool IsLikelyLowCardinality(StringColumn col, int rowCount)
        {
            if (rowCount < 1024) return false;

            int sampleSize = 64;
            var set = new HashSet<string>();

            int step = rowCount / sampleSize;
            for (int i = 0; i < rowCount; i += step)
            {
                string? val = col.Get(i);
                if (val != null) set.Add(val);

                if (set.Count > sampleSize * 0.8) return false;
            }
            return true;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/IGroupByStrategy.cs =====
namespace LeichtFrame.Core.Engine
{
    internal interface IGroupByStrategy
    {
        GroupedDataFrame Group(DataFrame frame, string columnName);
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/DirectAddressingStrategy.cs =====
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Engine
{
    internal unsafe class DirectAddressingStrategy : IGroupByStrategy
    {
        private readonly int? _knownMin;
        private readonly int? _knownMax;

        public DirectAddressingStrategy(int? min = null, int? max = null)
        {
            _knownMin = min;
            _knownMax = max;
        }

        public GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = (IntColumn)df[columnName];

            NativeGroupedData nativeData = ComputeNative(col, df.RowCount);

            return new GroupedDataFrame<int>(df, new[] { columnName }, nativeData, null);
        }

        internal NativeGroupedData ComputeNative(IntColumn col, int rowCount)
        {
            if (rowCount == 0) return new NativeGroupedData(0, 0);

            int min = _knownMin ?? col.Min();
            int max = _knownMax ?? col.Max();
            long range = (long)max - min;
            int bucketCount = (int)range + 1;
            int numThreads = Environment.ProcessorCount;

            nuint histogramSize = (nuint)(numThreads * bucketCount * sizeof(int));

            int* pGlobalHistograms = (int*)NativeMemory.AllocZeroed(histogramSize);
            int* pWriteOffsets = (int*)NativeMemory.Alloc(histogramSize);

            var result = new NativeGroupedData(rowCount, bucketCount);

            try
            {
                fixed (int* pInput = col.Values.Span)
                {
                    PartitionedHistogram.ComputeHistograms(pInput, pGlobalHistograms, rowCount, min, bucketCount, numThreads);

                    int* pOffsets = result.Offsets.Ptr;
                    int* pKeys = result.Keys.Ptr;
                    pOffsets[0] = 0;
                    int currentGlobalOffset = 0;
                    int activeGroups = 0;

                    for (int b = 0; b < bucketCount; b++)
                    {
                        int globalCountForBucket = 0;
                        for (int t = 0; t < numThreads; t++)
                        {
                            int localCount = pGlobalHistograms[t * bucketCount + b];
                            if (localCount > 0)
                            {
                                pWriteOffsets[t * bucketCount + b] = currentGlobalOffset + globalCountForBucket;
                                globalCountForBucket += localCount;
                            }
                        }

                        if (globalCountForBucket > 0)
                        {
                            pKeys[activeGroups] = b + min;
                            currentGlobalOffset += globalCountForBucket;
                            activeGroups++;
                            pOffsets[activeGroups] = currentGlobalOffset;
                        }
                    }

                    result.GroupCount = activeGroups;

                    PartitionedHistogram.ScatterIndices(pInput, result.Indices.Ptr, pWriteOffsets, rowCount, min, bucketCount, numThreads);
                }
                return result;
            }
            finally
            {
                NativeMemory.Free(pGlobalHistograms);
                NativeMemory.Free(pWriteOffsets);
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/GenericHashmapStrategy.cs =====
namespace LeichtFrame.Core.Engine
{
    /// <summary>
    /// Fallback strategy for types that don't have specialized high-performance solvers (like Double, Long, Bool, DateTime).
    /// Uses a high-performance Dictionary (PrimitiveKeyMap) from the Engine.
    /// </summary>
    internal class GenericHashMapStrategy : IGroupByStrategy
    {
        public GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = df[columnName];
            Type t = Nullable.GetUnderlyingType(col.DataType) ?? col.DataType;

            if (t == typeof(int)) return GroupT<int>(df, columnName);
            if (t == typeof(double)) return GroupT<double>(df, columnName);
            if (t == typeof(long)) return GroupT<long>(df, columnName);
            if (t == typeof(bool)) return GroupT<bool>(df, columnName);
            if (t == typeof(DateTime)) return GroupT<DateTime>(df, columnName);

            throw new NotSupportedException($"Type {t.Name} not supported in fallback strategy.");
        }

        private GroupedDataFrame GroupT<T>(DataFrame df, string columnName) where T : unmanaged, IEquatable<T>
        {
            var col = (IColumn<T>)df[columnName];

            var map = new PrimitiveKeyMap<T>(Math.Max(128, df.RowCount / 10), df.RowCount);

            var nullIndices = new List<int>();

            for (int i = 0; i < df.RowCount; i++)
            {
                if (col.IsNull(i))
                {
                    nullIndices.Add(i);
                    continue;
                }

                map.AddRow(col.GetValue(i), i);
            }

            var csr = map.ToCSR();
            map.Dispose();

            return new GroupedDataFrame<T>(
                df,
                new[] { columnName },
                csr.Keys,
                csr.GroupOffsets,
                csr.RowIndices,
                nullIndices.Count > 0 ? nullIndices.ToArray() : null
            );
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/IntDirectMapStrategy.cs =====
using System.Buffers;

namespace LeichtFrame.Core.Engine
{
    internal class IntDirectMapStrategy : IGroupByStrategy
    {
        private readonly int _min;
        private readonly int _range;

        public IntDirectMapStrategy(int min, int range)
        {
            _min = min;
            _range = range;
        }

        public GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = (IntColumn)df[columnName];
            int rowCount = df.RowCount;
            int bucketCount = _range + 1;
            var values = col.Values.Span;

            // 1. Z√§hlen (Histogramm)
            int[] counts = ArrayPool<int>.Shared.Rent(bucketCount);
            Array.Clear(counts, 0, bucketCount);

            for (int i = 0; i < rowCount; i++)
            {
                // Mapping: Wert -> Array Index (Offset-basiert)
                // TODO: Null Handling bei IntDirectMap ist hier vereinfacht (f√ºr Non-Nullable)
                if (!col.IsNull(i))
                {
                    counts[values[i] - _min]++;
                }
            }

            // 2. Offsets berechnen (Prefix Sum)
            int activeGroups = 0;
            for (int i = 0; i < bucketCount; i++)
            {
                if (counts[i] > 0) activeGroups++;
            }

            int[] keys = new int[activeGroups];
            int[] offsets = new int[activeGroups + 1];
            // Hilfsarray: Wo muss der Wert 'i' im Ziel-Index-Array hin?
            int[] mapBucketToGroup = ArrayPool<int>.Shared.Rent(bucketCount);

            int currentOffset = 0;
            int groupIdx = 0;

            for (int i = 0; i < bucketCount; i++)
            {
                int count = counts[i];
                if (count > 0)
                {
                    keys[groupIdx] = i + _min; // Rekonstruktion des echten Wertes
                    offsets[groupIdx] = currentOffset;
                    mapBucketToGroup[i] = groupIdx;

                    // Wir recyceln das 'counts' Array:
                    // Es speichert jetzt die "n√§chste Schreibposition" f√ºr diesen Bucket.
                    counts[i] = currentOffset;

                    currentOffset += count;
                    groupIdx++;
                }
            }
            offsets[activeGroups] = currentOffset; // Sentinel am Ende

            // 3. Schreiben (Scatter)
            int[] indices = new int[rowCount];
            // Nulls separat behandeln falls n√∂tig, hier vereinfacht
            List<int>? nullIndices = null;

            for (int i = 0; i < rowCount; i++)
            {
                if (col.IsNull(i))
                {
                    if (nullIndices == null) nullIndices = new List<int>();
                    nullIndices.Add(i);
                    continue;
                }

                int val = values[i] - _min;
                int dest = counts[val]; // Hole Schreibposition
                indices[dest] = i;
                counts[val]++; // Position inkrementieren
            }

            // Cleanup
            ArrayPool<int>.Shared.Return(counts);
            ArrayPool<int>.Shared.Return(mapBucketToGroup);

            // Wenn wir Nulls hatten, m√ºssen wir das Indices Array vielleicht k√ºrzen, 
            // da 'rowCount' alle Zeilen inkl. Nulls war.
            if (nullIndices != null && nullIndices.Count > 0)
            {
                int validRows = rowCount - nullIndices.Count;
                // Array.Resize w√§re hier m√∂glich, oder wir √ºbergeben Slice. 
                // Der Einfachheit halber Resize:
                Array.Resize(ref indices, validRows);
            }

            return new GroupedDataFrame<int>(
                df,
                new[] { columnName },
                keys,
                offsets,
                indices,
                nullIndices?.ToArray()
            );
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/IntRadixStrategy.cs =====
using System.Buffers;
using System.Collections.Concurrent;
using System.Runtime.InteropServices;
using LeichtFrame.Core.Engine.Memory;

namespace LeichtFrame.Core.Engine
{
    /// <summary>
    /// A high-performance grouping strategy for High-Cardinality Integer columns.
    /// Orchestrates the data flow between the DataFrame and the SIMD Engine.
    /// </summary>
    internal class IntRadixStrategy : IGroupByStrategy
    {
        public GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = (IntColumn)df[columnName];
            return GroupInt32(df, columnName, col);
        }

        private unsafe GroupedDataFrame GroupInt32(DataFrame df, string columnName, IntColumn col)
        {
            int rowCount = df.RowCount;

            if (rowCount == 0) return BuildCsr(df, columnName, col, null, null, 0);

            int parallelism = Environment.ProcessorCount;

            using var hashesBuffer = new UnsafeBuffer<int>(rowCount);
            using var indicesBuffer = new UnsafeBuffer<int>(rowCount);
            using var tempIndicesBuffer = new UnsafeBuffer<int>(rowCount);

            int* pHashes = hashesBuffer.Ptr;
            int* pIndices = indicesBuffer.Ptr;
            int* pTempIndices = tempIndicesBuffer.Ptr;

            ReadOnlyMemory<int> keysMemory = col.Values;
            int rangeSize = Math.Max(1, rowCount / parallelism);
            var rangePartitioner = Partitioner.Create(0, rowCount, rangeSize);

            Parallel.ForEach(rangePartitioner, range =>
            {
                ReadOnlySpan<int> keysSpan = keysMemory.Span;

                var sliceKeys = keysSpan.Slice(range.Item1, range.Item2 - range.Item1);
                var sliceHashes = new Span<int>(pHashes + range.Item1, range.Item2 - range.Item1);

                VectorizedHasher.HashIntegers(sliceKeys, sliceHashes);
            });

            SimdRadixSorter.Sort(pHashes, pIndices, pTempIndices, rowCount);

            return BuildCsr(df, columnName, col, pHashes, pIndices, rowCount);
        }

        /// <summary>
        /// Transforms the sorted indices and hashes into a CSR (Compressed Sparse Row) representation.
        /// </summary>
        private unsafe GroupedDataFrame BuildCsr(
            DataFrame df,
            string columnName,
            IntColumn col,
            int* pHashes,
            int* pIndices,
            int length)
        {
            if (length == 0)
                return new GroupedDataFrame<int>(df, new[] { columnName }, Array.Empty<int>(), new[] { 0 }, Array.Empty<int>(), null);

            int[] tempKeys = ArrayPool<int>.Shared.Rent(length);
            int[] tempOffsets = ArrayPool<int>.Shared.Rent(length + 1);

            int[] finalIndices = new int[length];

            Marshal.Copy((nint)pIndices, finalIndices, 0, length);

            int groupCount = 0;

            try
            {
                fixed (int* pKeys = tempKeys)
                fixed (int* pOffsets = tempOffsets)
                {
                    pOffsets[0] = 0;

                    var values = col.Values.Span;

                    int prevIdx = pIndices[0];
                    int prevVal = values[prevIdx];
                    int prevHash = pHashes[prevIdx];

                    for (int i = 1; i < length; i++)
                    {
                        int currIdx = pIndices[i];
                        int currHash = pHashes[currIdx];

                        bool isDiff = false;

                        if (currHash != prevHash)
                        {
                            isDiff = true;
                        }
                        else
                        {
                            int currVal = values[currIdx];
                            if (currVal != prevVal)
                            {
                                isDiff = true;
                                prevVal = currVal;
                            }
                        }

                        if (isDiff)
                        {
                            pKeys[groupCount] = prevVal;
                            groupCount++;
                            pOffsets[groupCount] = i;

                            prevHash = currHash;
                            prevVal = values[currIdx];
                        }
                    }

                    pKeys[groupCount] = prevVal;
                    groupCount++;
                    pOffsets[groupCount] = length;
                }

                int[] finalKeys = new int[groupCount];
                Array.Copy(tempKeys, finalKeys, groupCount);

                int[] finalOffsets = new int[groupCount + 1];
                Array.Copy(tempOffsets, finalOffsets, groupCount + 1);

                return new GroupedDataFrame<int>(
                    df,
                    new[] { columnName },
                    finalKeys,
                    finalOffsets,
                    finalIndices,
                    null);
            }
            finally
            {
                ArrayPool<int>.Shared.Return(tempKeys);
                ArrayPool<int>.Shared.Return(tempOffsets);
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/IntSwissMapStrategy.cs =====
using System.Runtime.InteropServices;
using LeichtFrame.Core.Engine.Collections;

namespace LeichtFrame.Core.Engine.Kernels.GroupBy.Strategies
{
    internal class IntSwissMapStrategy : IGroupByStrategy
    {
        public GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = (IntColumn)df[columnName];
            return GroupNative(df, columnName, col);
        }

        private unsafe GroupedDataFrame GroupNative(DataFrame df, string colName, IntColumn col)
        {
            int rowCount = df.RowCount;
            if (rowCount == 0)
                return new GroupedDataFrame<int>(df, new[] { colName }, Array.Empty<int>(), new[] { 0 }, Array.Empty<int>(), null);

            int* pRowToGroupId = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));

            NativeIntMap map = new NativeIntMap(Math.Max(1024, rowCount / 10));
            var nullIndices = new List<int>();

            try
            {
                // =========================================================
                // PHASE 1: Build Map & Assign IDs (The Heavy Lifting)
                // =========================================================
                ReadOnlySpan<int> data = col.Values.Span;
                fixed (int* pData = data)
                {
                    for (int i = 0; i < rowCount; i++)
                    {
                        if (col.IsNull(i))
                        {
                            nullIndices.Add(i);
                            pRowToGroupId[i] = -1;
                            continue;
                        }

                        pRowToGroupId[i] = map.GetOrAdd(pData[i]);
                    }
                }

                int groupCount = map.Count;

                // =========================================================
                // PHASE 2: Build CSR (Count Sort Style)
                // =========================================================

                var resultNative = new NativeGroupedData(rowCount, groupCount);

                int[] managedKeys = map.ExportKeys();
                Marshal.Copy(managedKeys, 0, (nint)resultNative.Keys.Ptr, groupCount);

                int* pOffsets = resultNative.Offsets.Ptr;
                int* pIndices = resultNative.Indices.Ptr;

                new Span<int>(pOffsets, groupCount + 1).Fill(0);

                for (int i = 0; i < rowCount; i++)
                {
                    int gid = pRowToGroupId[i];
                    if (gid != -1)
                    {
                        pOffsets[gid]++;
                    }
                }

                int currentOffset = 0;
                int* pWriteHeads = (int*)NativeMemory.Alloc((nuint)(groupCount * sizeof(int)));

                for (int i = 0; i < groupCount; i++)
                {
                    int count = pOffsets[i];
                    pOffsets[i] = currentOffset;
                    pWriteHeads[i] = currentOffset;
                    currentOffset += count;
                }
                pOffsets[groupCount] = currentOffset;

                for (int i = 0; i < rowCount; i++)
                {
                    int gid = pRowToGroupId[i];
                    if (gid != -1)
                    {
                        int dest = pWriteHeads[gid];
                        pIndices[dest] = i;
                        pWriteHeads[gid]++;
                    }
                }

                NativeMemory.Free(pWriteHeads);

                return new GroupedDataFrame<int>(
                    df,
                    new[] { colName },
                    resultNative,
                    nullIndices.Count > 0 ? nullIndices.ToArray() : null
                );
            }
            finally
            {
                map.Dispose();
                NativeMemory.Free(pRowToGroupId);
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/ManagedCodeStrategy.cs =====
using System.Collections.Concurrent;

namespace LeichtFrame.Core.Engine
{
    /// <summary>
    /// The existing implementation: Safe Managed Code.
    /// Uses PrimitiveKeyMap (Dictionary-based) and Managed Parallelism for Strings.
    /// </summary>
    internal class StandardStrategy : IGroupByStrategy
    {
        private const int ParallelThreshold = 100_000;
        private const int BufferSize = 512;

        public GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = df[columnName];
            Type t = Nullable.GetUnderlyingType(col.DataType) ?? col.DataType;

            // Primitives -> Sequential
            if (t == typeof(int)) return GroupByPrimitiveSequential<int>(df, columnName);
            if (t == typeof(double)) return GroupByPrimitiveSequential<double>(df, columnName);
            if (t == typeof(long)) return GroupByPrimitiveSequential<long>(df, columnName);
            if (t == typeof(bool)) return GroupByPrimitiveSequential<bool>(df, columnName);
            if (t == typeof(DateTime)) return GroupByPrimitiveSequential<DateTime>(df, columnName);

            // Strings -> Smart Dispatch
            if (t == typeof(string)) return GroupByString(df, columnName);

            throw new NotSupportedException($"GroupBy not implemented for type {t.Name}");
        }

        private GroupedDataFrame GroupByPrimitiveSequential<T>(DataFrame df, string columnName) where T : unmanaged, IEquatable<T>
        {
            var col = (IColumn<T>)df[columnName];
            var map = new PrimitiveKeyMap<T>(Math.Max(128, df.RowCount / 10), df.RowCount);
            var nullIndices = new List<int>();
            for (int i = 0; i < df.RowCount; i++)
            {
                if (col.IsNull(i)) { nullIndices.Add(i); continue; }
                map.AddRow(col.GetValue(i), i);
            }
            var csr = map.ToCSR();
            map.Dispose();
            return new GroupedDataFrame<T>(df, new[] { columnName }, csr.Keys, csr.GroupOffsets, csr.RowIndices, nullIndices.Count > 0 ? nullIndices.ToArray() : null);
        }

        private GroupedDataFrame GroupByString(DataFrame df, string columnName)
        {
            var col = (StringColumn)df[columnName];

            if (df.RowCount < ParallelThreshold)
            {
                return GroupByStringSequential(df, columnName);
            }

            if (ShouldUseParallelStringProcessing(col))
            {
                return GroupByStringParallel(df, columnName);
            }

            return GroupByStringSequential(df, columnName);
        }

        private bool ShouldUseParallelStringProcessing(StringColumn col)
        {
            int sampleSize = Math.Min(col.Length, 100);
            if (sampleSize == 0) return false;
            var uniqueSampler = new HashSet<string>(sampleSize);
            int uniqueCount = 0;

            for (int i = 0; i < sampleSize; i += 1)
            {
                string? val = col.Get(i);
                if (val != null && uniqueSampler.Add(val)) uniqueCount++;
                if (uniqueCount > 20) return true;
            }
            return false;
        }

        private GroupedDataFrame GroupByStringSequential(DataFrame df, string columnName)
        {
            var col = (StringColumn)df[columnName];
            var map = new StringKeyMap(col.RawBytes, col.Offsets, Math.Max(128, df.RowCount / 100), df.RowCount);
            var nullIndices = new List<int>();

            for (int i = 0; i < df.RowCount; i++)
            {
                if (col.IsNull(i)) { nullIndices.Add(i); continue; }
                map.AddRow(i);
            }

            var csr = map.ToCSR();
            map.Dispose();
            return new GroupedDataFrame<string>(df, new[] { columnName }, csr.Keys, csr.Offsets, csr.Indices, nullIndices.Count > 0 ? nullIndices.ToArray() : null);
        }

        private GroupedDataFrame GroupByStringParallel(DataFrame df, string columnName)
        {
            var col = (StringColumn)df[columnName];
            int rowCount = df.RowCount;
            int partitionCount = Math.Max(16, Environment.ProcessorCount * 2);
            byte[] rawBytes = col.RawBytes;
            int[] offsets = col.Offsets;

            var globalBuckets = new List<int>[partitionCount];
            for (int i = 0; i < partitionCount; i++) globalBuckets[i] = new List<int>(rowCount / partitionCount);
            var globalLocks = new object[partitionCount];
            for (int i = 0; i < partitionCount; i++) globalLocks[i] = new object();
            var nullIndices = new ConcurrentBag<int>();

            Parallel.ForEach(Partitioner.Create(0, rowCount), () =>
            {
                var buffers = new int[partitionCount][];
                for (int i = 0; i < partitionCount; i++) buffers[i] = new int[BufferSize];
                var counts = new int[partitionCount];
                return (buffers, counts);
            },
            (range, state, localState) =>
            {
                var (buffers, counts) = localState;
                for (int i = range.Item1; i < range.Item2; i++)
                {
                    if (col.IsNull(i)) { nullIndices.Add(i); continue; }
                    int start = offsets[i];
                    int len = offsets[i + 1] - start;
                    int hash = -2128831035;
                    int end = start + len;
                    for (int k = start; k < end; k++) hash = (hash ^ rawBytes[k]) * 16777619;
                    int pIdx = (hash & 0x7FFFFFFF) % partitionCount;
                    buffers[pIdx][counts[pIdx]] = i;
                    counts[pIdx]++;
                    if (counts[pIdx] == BufferSize)
                    {
                        lock (globalLocks[pIdx])
                        {
                            var gBucket = globalBuckets[pIdx];
                            var buffer = buffers[pIdx];
                            for (int k = 0; k < BufferSize; k++) gBucket.Add(buffer[k]);
                        }
                        counts[pIdx] = 0;
                    }
                }
                return localState;
            },
            (localState) =>
            {
                var (buffers, counts) = localState;
                for (int p = 0; p < partitionCount; p++)
                {
                    if (counts[p] > 0)
                    {
                        lock (globalLocks[p])
                        {
                            var gBucket = globalBuckets[p];
                            var buffer = buffers[p];
                            int count = counts[p];
                            for (int k = 0; k < count; k++) gBucket.Add(buffer[k]);
                        }
                    }
                }
            });

            var partialResults = new (string[] Keys, int[] Offsets, int[] Indices)[partitionCount];
            Parallel.For(0, partitionCount, p =>
            {
                var indices = globalBuckets[p];
                if (indices.Count == 0)
                {
                    partialResults[p] = (Array.Empty<string>(), new int[] { 0 }, Array.Empty<int>());
                    return;
                }
                var map = new StringKeyMap(rawBytes, offsets, indices.Count, indices.Count);
                for (int k = 0; k < indices.Count; k++) map.AddRow(indices[k], k);
                partialResults[p] = map.ToCSR();
                map.Dispose();
            });

            int totalGroups = 0;
            int totalIndices = 0;
            foreach (var pr in partialResults) { totalGroups += pr.Keys.Length; totalIndices += pr.Indices.Length; }

            var finalKeys = new string[totalGroups];
            var finalOffsets = new int[totalGroups + 1];
            var finalIndices = new int[totalIndices];
            int keyOffset = 0; int indexOffset = 0; int currentOffsetValue = 0;
            finalOffsets[0] = 0;

            for (int p = 0; p < partitionCount; p++)
            {
                var (pKeys, pOffsets, pIndices) = partialResults[p];
                int pGroupCount = pKeys.Length;
                if (pGroupCount == 0) continue;
                Array.Copy(pKeys, 0, finalKeys, keyOffset, pGroupCount);
                var currentBucket = globalBuckets[p];
                for (int j = 0; j < pIndices.Length; j++) finalIndices[indexOffset + j] = currentBucket[pIndices[j]];
                for (int i = 0; i < pGroupCount; i++)
                {
                    int groupSize = pOffsets[i + 1] - pOffsets[i];
                    currentOffsetValue += groupSize;
                    finalOffsets[keyOffset + i + 1] = currentOffsetValue;
                }
                keyOffset += pGroupCount;
                indexOffset += pIndices.Length;
            }
            return new GroupedDataFrame<string>(df, new[] { columnName }, finalKeys, finalOffsets, finalIndices, !nullIndices.IsEmpty ? nullIndices.ToArray() : null);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/MultiColumnHashStrategy.cs =====
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Engine.Kernels.GroupBy.Strategies
{
    internal unsafe class MultiColumnHashStrategy : IGroupByStrategy
    {
        public GroupedDataFrame Group(DataFrame df, string ignoredColumnName)
        {
            throw new InvalidOperationException("Use the overload with multiple columns.");
        }

        public GroupedDataFrame Group(DataFrame df, string[] columnNames)
        {
            int rowCount = df.RowCount;
            if (rowCount == 0) return new GroupedDataFrame<int>(df, columnNames, Array.Empty<int>(), new[] { 0 }, Array.Empty<int>(), null);

            var columns = new IColumn[columnNames.Length];
            for (int i = 0; i < columnNames.Length; i++) columns[i] = df[columnNames[i]];

            int* pHashes = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));

            int* pMap = null;
            int* pGroupIds = null;

            try
            {
                ComputeCombinedHashes(columns, pHashes, rowCount);

                int capacity = GetNextPowerOfTwo(rowCount * 2);
                if (capacity < 16) capacity = 16;
                int mask = capacity - 1;

                pMap = (int*)NativeMemory.Alloc((nuint)(capacity * sizeof(int)));
                new Span<int>(pMap, capacity).Fill(-1);

                pGroupIds = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));

                int groupCount = 0;

                for (int i = 0; i < rowCount; i++)
                {
                    int hash = pHashes[i];
                    int slot = hash & mask;

                    while (true)
                    {
                        int existingRow = pMap[slot];

                        if (existingRow == -1)
                        {
                            pMap[slot] = i;
                            pGroupIds[i] = groupCount++;
                            break;
                        }

                        if (RowsEqual(columns, i, existingRow))
                        {
                            pGroupIds[i] = pGroupIds[existingRow];
                            break;
                        }

                        slot = (slot + 1) & mask;
                    }
                }

                return BuildCsrFromGroupIds(df, columnNames, pGroupIds, rowCount, groupCount);
            }
            finally
            {
                if (pHashes != null) NativeMemory.Free(pHashes);
                if (pMap != null) NativeMemory.Free(pMap);
                if (pGroupIds != null) NativeMemory.Free(pGroupIds);
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private void ComputeCombinedHashes(IColumn[] columns, int* pHashes, int rowCount)
        {
            new Span<int>(pHashes, rowCount).Fill(0);

            foreach (var col in columns)
            {
                const int Multiplier = 397;

                if (col is IntColumn ic)
                {
                    var span = ic.Values.Span;
                    for (int i = 0; i < rowCount; i++)
                    {
                        pHashes[i] = (pHashes[i] * Multiplier) ^ span[i];
                    }
                }
                else if (col is StringColumn sc)
                {
                    for (int i = 0; i < rowCount; i++)
                    {
                        int h = sc.GetHashCodeRaw(i);
                        pHashes[i] = (pHashes[i] * Multiplier) ^ h;
                    }
                }
                else
                {
                    for (int i = 0; i < rowCount; i++)
                    {
                        object? val = col.GetValue(i);
                        int h = val?.GetHashCode() ?? 0;
                        pHashes[i] = (pHashes[i] * Multiplier) ^ h;
                    }
                }
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private bool RowsEqual(IColumn[] columns, int rowA, int rowB)
        {
            foreach (var col in columns)
            {
                if (col is IntColumn ic)
                {
                    if (ic.Get(rowA) != ic.Get(rowB)) return false;
                }
                else if (col is StringColumn sc)
                {
                    if (sc.CompareRaw(rowA, rowB) != 0) return false;
                }
                else
                {
                    if (!object.Equals(col.GetValue(rowA), col.GetValue(rowB))) return false;
                }
            }
            return true;
        }

        private GroupedDataFrame BuildCsrFromGroupIds(DataFrame df, string[] cols, int* pGroupIds, int rowCount, int groupCount)
        {
            int[] offsets = new int[groupCount + 1];
            int[] indices = new int[rowCount];

            int[] counts = new int[groupCount];
            for (int i = 0; i < rowCount; i++) counts[pGroupIds[i]]++;

            int current = 0;
            for (int i = 0; i < groupCount; i++)
            {
                offsets[i] = current;
                current += counts[i];
            }
            offsets[groupCount] = current;

            var writePos = new int[groupCount];
            Array.Copy(offsets, writePos, groupCount);

            for (int i = 0; i < rowCount; i++)
            {
                int gid = pGroupIds[i];
                int pos = writePos[gid]++;
                indices[pos] = i;
            }

            int[] representativeRows = new int[groupCount];
            for (int i = 0; i < groupCount; i++)
            {
                representativeRows[i] = indices[offsets[i]];
            }

            return new GroupedDataFrame<int>(df, cols, representativeRows, offsets, indices, null);
        }

        private int GetNextPowerOfTwo(int x)
        {
            int power = 1;
            while (power < x) power *= 2;
            return power;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/NativeGroupedData.cs =====
using LeichtFrame.Core.Engine.Memory;

namespace LeichtFrame.Core.Engine
{
    /// <summary>
    /// H√§lt das Ergebnis eines GroupBy im unmanaged Memory.
    /// Wird verwendet, um Allocations zu vermeiden, bis der User die Daten wirklich braucht.
    /// </summary>
    internal unsafe class NativeGroupedData : IDisposable
    {
        public UnsafeBuffer<int> Keys;      // Die Gruppen-Keys
        public UnsafeBuffer<int> Offsets;   // CSR Offsets
        public UnsafeBuffer<int> Indices;   // CSR Indices (Sorted row pointers)
        public int GroupCount;
        public int RowCount;

        public bool IsDisposed { get; private set; }

        public NativeGroupedData(int rowCount, int groupCount)
        {
            RowCount = rowCount;
            GroupCount = groupCount;

            // Wir allokieren im Native Heap -> GC sieht das nicht!
            Indices = new UnsafeBuffer<int>(rowCount);
            Offsets = new UnsafeBuffer<int>(groupCount + 1);
            Keys = new UnsafeBuffer<int>(groupCount);
        }

        public void Dispose()
        {
            if (!IsDisposed)
            {
                Keys.Dispose();
                Offsets.Dispose();
                Indices.Dispose();
                IsDisposed = true;
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/ParallelSwissMapExecutor.cs =====
using System.Runtime.InteropServices;
using LeichtFrame.Core.Engine.Algorithms.Partitioning;
using LeichtFrame.Core.Engine.Collections;

namespace LeichtFrame.Core.Engine.Kernels.GroupBy.Strategies
{
    internal static unsafe class ParallelSwissMapExecutor
    {
        private const int MinRowsForParallel = 500_000;

        // ---------------------------------------------------------
        // STRING VARIANT
        // ---------------------------------------------------------
        public static NativeGroupedData? TryExecuteString(
            byte* pBytes, int* pOffsets, int* pHashes, int rowCount)
        {
            if (rowCount < MinRowsForParallel) return null;

            int partitionCount = DeterminePartitionCount(rowCount);
            int shift = 32 - System.Numerics.BitOperations.Log2((uint)partitionCount);
            int[] partitionOffsets = new int[partitionCount + 1];

            // 1. Partitionierung (Shuffle)
            RadixPartitioner.Partition(
                pHashes, rowCount, partitionCount, shift,
                out int* pPartHashes, out int* pPartRowIndices, partitionOffsets
            );

            try
            {
                // 2. Parallel Build (Independent Maps)
                var partitionResults = new NativeGroupedData[partitionCount];

                Parallel.For(0, partitionCount, p =>
                {
                    int start = partitionOffsets[p];
                    int end = partitionOffsets[p + 1];
                    int len = end - start;

                    if (len == 0)
                    {
                        partitionResults[p] = new NativeGroupedData(0, 0);
                        return;
                    }

                    var map = new NativeStringMap(Math.Max(64, len), pBytes, pOffsets);
                    int* pLocalGroupIds = (int*)NativeMemory.Alloc((nuint)(len * sizeof(int)));

                    try
                    {
                        for (int i = 0; i < len; i++)
                        {
                            int globalRowIdx = pPartRowIndices[start + i];
                            int hash = pPartHashes[start + i];

                            pLocalGroupIds[i] = map.GetOrAdd(globalRowIdx, hash);
                        }

                        int groupCount = map.Count;
                        var localRes = new NativeGroupedData(len, groupCount);

                        int[] keys = map.ExportKeysAsRowIndices();
                        Marshal.Copy(keys, 0, (nint)localRes.Keys.Ptr, groupCount);

                        BuildCsr(pLocalGroupIds, localRes, len, groupCount);
                        partitionResults[p] = localRes;
                    }
                    finally
                    {
                        map.Dispose();
                        NativeMemory.Free(pLocalGroupIds);
                    }
                });

                // 3. Merge Results
                return MergeResults(partitionResults, pPartRowIndices, partitionOffsets, rowCount);
            }
            finally
            {
                NativeMemory.Free(pPartHashes);
                NativeMemory.Free(pPartRowIndices);
            }
        }

        // ---------------------------------------------------------
        // HELPERS
        // ---------------------------------------------------------

        private static void BuildCsr(int* groupIds, NativeGroupedData res, int len, int groupCount)
        {
            int* pOffsets = res.Offsets.Ptr;
            int* pIndices = res.Indices.Ptr;

            new Span<int>(pOffsets, groupCount + 1).Fill(0);

            // Histogram
            for (int i = 0; i < len; i++) pOffsets[groupIds[i]]++;

            // Prefix Sum & Temp Write Heads
            int current = 0;
            int* writeHeads = (int*)NativeMemory.Alloc((nuint)(groupCount * 4));
            for (int i = 0; i < groupCount; i++)
            {
                int c = pOffsets[i];
                pOffsets[i] = current;
                writeHeads[i] = current;
                current += c;
            }
            pOffsets[groupCount] = current;

            // Scatter
            for (int i = 0; i < len; i++)
            {
                int gid = groupIds[i];
                pIndices[writeHeads[gid]++] = i;
            }
            NativeMemory.Free(writeHeads);
        }

        private static NativeGroupedData MergeResults(
            NativeGroupedData[] parts,
            int* pPartRowIndices,
            int[] partitionOffsets,
            int totalRows)
        {
            int totalGroups = 0;
            foreach (var p in parts) totalGroups += p.GroupCount;

            var result = new NativeGroupedData(totalRows, totalGroups);

            int globalGroupOffset = 0;
            int globalIndexOffset = 0;

            result.Offsets.Ptr[0] = 0;

            for (int p = 0; p < parts.Length; p++)
            {
                var part = parts[p];
                int count = part.GroupCount;
                if (count == 0) { part.Dispose(); continue; }

                nuint byteCountKeys = (nuint)(count * sizeof(int));
                NativeMemory.Copy(part.Keys.Ptr, result.Keys.Ptr + globalGroupOffset, byteCountKeys);

                int* pLocalOffsets = part.Offsets.Ptr;
                int* pGlobalOffsets = result.Offsets.Ptr + globalGroupOffset;

                for (int i = 0; i < count; i++)
                {
                    int groupSize = pLocalOffsets[i + 1] - pLocalOffsets[i];
                    globalIndexOffset += groupSize;
                    pGlobalOffsets[i + 1] = globalIndexOffset;
                }

                int partStartInGlobalArray = partitionOffsets[p];
                int partRowCount = part.RowCount;

                int writeStart = result.Offsets.Ptr[globalGroupOffset];

                int* pLocalIndices = part.Indices.Ptr;
                int* pGlobalIndices = result.Indices.Ptr;

                for (int i = 0; i < partRowCount; i++)
                {
                    int localIdx = pLocalIndices[i];
                    int globalRowIdx = pPartRowIndices[partStartInGlobalArray + localIdx];
                    pGlobalIndices[writeStart + i] = globalRowIdx;
                }

                globalGroupOffset += count;
                part.Dispose();
            }

            return result;
        }

        private static int DeterminePartitionCount(int rowCount)
        {
            int targetSize = 100_000;
            int parts = rowCount / targetSize;

            if (parts < 16) return 16;
            if (parts > 1024) return 1024;
            return (int)System.Numerics.BitOperations.RoundUpToPowerOf2((uint)parts);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/RowLayoutHashStrategy.cs =====
using System.Runtime.InteropServices;
using LeichtFrame.Core.Engine.Collections;
using LeichtFrame.Core.Engine.Algorithms.Packing;

namespace LeichtFrame.Core.Engine.Kernels.GroupBy.Strategies
{
    internal class RowLayoutHashStrategy : IGroupByStrategy
    {
        public GroupedDataFrame Group(DataFrame df, string ignored) => throw new NotSupportedException();

        public unsafe GroupedDataFrame Group(DataFrame df, string[] cols)
        {
            // 1. Pack Rows
            var packed = RowLayoutPacking.Pack(df, cols);
            byte* pRows = (byte*)packed.Buffer;
            int width = packed.Width;
            int rowCount = df.RowCount;

            // 2. Hash Rows (Vectorized over Packed Data)
            int* pHashes = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));
            // VectorizedHasher.HashBytesStride(pRows, width, rowCount, pHashes) -> TODO
            // Fallback Scalar Hash Loop:
            for (int i = 0; i < rowCount; i++)
            {
                byte* pRow = pRows + (i * width);
                int h = unchecked((int)2166136261);
                for (int b = 0; b < width; b++) h = (h ^ pRow[b]) * 16777619;
                pHashes[i] = h;
            }

            // 3. Map Build
            var map = new NativeRowMap(Math.Max(1024, rowCount / 10), pRows, width);
            int* pRowToGroupId = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));

            try
            {
                for (int i = 0; i < rowCount; i++)
                {
                    pRowToGroupId[i] = map.GetOrAdd(i, pHashes[i]);
                }

                // 4. CSR Construction (Standard)
                int groupCount = map.Count;
                var resultNative = new NativeGroupedData(rowCount, groupCount);
                int* pOffsets = resultNative.Offsets.Ptr;
                int* pIndices = resultNative.Indices.Ptr;
                int* pKeys = resultNative.Keys.Ptr;

                int[] repRowIndices = map.ExportKeysAsRowIndices();
                Marshal.Copy(repRowIndices, 0, (nint)pKeys, groupCount);

                // Histogram, PrefixSum, Scatter (Standard Copy-Paste from other strategies)
                // ... (K√ºrzen wir hier ab, ist identisch zu IntSwissMap) ...
                new Span<int>(pOffsets, groupCount + 1).Fill(0);
                for (int i = 0; i < rowCount; i++) pOffsets[pRowToGroupId[i]]++;

                int current = 0;
                int* pWrite = (int*)NativeMemory.Alloc((nuint)(groupCount * 4));
                for (int i = 0; i < groupCount; i++) { int c = pOffsets[i]; pOffsets[i] = current; pWrite[i] = current; current += c; }
                pOffsets[groupCount] = current;

                for (int i = 0; i < rowCount; i++)
                {
                    int gid = pRowToGroupId[i];
                    pIndices[pWrite[gid]++] = i;
                }
                NativeMemory.Free(pWrite);

                var gdf = new GroupedDataFrame<int>(df, cols, resultNative, null);
                gdf.KeysAreRowIndices = true; // Wichtig f√ºr Rekonstruktion!
                return gdf;
            }
            finally
            {
                map.Dispose();
                NativeMemory.Free(pHashes);
                NativeMemory.Free(pRowToGroupId);
                NativeMemory.Free(pRows); // Packed Buffer free
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/StringDictionaryStrategy.cs =====
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Engine.Kernels.GroupBy.Strategies
{
    internal class StringDictionaryStrategy : IGroupByStrategy
    {
        private const int MaxCardinality = 65_536;

        public unsafe GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = (StringColumn)df[columnName];

            var dict = new Dictionary<string, int>(1024);
            int nextId = 0;

            int[] codes = new int[df.RowCount];
            bool aborted = false;
            bool hasNulls = false;
            int nullId = 0;
            int stringStartId = 1;

            var reverseMap = new List<string?>();
            reverseMap.Add(null);

            for (int i = 0; i < df.RowCount; i++)
            {
                if (col.IsNull(i))
                {
                    hasNulls = true;
                    codes[i] = nullId;
                    continue;
                }

                string? val = col.Get(i);

                if (!dict.TryGetValue(val!, out int id))
                {
                    if (nextId >= MaxCardinality)
                    {
                        aborted = true;
                        break;
                    }

                    id = stringStartId + nextId;
                    dict[val!] = id;
                    reverseMap.Add(val);
                    nextId++;
                }
                codes[i] = id;
            }

            if (aborted)
            {
                return new StringSwissMapStrategy().Group(df, columnName);
            }

            var codeCol = new IntColumn("Codes", codes, df.RowCount);

            var histogramStrategy = new DirectAddressingStrategy(min: 0, max: reverseMap.Count);

            NativeGroupedData nativeResult = histogramStrategy.ComputeNative(codeCol, df.RowCount);

            int groupCount = nativeResult.GroupCount;
            string[] stringKeys = new string[groupCount];
            int[] nativeKeys = new int[groupCount];

            Marshal.Copy((nint)nativeResult.Keys.Ptr, nativeKeys, 0, groupCount);

            var cleanKeys = new List<string>();
            var cleanOffsets = new List<int>();

            return new DictionaryGroupedDataFrame(
                df,
                new[] { columnName },
                nativeResult,
                reverseMap.ToArray(),
                hasNullCodeZero: hasNulls
            );
        }
    }

    internal class DictionaryGroupedDataFrame : GroupedDataFrame
    {
        private readonly string?[] _dictionary;
        private readonly bool _hasNullCodeZero;
        private string[]? _resolvedKeys;
        private int[]? _nullIndices;
        private int[]? _offsets;
        private int[]? _indices;
        private int _realGroupCount;

        public DictionaryGroupedDataFrame(
            DataFrame df,
            string[] colNames,
            NativeGroupedData nativeData,
            string?[] dictionary,
            bool hasNullCodeZero)
            : base(df, colNames, nativeData)
        {
            _dictionary = dictionary;
            _hasNullCodeZero = hasNullCodeZero;

            EnsureParsed();
            nativeData.Dispose();
            NativeData = null;
        }

        private unsafe void EnsureParsed()
        {
            if (_resolvedKeys != null) return;

            int rawCount = NativeData!.GroupCount;
            int[] rawKeys = new int[rawCount];
            int[] rawOffsets = new int[rawCount + 1];

            Marshal.Copy((nint)NativeData.Keys.Ptr, rawKeys, 0, rawCount);
            Marshal.Copy((nint)NativeData.Offsets.Ptr, rawOffsets, 0, rawCount + 1);

            var finalKeys = new List<string>();
            var finalOffsets = new List<int> { 0 };

            bool nullGroupFound = _hasNullCodeZero && rawCount > 0 && rawKeys[0] == 0;

            int startGroupIdx = nullGroupFound ? 1 : 0;
            _realGroupCount = rawCount - startGroupIdx;

            if (nullGroupFound)
            {
                int start = rawOffsets[0];
                int end = rawOffsets[1];
                int len = end - start;
                _nullIndices = new int[len];
                Marshal.Copy((nint)NativeData.Indices.Ptr + (start * 4), _nullIndices, 0, len);
            }

            _resolvedKeys = new string[_realGroupCount];
            for (int i = 0; i < _realGroupCount; i++)
            {
                int code = rawKeys[i + startGroupIdx];
                _resolvedKeys[i] = _dictionary[code]!;
            }

            _offsets = new int[_realGroupCount + 1];
            int shift = nullGroupFound ? rawOffsets[1] : 0;

            for (int i = 0; i <= _realGroupCount; i++)
            {
                _offsets[i] = rawOffsets[i + startGroupIdx] - shift;
            }

            int totalIndices = rawOffsets[rawCount] - shift;
            _indices = new int[totalIndices];
            Marshal.Copy((nint)NativeData.Indices.Ptr + (shift * 4), _indices, 0, totalIndices);
        }

        public override int GroupCount { get { EnsureParsed(); return _realGroupCount; } }
        public override Array GetKeys() { EnsureParsed(); return _resolvedKeys!; }
        public override int[] GroupOffsets { get { EnsureParsed(); return _offsets!; } }
        public override int[] RowIndices { get { EnsureParsed(); return _indices!; } }
        public override int[]? NullGroupIndices { get { EnsureParsed(); return _nullIndices; } }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/GroupBy/Strategies/StringSwissMapStrategy.cs =====
using System.Runtime.InteropServices;
using LeichtFrame.Core.Engine.Collections;

namespace LeichtFrame.Core.Engine.Kernels.GroupBy.Strategies
{
    internal class StringSwissMapStrategy : IGroupByStrategy
    {
        public GroupedDataFrame Group(DataFrame df, string columnName)
        {
            var col = (StringColumn)df[columnName];
            return GroupNative(df, columnName, col);
        }

        private unsafe GroupedDataFrame GroupNative(DataFrame df, string colName, StringColumn col)
        {
            int rowCount = df.RowCount;
            if (rowCount == 0)
                return new GroupedDataFrame<string>(df, new[] { colName }, Array.Empty<string>(), new[] { 0 }, Array.Empty<int>(), null);

            int* pHashes = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));

            fixed (byte* pBytes = col.RawBytes)
            fixed (int* pOffsets = col.Offsets)
            {
                VectorizedHasher.HashStrings(pBytes, pOffsets, pHashes, rowCount);

                if (!col.IsNullable)
                {
                    var parallelRes = ParallelSwissMapExecutor.TryExecuteString(pBytes, pOffsets, pHashes, rowCount);
                    if (parallelRes != null)
                    {
                        var gdf = new GroupedDataFrame<string>(df, new[] { colName }, parallelRes, null);
                        gdf.KeysAreRowIndices = true;
                        NativeMemory.Free(pHashes);
                        return gdf;
                    }
                }

                int* pRowToGroupId = (int*)NativeMemory.Alloc((nuint)(rowCount * sizeof(int)));
                var nullIndices = new List<int>();

                var map = new NativeStringMap(Math.Max(1024, rowCount / 10), pBytes, pOffsets);

                try
                {
                    // -- PHASE 1: INSERT --
                    for (int i = 0; i < rowCount; i++)
                    {
                        if (col.IsNull(i))
                        {
                            nullIndices.Add(i);
                            pRowToGroupId[i] = -1;
                            continue;
                        }

                        pRowToGroupId[i] = map.GetOrAdd(i, pHashes[i]);
                    }

                    int groupCount = map.Count;

                    // -- PHASE 2: CSR --

                    var resultNative = new NativeGroupedData(rowCount, groupCount);
                    int* pOffsetsRes = resultNative.Offsets.Ptr;
                    int* pIndicesRes = resultNative.Indices.Ptr;
                    int* pKeysRes = resultNative.Keys.Ptr;

                    // A. Keys
                    int[] repRowIndices = map.ExportKeysAsRowIndices();
                    Marshal.Copy(repRowIndices, 0, (nint)pKeysRes, groupCount);

                    // B. Histogramm
                    new Span<int>(pOffsetsRes, groupCount + 1).Fill(0);
                    for (int i = 0; i < rowCount; i++)
                    {
                        int gid = pRowToGroupId[i];
                        if (gid != -1) pOffsetsRes[gid]++;
                    }

                    // C. Prefix Sum
                    int currentOffset = 0;
                    int* pWriteHeads = (int*)NativeMemory.Alloc((nuint)(groupCount * sizeof(int)));
                    for (int i = 0; i < groupCount; i++)
                    {
                        int c = pOffsetsRes[i];
                        pOffsetsRes[i] = currentOffset;
                        pWriteHeads[i] = currentOffset;
                        currentOffset += c;
                    }
                    pOffsetsRes[groupCount] = currentOffset;

                    // D. Scatter
                    for (int i = 0; i < rowCount; i++)
                    {
                        int gid = pRowToGroupId[i];
                        if (gid != -1)
                        {
                            int dest = pWriteHeads[gid];
                            pIndicesRes[dest] = i;
                            pWriteHeads[gid]++;
                        }
                    }
                    NativeMemory.Free(pWriteHeads);

                    var gdf = new GroupedDataFrame<string>(
                        df,
                        new[] { colName },
                        resultNative,
                        nullIndices.Count > 0 ? nullIndices.ToArray() : null
                    );

                    gdf.KeysAreRowIndices = true;

                    return gdf;
                }
                finally
                {
                    map.Dispose();
                    NativeMemory.Free(pRowToGroupId);
                    NativeMemory.Free(pHashes);
                }
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/Join/IJoinStrategy.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Join/JoinDispatcher.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Rename/IRenameStrategy.cs =====
namespace LeichtFrame.Core.Engine
{
    internal interface IRenameStrategy
    {
        /// <summary>
        /// Renames a column. Should aim for Zero-Copy (Shallow Clone) if possible.
        /// </summary>
        IColumn Rename(IColumn col, string newName);
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/Rename/RenameDispatcher.cs =====
namespace LeichtFrame.Core.Engine.Kernels.Rename
{
    /// <summary>
    /// Rename Dispatcher
    /// </summary>
    public static class RenameDispatcher
    {
        private static readonly IRenameStrategy _strategy = new ShallowCopyRenameStrategy();

        /// <summary>
        /// Execute rename
        /// </summary>
        /// <param name="col"></param>
        /// <param name="newName"></param>
        /// <returns></returns>
        public static IColumn Execute(IColumn col, string newName)
        {
            return _strategy.Rename(col, newName);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/Rename/Strategies/ShallowCopyRenameStrategy.cs =====
using System.Reflection;

namespace LeichtFrame.Core.Engine
{
    internal class ShallowCopyRenameStrategy : IRenameStrategy
    {
        private static readonly FieldInfo? _nameField = typeof(Column).GetField("<Name>k__BackingField",
            BindingFlags.Instance | BindingFlags.NonPublic);

        public IColumn Rename(IColumn col, string newName)
        {
            if (col.Name == newName) return col;

            if (col is Column baseCol)
            {
                var newCol = baseCol.ShallowClone();

                if (_nameField != null)
                {
                    _nameField.SetValue(newCol, newName);
                }
                else
                {
                    throw new InvalidOperationException("Cannot rename: Name backing field not found.");
                }

                return newCol;
            }

            throw new NotSupportedException($"Rename not supported for type {col.GetType().Name}");
        }
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Kernels/Sort/ISortStrategy.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Sort/SortDispatcher.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Transform/ITransformStrategy.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Kernels/Transform/TransformDispatcher.cs =====

===== FILE: src/LeichtFrame.Core/Engine/Memory/Buffers/UnsafeBuffer.cs =====
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Engine.Memory
{
    /// <summary>
    /// Wrapper around NativeMemory to avoid GC pressure for large internal buffers.
    /// </summary>
    internal unsafe struct UnsafeBuffer<T> : IDisposable where T : unmanaged
    {
        public T* Ptr;
        public int Length;

        public UnsafeBuffer(int length)
        {
            Length = length;
            Ptr = (T*)NativeMemory.Alloc((nuint)length, (nuint)sizeof(T));
        }

        public void Dispose()
        {
            if (Ptr != null)
            {
                NativeMemory.Free(Ptr);
                Ptr = null;
            }
        }

        public ref T this[int index] => ref Ptr[index];
    }
}
===== FILE: src/LeichtFrame.Core/Engine/Memory/Tracking/NullBitmap.cs =====
using System.Buffers;
using System.Runtime.CompilerServices;

namespace LeichtFrame.Core.Engine.Memory
{
    /// <summary>
    /// A memory-efficient bitset used to track null values in nullable columns.
    /// Uses 1 bit per row, resulting in very low memory overhead (approx. 1.5% of an integer array).
    /// </summary>
    public class NullBitmap : IDisposable
    {
        private ulong[] _buffer;
        private int _capacity;

        /// <summary>
        /// Initializes a new instance of the <see cref="NullBitmap"/> class.
        /// </summary>
        /// <param name="capacity">The initial number of bits (rows) to support.</param>
        public NullBitmap(int capacity)
        {
            _capacity = capacity;
            // Calculate how many ulongs are needed to cover 'capacity' bits
            int ulongCount = (capacity + 63) >> 6;
            _buffer = ArrayPool<ulong>.Shared.Rent(ulongCount);

            // IMPORTANT: Arrays from ArrayPool are "dirty", we need to clear them.
            Array.Clear(_buffer, 0, ulongCount);
        }

        /// <summary>
        /// Checks if the bit at the specified index is set (meaning the value is null).
        /// </summary>
        /// <param name="index">The zero-based index to check.</param>
        /// <returns><c>true</c> if the bit is set (null); otherwise, <c>false</c>.</returns>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public bool IsNull(int index)
        {
            // index >> 6 is identical to index / 64, but often faster
            // index & 63 is identical to index % 64
            return (_buffer[index >> 6] & (1UL << (index & 63))) != 0;
        }

        /// <summary>
        /// Sets the bit at the specified index (marking the value as null).
        /// </summary>
        /// <param name="index">The zero-based index to set.</param>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public void SetNull(int index)
        {
            _buffer[index >> 6] |= (1UL << (index & 63));
        }

        /// <summary>
        /// Clears the bit at the specified index (marking the value as not null).
        /// </summary>
        /// <param name="index">The zero-based index to clear.</param>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public void SetNotNull(int index)
        {
            _buffer[index >> 6] &= ~(1UL << (index & 63));
        }

        /// <summary>
        /// Merges two bitmaps using bitwise OR. 
        /// Result has a bit set (is null) if either A OR B has that bit set.
        /// </summary>
        public static NullBitmap? MergeOr(NullBitmap? a, NullBitmap? b, int length)
        {
            if (a == null && b == null) return null;

            var result = new NullBitmap(length);

            // Access internal buffers via Unsafe or assumes friend access? 
            // For MVP clean code, we iterate ulongs. 
            // Since we don't expose the ulong array publicly, we implement the logic here.

            int ulongCount = (length + 63) >> 6;

            // Note: This relies on the internal _buffer. 
            // If we are strictly outside, we can't access _buffer.
            // Let's assume for this specific internal helper we can access it 
            // or we implement it as an instance method "Or(other)".

            // Let's implement the logic assuming we act on 'result' using 'a' and 'b'.
            // To be safe and clean without friend-assemblies:
            for (int i = 0; i < ulongCount; i++)
            {
                ulong valA = (a != null && i < a._buffer.Length) ? a._buffer[i] : 0;
                ulong valB = (b != null && i < b._buffer.Length) ? b._buffer[i] : 0;
                result._buffer[i] = valA | valB;
            }

            return result;
        }

        /// <summary>
        /// Resizes the internal buffer to accommodate at least the specified number of bits.
        /// Preserves existing data.
        /// </summary>
        /// <param name="newCapacity">The new minimum capacity.</param>
        public void Resize(int newCapacity)
        {
            if (newCapacity <= _capacity) return;

            int oldUlongCount = (_capacity + 63) >> 6;
            int newUlongCount = (newCapacity + 63) >> 6;

            // Case 1: Buffer too small -> New buffer needed
            if (newUlongCount > _buffer.Length)
            {
                var newBuffer = ArrayPool<ulong>.Shared.Rent(newUlongCount);

                // Save old data
                Array.Copy(_buffer, newBuffer, oldUlongCount);

                // Clear the new area in the new buffer
                Array.Clear(newBuffer, oldUlongCount, newUlongCount - oldUlongCount);

                ArrayPool<ulong>.Shared.Return(_buffer);
                _buffer = newBuffer;
            }
            // Case 2: Buffer still large enough, but we now use more "words" from it
            else if (newUlongCount > oldUlongCount)
            {
                // Clear the "freshly uncovered" area in the existing dirty buffer
                Array.Clear(_buffer, oldUlongCount, newUlongCount - oldUlongCount);
            }

            _capacity = newCapacity;
        }

        /// <inheritdoc />
        public void Dispose()
        {
            if (_buffer != null)
            {
                ArrayPool<ulong>.Shared.Return(_buffer);
                _buffer = null!;
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Execution/Compilation/ExpressionCompiler.cs =====
using System.Linq.Expressions;
using LeichtFrame.Core.Expressions;

namespace LeichtFrame.Core.Execution.Compilation
{
    /// <summary>
    /// Double Kernel
    /// </summary>
    /// <param name="length"></param>
    /// <param name="result"></param>
    /// <param name="inputs"></param>
    public delegate void DoubleKernel(int length, double[] result, double[][] inputs);

    /// <summary>
    /// Int Kernel
    /// </summary>
    /// <param name="length"></param>
    /// <param name="result"></param>
    /// <param name="inputs"></param>
    public delegate void IntKernel(int length, int[] result, int[][] inputs);

    /// <summary>
    /// Expression Compiler for double and int
    /// </summary>
    public static class ExpressionCompiler
    {
        /// <summary>
        /// Compiles double
        /// </summary>
        /// <param name="rootExpr"></param>
        /// <param name="colMapping"></param>
        /// <returns></returns>
        public static DoubleKernel CompileDouble(Expr rootExpr, Dictionary<string, int> colMapping)
        {
            return Compile<DoubleKernel, double>(rootExpr, colMapping);
        }

        /// <summary>
        /// Compiles int
        /// </summary>
        /// <param name="rootExpr"></param>
        /// <param name="colMapping"></param>
        /// <returns></returns>
        public static IntKernel CompileInt(Expr rootExpr, Dictionary<string, int> colMapping)
        {
            return Compile<IntKernel, int>(rootExpr, colMapping);
        }

        private static TKernel Compile<TKernel, T>(Expr rootExpr, Dictionary<string, int> colMapping)
        {
            var typeT = typeof(T);
            var typeArr = typeof(T[]);
            var typeArrArr = typeof(T[][]);

            var paramLength = Expression.Parameter(typeof(int), "length");
            var paramResult = Expression.Parameter(typeArr, "result");
            var paramInputs = Expression.Parameter(typeArrArr, "inputs");

            var varI = Expression.Variable(typeof(int), "i");
            var breakLabel = Expression.Label();

            var calculation = Visit<T>(rootExpr, varI, paramInputs, colMapping);

            var loopBody = Expression.Assign(
                Expression.ArrayAccess(paramResult, varI),
                calculation
            );

            var loop = Expression.Block(
                new[] { varI },
                Expression.Assign(varI, Expression.Constant(0)),
                Expression.Loop(
                    Expression.IfThenElse(
                        Expression.LessThan(varI, paramLength),
                        Expression.Block(
                            loopBody,
                            Expression.PostIncrementAssign(varI)
                        ),
                        Expression.Break(breakLabel)
                    ),
                    breakLabel
                )
            );

            var lambda = Expression.Lambda<TKernel>(loop, paramLength, paramResult, paramInputs);
            return lambda.Compile();
        }

        private static Expression Visit<T>(Expr node, ParameterExpression varI, ParameterExpression paramInputs, Dictionary<string, int> colMapping)
        {
            if (node is BinaryExpr b)
            {
                var left = Visit<T>(b.Left, varI, paramInputs, colMapping);
                var right = Visit<T>(b.Right, varI, paramInputs, colMapping);

                return b.Op switch
                {
                    BinaryOp.Add => Expression.Add(left, right),
                    BinaryOp.Subtract => Expression.Subtract(left, right),
                    BinaryOp.Multiply => Expression.Multiply(left, right),
                    BinaryOp.Divide => Expression.Divide(left, right),
                    _ => throw new NotSupportedException($"Op {b.Op} not supported in JIT")
                };
            }

            if (node is ColExpr c)
            {
                int colIndex = colMapping[c.Name];
                var colArray = Expression.ArrayAccess(paramInputs, Expression.Constant(colIndex));
                return Expression.ArrayAccess(colArray, varI);
            }

            if (node is LitExpr l)
            {
                var val = Convert.ChangeType(l.Value, typeof(T));
                return Expression.Constant(val);
            }

            if (node is AliasExpr a)
            {
                return Visit<T>(a.Child, varI, paramInputs, colMapping);
            }

            throw new NotSupportedException($"Node {node.GetType().Name} not supported in JIT");
        }
    }
}
===== FILE: src/LeichtFrame.Core/Execution/Evaluator.cs =====
using System.Buffers;
using System.Reflection;
using LeichtFrame.Core.Expressions;
using LeichtFrame.Core.Execution.Compilation;

namespace LeichtFrame.Core.Execution
{
    internal static class Evaluator
    {
        private static readonly Dictionary<Expr, DoubleKernel> _doubleKernelCache = new();
        private static readonly Dictionary<Expr, IntKernel> _intKernelCache = new();

        public static IColumn Evaluate(Expr expr, DataFrame df)
        {
            if (expr is ColExpr c) return df[c.Name];
            if (expr is AliasExpr a) return Evaluate(a.Child, df);
            if (expr is LitExpr l) return CreateLiteralColumn(l, df.RowCount);

            if (expr is BinaryExpr b)
            {
                return EvaluateBinary(b, df);
            }

            throw new NotImplementedException($"Expr type {expr.GetType().Name} not supported");
        }

        private static IColumn EvaluateBinary(BinaryExpr b, DataFrame df)
        {
            if (b.Right is LitExpr rightLit)
            {
                var leftCol = Evaluate(b.Left, df);
                return ExecuteScalar(leftCol, rightLit.Value, b.Op, leftIsScalar: false);
            }

            if (b.Left is LitExpr leftLit)
            {
                var rightCol = Evaluate(b.Right, df);
                return ExecuteScalar(rightCol, leftLit.Value, b.Op, leftIsScalar: true);
            }

            try
            {
                return EvaluateJit(b, df);
            }
            catch (Exception)
            {
                return EvaluateBinaryRecursiveFallback(b, df);
            }
        }

        private static IColumn EvaluateJit(Expr expr, DataFrame df)
        {
            var inputCols = new List<string>();
            CollectColumns(expr, inputCols);

            bool isDoubleMode = false;

            foreach (var name in inputCols)
            {
                var type = df[name].DataType;

                if (type == typeof(double))
                {
                    isDoubleMode = true;
                }
                else if (type != typeof(int))
                {
                    throw new NotSupportedException("JIT supports only Int/Double columns.");
                }
            }

            return isDoubleMode
                ? EvaluateJitDouble(expr, df, inputCols)
                : EvaluateJitInt(expr, df, inputCols);
        }

        private static IColumn EvaluateJitDouble(Expr expr, DataFrame df, List<string> inputCols)
        {
            var inputArrays = new double[inputCols.Count][];
            var colMapping = new Dictionary<string, int>();

            for (int i = 0; i < inputCols.Count; i++)
            {
                string name = inputCols[i];
                colMapping[name] = i;
                var col = df[name];

                if (col is DoubleColumn dc)
                {
                    inputArrays[i] = dc.Values.ToArray();
                }
                else if (col is IntColumn ic)
                {
                    inputArrays[i] = ic.Values.ToArray().Select(x => (double)x).ToArray();
                }
            }

            if (!_doubleKernelCache.TryGetValue(expr, out var kernel))
            {
                kernel = ExpressionCompiler.CompileDouble(expr, colMapping);
                _doubleKernelCache[expr] = kernel;
            }

            var resultArr = ArrayPool<double>.Shared.Rent(df.RowCount);

            kernel(df.RowCount, resultArr, inputArrays);

            return new DoubleColumn("JIT_Res_Dbl", resultArr, df.RowCount);
        }

        private static IColumn EvaluateJitInt(Expr expr, DataFrame df, List<string> inputCols)
        {
            var inputArrays = new int[inputCols.Count][];
            var colMapping = new Dictionary<string, int>();

            for (int i = 0; i < inputCols.Count; i++)
            {
                string name = inputCols[i];
                colMapping[name] = i;
                var col = df[name];

                if (col is IntColumn ic)
                {
                    inputArrays[i] = ic.Values.ToArray();
                }
                else
                {
                    throw new InvalidOperationException("Unexpected type in Int JIT");
                }
            }

            if (!_intKernelCache.TryGetValue(expr, out var kernel))
            {
                kernel = ExpressionCompiler.CompileInt(expr, colMapping);
                _intKernelCache[expr] = kernel;
            }

            var resultArr = ArrayPool<int>.Shared.Rent(df.RowCount);

            kernel(df.RowCount, resultArr, inputArrays);

            return new IntColumn("JIT_Res_Int", resultArr, df.RowCount);
        }

        private static IColumn EvaluateBinaryRecursiveFallback(BinaryExpr b, DataFrame df)
        {
            var left = Evaluate(b.Left, df);
            var right = Evaluate(b.Right, df);

            if (left is IntColumn lInt && right is IntColumn rInt)
            {
                return b.Op switch
                {
                    BinaryOp.Add => lInt + rInt,
                    BinaryOp.Subtract => lInt - rInt,
                    BinaryOp.Multiply => lInt * rInt,
                    BinaryOp.Divide => lInt / rInt,
                    _ => throw new NotSupportedException($"Op {b.Op} not supported for Int")
                };
            }

            if (left is DoubleColumn lDbl && right is DoubleColumn rDbl)
            {
                return b.Op switch
                {
                    BinaryOp.Add => lDbl + rDbl,
                    BinaryOp.Subtract => lDbl - rDbl,
                    BinaryOp.Multiply => lDbl * rDbl,
                    BinaryOp.Divide => lDbl / rDbl,
                    _ => throw new NotSupportedException($"Op {b.Op} not supported for Double")
                };
            }

            throw new NotSupportedException($"Type mismatch: {left.DataType.Name} vs {right.DataType.Name}");
        }

        private static IColumn ExecuteScalar(IColumn col, object? scalarVal, BinaryOp op, bool leftIsScalar)
        {
            if (scalarVal == null) throw new NotSupportedException("Scalar math with null literals not implemented yet.");

            // === DOUBLE ===
            if (col is DoubleColumn dc)
            {
                double val = Convert.ToDouble(scalarVal);
                if (leftIsScalar)
                {
                    return op switch
                    {
                        BinaryOp.Add => dc + val,
                        BinaryOp.Multiply => dc * val,
                        // Trick: 1.0 - col => -1.0 * col + 1.0
                        BinaryOp.Subtract => (dc * -1.0) + val,
                        _ => throw new NotSupportedException($"Scalar Left operation {op} not optimized for Double yet.")
                    };
                }
                else
                {
                    return op switch
                    {
                        BinaryOp.Add => dc + val,
                        BinaryOp.Subtract => dc - val,
                        BinaryOp.Multiply => dc * val,
                        BinaryOp.Divide => dc / val,
                        _ => throw new NotSupportedException($"Op {op} not supported")
                    };
                }
            }

            // === INT ===
            if (col is IntColumn ic)
            {
                int val = Convert.ToInt32(scalarVal);
                if (leftIsScalar)
                {
                    return op switch
                    {
                        BinaryOp.Add => ic + val,
                        BinaryOp.Multiply => ic * val,
                        // Trick: 10 - col => -1 * col + 10
                        BinaryOp.Subtract => (ic * -1) + val,
                        _ => throw new NotSupportedException($"Scalar Left operation {op} not optimized for Int yet.")
                    };
                }
                else
                {
                    return op switch
                    {
                        BinaryOp.Add => ic + val,
                        BinaryOp.Subtract => ic - val,
                        BinaryOp.Multiply => ic * val,
                        BinaryOp.Divide => ic / val,
                        _ => throw new NotSupportedException($"Op {op} not supported")
                    };
                }
            }

            throw new NotSupportedException($"Scalar math not supported for column type {col.DataType.Name}");
        }

        private static IColumn CreateLiteralColumn(LitExpr l, int rowCount)
        {
            if (l.Value == null)
            {
                var col = new IntColumn("Lit", rowCount, true);
                for (int i = 0; i < rowCount; i++) col.SetNull(i);
                return col;
            }

            if (l.Value is int iVal)
            {
                var col = new IntColumn("Lit", rowCount);
                for (int i = 0; i < rowCount; i++) col.Append(iVal);
                return col;
            }
            if (l.Value is double dVal)
            {
                var col = new DoubleColumn("Lit", rowCount);
                for (int i = 0; i < rowCount; i++) col.Append(dVal);
                return col;
            }
            if (l.Value is string sVal)
            {
                var col = new StringColumn("Lit", rowCount);
                for (int i = 0; i < rowCount; i++) col.Append(sVal);
                return col;
            }
            if (l.Value is bool bVal)
            {
                var col = new BoolColumn("Lit", rowCount);
                for (int i = 0; i < rowCount; i++) col.Append(bVal);
                return col;
            }
            if (l.Value is DateTime dtVal)
            {
                var col = new DateTimeColumn("Lit", rowCount);
                for (int i = 0; i < rowCount; i++) col.Append(dtVal);
                return col;
            }

            throw new NotImplementedException($"Literal type {l.Value?.GetType().Name ?? "null"} not supported.");
        }

        private static void CollectColumns(Expr expr, List<string> cols)
        {
            if (expr is ColExpr c && !cols.Contains(c.Name)) cols.Add(c.Name);
            if (expr is BinaryExpr b) { CollectColumns(b.Left, cols); CollectColumns(b.Right, cols); }
            if (expr is AliasExpr a) CollectColumns(a.Child, cols);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Execution/PhysicalPlanner.cs =====
using LeichtFrame.Core.Plans;
using LeichtFrame.Core.Expressions;
using LeichtFrame.Core.Operations.Aggregate;
using LeichtFrame.Core.Operations.Filter;
using LeichtFrame.Core.Operations.Join;
using LeichtFrame.Core.Operations.Sort;
using LeichtFrame.Core.Operations.GroupBy;
using LeichtFrame.Core.Operations.Transform;

namespace LeichtFrame.Core.Execution
{
    /// <summary>
    /// Translates a Logical Plan into physical execution steps and produces a materialized DataFrame.
    /// </summary>
    public class PhysicalPlanner
    {
        /// <summary>
        /// Executes the logical plan.
        /// </summary>
        /// <param name="plan">The logical plan to execute.</param>
        /// <returns>The resulting materialized DataFrame.</returns>
        public DataFrame Execute(LogicalPlan plan)
        {
            return plan switch
            {
                DataFrameScan scan => scan.Source,
                Filter filter => ExecuteFilter(filter),
                Projection proj => ExecuteProjection(proj),
                Aggregate agg => ExecuteAggregate(agg),
                Join join => ExecuteJoin(join),
                Sort sort => ExecuteSort(sort),
                _ => throw new NotImplementedException($"Unknown plan node: {plan.GetType().Name}")
            };
        }

        private DataFrame ExecuteFilter(Filter node)
        {
            var inputDf = Execute(node.Input);

            if (node.Predicate is BinaryExpr bin && bin.Left is ColExpr c && bin.Right is LitExpr l)
            {
                var op = MapOp(bin.Op);

                if (l.Value is int iVal) return inputDf.WhereVec(c.Name, op, iVal);
                if (l.Value is double dVal) return inputDf.WhereVec(c.Name, op, dVal);
                if (l.Value is DateTime dtVal) return inputDf.WhereVec(c.Name, op, dtVal);
            }

            throw new NotImplementedException("Complex filters require expression compilation or simple comparisons.");
        }

        private DataFrame ExecuteProjection(Projection node)
        {
            var inputDf = Execute(node.Input);
            var newColumns = new List<IColumn>();

            foreach (var expr in node.Expressions)
            {
                var resultCol = Evaluator.Evaluate(expr, inputDf);
                string targetName = expr is AliasExpr a ? a.Alias : resultCol.Name;

                if (resultCol.Name != targetName)
                {
                    resultCol = resultCol.Rename(targetName);
                }

                newColumns.Add(resultCol);
            }

            return new DataFrame(newColumns);
        }

        private DataFrame ExecuteAggregate(Aggregate node)
        {
            var inputDf = Execute(node.Input);

            // 1. Extract Group Columns
            var colNames = new List<string>();
            foreach (var expr in node.GroupExprs)
            {
                if (expr is ColExpr c)
                    colNames.Add(c.Name);
                else
                    throw new NotImplementedException("Only column references supported in GroupBy.");
            }

            // 2. Perform Grouping
            using var groupedDf = GroupingOps.GroupBy(inputDf, colNames.ToArray());

            // --- PERFORMANCE FAST PATH ---
            if (node.AggExprs.Count == 1 && colNames.Count == 1)
            {
                var aggExpr = node.AggExprs[0];
                string targetName = aggExpr is AliasExpr a ? a.Alias : "Count";
                Expr core = aggExpr is AliasExpr alias ? alias.Child : aggExpr;

                if (core is AggExpr ae && ae.Op == AggOpType.Count)
                {
                    var result = groupedDf.Count();

                    if (targetName != "Count")
                    {
                        var countCol = result["Count"];

                        var renamedCol = countCol.Rename(targetName);

                        var newCols = new List<IColumn>();
                        foreach (var c in result.Columns)
                        {
                            if (c.Name == "Count") newCols.Add(renamedCol);
                            else newCols.Add(c);
                        }
                        return new DataFrame(newCols);
                    }

                    return result;
                }
            }
            // -----------------------------

            // 3. General Path
            var aggDefs = new List<AggregationDef>();

            foreach (var expr in node.AggExprs)
            {
                string targetName = expr is AliasExpr a ? a.Alias : null!;
                Expr coreExpr = expr is AliasExpr aliasExpr ? aliasExpr.Child : expr;

                if (coreExpr is AggExpr agg)
                {
                    string sourceCol = agg.Child is ColExpr c ? c.Name : "";

                    var def = agg.Op switch
                    {
                        AggOpType.Sum => Agg.Sum(sourceCol, targetName),
                        AggOpType.Min => Agg.Min(sourceCol, targetName),
                        AggOpType.Max => Agg.Max(sourceCol, targetName),
                        AggOpType.Mean => Agg.Mean(sourceCol, targetName),
                        AggOpType.Count => Agg.Count(targetName),
                        _ => throw new NotImplementedException($"Agg op {agg.Op} not supported")
                    };
                    aggDefs.Add(def);
                }
            }

            return groupedDf.Aggregate(aggDefs.ToArray());
        }

        private DataFrame ExecuteJoin(Join node)
        {
            var leftDf = Execute(node.Left);
            var rightDf = Execute(node.Right);

            return DataFrameJoinExtensions.Join(leftDf, rightDf, node.LeftOn, node.JoinType);
        }

        private DataFrame ExecuteSort(Sort node)
        {
            var df = Execute(node.Input);

            string[] names = node.SortColumns.Select(x => x.Name).ToArray();
            bool[] ascending = node.SortColumns.Select(x => x.Ascending).ToArray();

            return OrderOps.OrderBy(df, names, ascending);
        }

        /// <summary>
        /// Helper to map logical expressions to physical aggregation definitions.
        /// Exposed internally for PhysicalStreamer.
        /// </summary>
        internal AggregationDef[] MapAggregations(List<Expr> aggExprs)
        {
            var aggDefs = new List<AggregationDef>();
            foreach (var expr in aggExprs)
            {
                string targetName = expr is AliasExpr a ? a.Alias : "Agg";
                Expr coreExpr = expr is AliasExpr aliasExpr ? aliasExpr.Child : expr;

                if (coreExpr is AggExpr agg)
                {
                    string sourceCol = agg.Child is ColExpr c ? c.Name : "";

                    var def = agg.Op switch
                    {
                        AggOpType.Sum => Agg.Sum(sourceCol, targetName),
                        AggOpType.Min => Agg.Min(sourceCol, targetName),
                        AggOpType.Max => Agg.Max(sourceCol, targetName),
                        AggOpType.Mean => Agg.Mean(sourceCol, targetName),
                        AggOpType.Count => Agg.Count(targetName),
                        _ => throw new NotImplementedException($"Agg op {agg.Op} not supported")
                    };
                    aggDefs.Add(def);
                }
            }
            return aggDefs.ToArray();
        }

        // --- Helpers ---

        private IColumn RenameColumn(IColumn col, string newName)
        {
            var field = typeof(Column).GetField("<Name>k__BackingField",
                System.Reflection.BindingFlags.Instance | System.Reflection.BindingFlags.NonPublic);

            if (field != null)
            {
                field.SetValue(col, newName);
                return col;
            }

            Console.WriteLine($"Warnung: Slow Rename f√ºr {col.Name} -> {newName}");
            var indices = new int[col.Length];
            for (int i = 0; i < indices.Length; i++) indices[i] = i;

            var newCol = col.CloneSubset(indices);

            return newCol;
        }

        private CompareOp MapOp(BinaryOp op)
        {
            return op switch
            {
                BinaryOp.Equal => CompareOp.Equal,
                BinaryOp.NotEqual => CompareOp.NotEqual,
                BinaryOp.GreaterThan => CompareOp.GreaterThan,
                BinaryOp.GreaterThanOrEqual => CompareOp.GreaterThanOrEqual,
                BinaryOp.LessThan => CompareOp.LessThan,
                BinaryOp.LessThanOrEqual => CompareOp.LessThanOrEqual,
                _ => throw new NotSupportedException($"Operator {op} not supported in filter.")
            };
        }
    }
}
===== FILE: src/LeichtFrame.Core/Execution/PhysicalStreamer.cs =====
using System.Collections;
using LeichtFrame.Core.Plans;
using LeichtFrame.Core.Operations.GroupBy;
using LeichtFrame.Core.Operations.Aggregate;
using LeichtFrame.Core.Expressions;
using LeichtFrame.Core.Engine;

namespace LeichtFrame.Core.Execution
{
    /// <summary>
    /// Executes a Logical Plan in a streaming fashion to minimize memory allocation.
    /// </summary>
    public static class PhysicalStreamer
    {
        /// <summary>
        /// Execute Logical Plan
        /// </summary>
        /// <param name="plan"></param>
        /// <returns></returns>
        public static IEnumerable<RowView> Execute(LogicalPlan plan)
        {
            if (plan is Aggregate aggNode)
            {
                return new AggregateStreamEnumerable(aggNode);
            }

            var df = new PhysicalPlanner().Execute(plan);
            return new DataFrameEnumerable(df);
        }

        private class DataFrameEnumerable : IEnumerable<RowView>
        {
            private readonly DataFrame _df;
            public DataFrameEnumerable(DataFrame df) { _df = df; }
            public IEnumerator<RowView> GetEnumerator()
            {
                for (int i = 0; i < _df.RowCount; i++)
                    yield return new RowView(i, _df.Columns, _df.Schema);
            }
            IEnumerator IEnumerable.GetEnumerator() => GetEnumerator();
        }

        private class AggregateStreamEnumerable : IEnumerable<RowView>
        {
            private readonly Aggregate _node;

            public AggregateStreamEnumerable(Aggregate node)
            {
                _node = node;
            }

            public IEnumerator<RowView> GetEnumerator()
            {
                var inputDf = new PhysicalPlanner().Execute(_node.Input);
                var groupCols = _node.GroupExprs.Cast<ColExpr>().Select(c => c.Name).ToArray();

                var gdf = GroupingOps.GroupBy(inputDf, groupCols);

                bool isSimpleCount = _node.AggExprs.Count == 1
                                     && _node.AggExprs[0] is AliasExpr alias
                                     && alias.Child is AggExpr agg
                                     && agg.Op == AggOpType.Count;

                if (isSimpleCount && gdf.NativeData != null)
                {
                    var aliasExpr = (AliasExpr)_node.AggExprs[0];
                    return new FastNativeCountEnumerator(gdf, groupCols[0], aliasExpr.Alias);
                }

                // Fallback / Slow Path
                var planner = new PhysicalPlanner();
                var aggDefs = planner.MapAggregations(_node.AggExprs);
                var materializedResult = gdf.Aggregate(aggDefs);

                gdf.Dispose();

                return new DataFrameEnumerable(materializedResult).GetEnumerator();
            }

            IEnumerator IEnumerable.GetEnumerator() => GetEnumerator();
        }

        // --- UNSAFE ENUMERATOR ---
        private unsafe class FastNativeCountEnumerator : IEnumerator<RowView>
        {
            private readonly GroupedDataFrame _gdf;
            private readonly NativeGroupedData _native;
            private readonly int* _pKeys;
            private readonly int* _pOffsets;
            private readonly int _groupCount;

            private int _currentIndex = -1;
            private bool _inNullGroup = false;

            // Flyweight Components
            private readonly IFlyweightKeyColumn _keyCol;
            private readonly ScalarIntColumn _valCol;
            private readonly RowView _currentView;

            public FastNativeCountEnumerator(GroupedDataFrame gdf, string keyName, string countName)
            {
                _gdf = gdf;
                _native = gdf.NativeData!;

                _pKeys = _native.Keys.Ptr;
                _pOffsets = _native.Offsets.Ptr;
                _groupCount = _native.GroupCount;

                _valCol = new ScalarIntColumn(countName);

                // --- KEY COLUMN FACTORY ---
                var sourceCol = gdf.Source[keyName];

                if (gdf.KeysAreRowIndices)
                {
                    // Indirect Mode: Native Key = RowIndex
                    _keyCol = CreateIndirectColumn(sourceCol, keyName);
                }
                else
                {
                    // Direct Mode: Native Key = Value
                    if (sourceCol.DataType != typeof(int))
                        throw new InvalidOperationException("Direct Native Keys only supported for Int.");

                    _keyCol = new ScalarIntColumn(keyName);
                }

                // Schema bauen
                var schema = new DataFrameSchema(new[] {
                    new ColumnDefinition(keyName, _keyCol.DataType, IsNullable: true),
                    new ColumnDefinition(countName, typeof(int))
                });

                _currentView = new RowView(0, new IColumn[] { (IColumn)_keyCol, _valCol }, schema);
            }

            private IFlyweightKeyColumn CreateIndirectColumn(IColumn source, string name)
            {
                if (source is IntColumn ic) return new IndirectScalarColumn<int>(name, ic);
                if (source is DoubleColumn dc) return new IndirectScalarColumn<double>(name, dc);
                if (source is StringColumn sc) return new IndirectScalarColumn<string?>(name, sc);
                if (source is BoolColumn bc) return new IndirectScalarColumn<bool>(name, bc);
                if (source is DateTimeColumn dtc) return new IndirectScalarColumn<DateTime>(name, dtc);

                throw new NotSupportedException($"Streaming not supported for type {source.DataType.Name}");
            }

            public RowView Current => _currentView;
            object IEnumerator.Current => _currentView;

            public bool MoveNext()
            {
                // A. Native Groups
                if (_currentIndex < _groupCount - 1)
                {
                    _currentIndex++;

                    int keyOrIndex = _pKeys[_currentIndex];
                    int count = _pOffsets[_currentIndex + 1] - _pOffsets[_currentIndex];

                    // Update Flyweights
                    _keyCol.SetData(keyOrIndex, isNull: false);

                    _valCol.Value = count;
                    _valCol.IsNullValue = false;

                    return true;
                }

                // B. Null Group
                if (!_inNullGroup && _gdf.NullGroupIndices != null && _gdf.NullGroupIndices.Length > 0)
                {
                    _inNullGroup = true;

                    _keyCol.SetData(0, isNull: true); // Key is Null

                    _valCol.Value = _gdf.NullGroupIndices.Length;
                    _valCol.IsNullValue = false;
                    return true;
                }

                return false;
            }

            public void Reset() => throw new NotSupportedException();

            public void Dispose()
            {
                _gdf.Dispose();
            }
        }

        // --- HELPER CLASSES ---

        private interface IFlyweightKeyColumn : IColumn
        {
            void SetData(int keyOrIndex, bool isNull);
        }

        // 1. Direct Int Column
        private class ScalarIntColumn : IColumn<int>, IFlyweightKeyColumn
        {
            public int Value;
            public bool IsNullValue;

            public string Name { get; }
            public Type DataType => typeof(int);
            public int Length => 1;
            public bool IsNullable => true;

            public ScalarIntColumn(string name) { Name = name; }

            public void SetData(int key, bool isNull)
            {
                Value = key;
                IsNullValue = isNull;
            }

            public int GetValue(int index) => Value;
            object? IColumn.GetValue(int index) => IsNullValue ? null : Value;

            public void SetValue(int index, int value) => Value = value;
            public bool IsNull(int index) => IsNullValue;

            // Stubs
            public ReadOnlySpan<int> AsSpan() => throw new NotSupportedException();
            public ReadOnlyMemory<int> Slice(int start, int length) => throw new NotSupportedException();
            public void Append(int value) => throw new NotSupportedException();
            public void AppendObject(object? value) => throw new NotSupportedException();
            public void SetNull(int index) => throw new NotSupportedException();
            public void EnsureCapacity(int capacity) { }
            public IColumn CloneSubset(IReadOnlyList<int> indices) => throw new NotSupportedException();
            public object? ComputeSum(int[] indices, int start, int end) => null;
            public object? ComputeMean(int[] indices, int start, int end) => null;
            public object? ComputeMin(int[] indices, int start, int end) => null;
            public object? ComputeMax(int[] indices, int start, int end) => null;
        }

        // 2. Indirect Column (Lookup from Source)
        private class IndirectScalarColumn<T> : IColumn<T>, IFlyweightKeyColumn
        {
            private readonly IColumn<T> _source;
            private int _currentRowIndex;
            private bool _isNull;

            public string Name { get; }
            public Type DataType => typeof(T);
            public int Length => 1;
            public bool IsNullable => true;

            public IndirectScalarColumn(string name, IColumn<T> source)
            {
                Name = name;
                _source = source;
            }

            public void SetData(int rowIndex, bool isNull)
            {
                _currentRowIndex = rowIndex;
                _isNull = isNull;
            }

            public T GetValue(int index) => _source.GetValue(_currentRowIndex);

            object? IColumn.GetValue(int index)
            {
                if (_isNull) return null;
                return _source.GetValue(_currentRowIndex);
            }

            public bool IsNull(int index) => _isNull;

            // Stubs
            public void SetValue(int index, T value) => throw new NotSupportedException();
            public ReadOnlySpan<T> AsSpan() => throw new NotSupportedException();
            public ReadOnlyMemory<T> Slice(int start, int length) => throw new NotSupportedException();
            public void Append(T value) => throw new NotSupportedException();
            public void AppendObject(object? value) => throw new NotSupportedException();
            public void SetNull(int index) => throw new NotSupportedException();
            public void EnsureCapacity(int capacity) { }
            public IColumn CloneSubset(IReadOnlyList<int> indices) => throw new NotSupportedException();
            public object? ComputeSum(int[] indices, int start, int end) => null;
            public object? ComputeMean(int[] indices, int start, int end) => null;
            public object? ComputeMin(int[] indices, int start, int end) => null;
            public object? ComputeMax(int[] indices, int start, int end) => null;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Expressions/Expr.cs =====
namespace LeichtFrame.Core.Expressions
{
    /// <summary>
    /// Represents the abstract base class for all nodes in the expression tree.
    /// Supports operator overloading to enable a fluent syntax.
    /// </summary>
    public abstract class Expr
    {
        // --- Arithmetic Operators ---

        /// <summary>Creates a generic addition expression.</summary>
        public static Expr operator +(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.Add, right);

        /// <summary>Creates a generic subtraction expression.</summary>
        public static Expr operator -(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.Subtract, right);

        /// <summary>Creates a generic multiplication expression.</summary>
        public static Expr operator *(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.Multiply, right);

        /// <summary>Creates a generic division expression.</summary>
        public static Expr operator /(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.Divide, right);

        // --- Comparison Operators ---

        /// <summary>Creates a generic greater-than expression.</summary>
        public static Expr operator >(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.GreaterThan, right);

        /// <summary>Creates a generic less-than expression.</summary>
        public static Expr operator <(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.LessThan, right);

        /// <summary>Creates a generic greater-than-or-equal expression.</summary>
        public static Expr operator >=(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.GreaterThanOrEqual, right);

        /// <summary>Creates a generic less-than-or-equal expression.</summary>
        public static Expr operator <=(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.LessThanOrEqual, right);

        /// <summary>Creates a generic equality expression (AST node).</summary>
        public static Expr operator ==(Expr left, Expr right)
        {
            // Null-Safety for DSL usage
            if (ReferenceEquals(left, null)) return new LitExpr(null);
            if (ReferenceEquals(right, null)) return new LitExpr(null);
            return new BinaryExpr(left, BinaryOp.Equal, right);
        }

        /// <summary>Creates a generic inequality expression (AST node).</summary>
        public static Expr operator !=(Expr left, Expr right)
        {
            if (ReferenceEquals(left, null)) return new LitExpr(null);
            if (ReferenceEquals(right, null)) return new LitExpr(null);
            return new BinaryExpr(left, BinaryOp.NotEqual, right);
        }

        // --- Boolean Logic ---

        /// <summary>Creates a logical AND expression.</summary>
        public static Expr operator &(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.And, right);

        /// <summary>Creates a logical OR expression.</summary>
        public static Expr operator |(Expr left, Expr right) => new BinaryExpr(left, BinaryOp.Or, right);

        // --- Implicit Conversions ---

        /// <summary>Implicitly converts an int to a literal expression.</summary>
        public static implicit operator Expr(int v) => new LitExpr(v);

        /// <summary>Implicitly converts a double to a literal expression.</summary>
        public static implicit operator Expr(double v) => new LitExpr(v);

        /// <summary>Implicitly converts a string to a literal expression.</summary>
        public static implicit operator Expr(string v) => new LitExpr(v);

        /// <summary>Implicitly converts a bool to a literal expression.</summary>
        public static implicit operator Expr(bool v) => new LitExpr(v);

        /// <summary>Implicitly converts a DateTime to a literal expression.</summary>
        public static implicit operator Expr(DateTime v) => new LitExpr(v);

        // --- Fluent Methods ---

        /// <summary>
        /// Assigns an alias (new name) to the current expression.
        /// </summary>
        /// <param name="alias">The name to assign.</param>
        /// <returns>An <see cref="AliasExpr"/> wrapping this expression.</returns>
        public Expr As(string alias) => new AliasExpr(this, alias);

        // Required overrides to suppress compiler warnings when overloading ==/!=
        /// <inheritdoc />
        public override bool Equals(object? obj) => ReferenceEquals(this, obj);
        /// <inheritdoc />
        public override int GetHashCode() => base.GetHashCode();
    }
}
===== FILE: src/LeichtFrame.Core/Expressions/ExprExtensions.cs =====
using LeichtFrame.Core.Expressions;

namespace LeichtFrame.Core
{
    /// <summary>
    /// Provides fluent extension methods for building expressions, aggregations, and aliases.
    /// Enables syntax like: "ColumnName".Sum().As("Total")
    /// </summary>
    public static class ExprExtensions
    {
        // =========================================================
        // Extensions for existing Expr objects
        // =========================================================

        /// <summary>
        /// Creates a summation aggregation expression.
        /// </summary>
        /// <param name="expr">The expression to aggregate.</param>
        public static Expr Sum(this Expr expr) => new AggExpr(AggOpType.Sum, expr);

        /// <summary>
        /// Creates a minimum value aggregation expression.
        /// </summary>
        /// <param name="expr">The expression to aggregate.</param>
        public static Expr Min(this Expr expr) => new AggExpr(AggOpType.Min, expr);

        /// <summary>
        /// Creates a maximum value aggregation expression.
        /// </summary>
        /// <param name="expr">The expression to aggregate.</param>
        public static Expr Max(this Expr expr) => new AggExpr(AggOpType.Max, expr);

        /// <summary>
        /// Creates an arithmetic mean (average) aggregation expression.
        /// </summary>
        /// <param name="expr">The expression to aggregate.</param>
        public static Expr Mean(this Expr expr) => new AggExpr(AggOpType.Mean, expr);

        /// <summary>
        /// Creates a count aggregation expression.
        /// </summary>
        /// <param name="expr">The expression to count.</param>
        public static Expr Count(this Expr expr) => new AggExpr(AggOpType.Count, expr);

        /// <summary>
        /// Aliases the expression with a new name.
        /// </summary>
        /// <param name="expr">The expression to rename.</param>
        /// <param name="alias">The new name for the column.</param>
        public static Expr As(this Expr expr, string alias) => new AliasExpr(expr, alias);

        // =========================================================
        // Extensions for Strings (Implicit Column Reference)
        // =========================================================

        /// <summary>Creates a Sum aggregation on the column specified by name.</summary>
        public static Expr Sum(this string colName) => new ColExpr(colName).Sum();

        /// <summary>Creates a Min aggregation on the column specified by name.</summary>
        public static Expr Min(this string colName) => new ColExpr(colName).Min();

        /// <summary>Creates a Max aggregation on the column specified by name.</summary>
        public static Expr Max(this string colName) => new ColExpr(colName).Max();

        /// <summary>Creates a Mean aggregation on the column specified by name.</summary>
        public static Expr Mean(this string colName) => new ColExpr(colName).Mean();

        /// <summary>
        /// Creates a Count aggregation.
        /// Use "*" to count all rows (equivalent to SQL COUNT(*)).
        /// Use a column name to count non-null values in that column.
        /// </summary>
        public static Expr Count(this string colName)
        {
            if (colName == "*")
            {
                return new AggExpr(AggOpType.Count, new LitExpr(1));
            }
            return new ColExpr(colName).Count();
        }

        /// <summary>
        /// Creates a column reference with an alias (e.g. for Select projections).
        /// Usage: select("OldName".As("NewName"))
        /// </summary>
        public static Expr As(this string colName, string alias) => new ColExpr(colName).As(alias);
    }

    /// <summary>
    /// Static Entry Point for expressions, similar to 'pl' in Polars.
    /// </summary>
    public static class Lf
    {
        /// <summary>
        /// Creates a column reference expression.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        public static Expr Col(string name) => new ColExpr(name);

        /// <summary>
        /// Creates a literal value expression.
        /// </summary>
        /// <param name="val">The value to wrap.</param>
        public static Expr Lit(object val) => new LitExpr(val);
    }
}
===== FILE: src/LeichtFrame.Core/Expressions/F.cs =====
namespace LeichtFrame.Core.Expressions
{
    /// <summary>
    /// Static entry point for the Fluent API. Provides factory methods for creating expressions.
    /// </summary>
    public static class F
    {
        /// <summary>
        /// Creates a column reference expression.
        /// </summary>
        /// <param name="name">The name of the column.</param>
        /// <returns>A <see cref="ColExpr"/> representing the column.</returns>
        public static Expr Col(string name) => new ColExpr(name);

        /// <summary>
        /// Creates a literal value expression.
        /// </summary>
        /// <param name="val">The value to wrap.</param>
        /// <returns>A <see cref="LitExpr"/> representing the constant.</returns>
        public static Expr Lit(object? val) => new LitExpr(val);

        /// <summary>Creates a Sum aggregation.</summary>
        public static Expr Sum(Expr expr) => new AggExpr(AggOpType.Sum, expr);

        /// <summary>Creates a Min aggregation.</summary>
        public static Expr Min(Expr expr) => new AggExpr(AggOpType.Min, expr);

        /// <summary>Creates a Max aggregation.</summary>
        public static Expr Max(Expr expr) => new AggExpr(AggOpType.Max, expr);

        /// <summary>Creates a Mean aggregation.</summary>
        public static Expr Mean(Expr expr) => new AggExpr(AggOpType.Mean, expr);

        /// <summary>Creates a Count aggregation.</summary>
        public static Expr Count() => new AggExpr(AggOpType.Count, new LitExpr(1));
    }
}
===== FILE: src/LeichtFrame.Core/Expressions/Nodes.cs =====
namespace LeichtFrame.Core.Expressions
{
    /// <summary>
    /// Defines the supported binary operations in the expression tree.
    /// </summary>
    public enum BinaryOp
    {
        /// <summary>Addition operation.</summary>
        Add,
        /// <summary>Subtraction operation.</summary>
        Subtract,
        /// <summary>Multiplication operation.</summary>
        Multiply,
        /// <summary>Division operation.</summary>
        Divide,
        /// <summary>Greater than comparison.</summary>
        GreaterThan,
        /// <summary>Less than comparison.</summary>
        LessThan,
        /// <summary>Greater than or equal comparison.</summary>
        GreaterThanOrEqual,
        /// <summary>Less than or equal comparison.</summary>
        LessThanOrEqual,
        /// <summary>Equality comparison.</summary>
        Equal,
        /// <summary>Inequality comparison.</summary>
        NotEqual,
        /// <summary>Logical AND operation.</summary>
        And,
        /// <summary>Logical OR operation.</summary>
        Or
    }

    /// <summary>
    /// Represents a constant value in an expression.
    /// </summary>
    public class LitExpr : Expr
    {
        /// <summary>The constant value (e.g., int, string, double).</summary>
        public object? Value { get; }

        /// <summary>
        /// Initializes a new instance of the <see cref="LitExpr"/> class.
        /// </summary>
        /// <param name="value">The literal value.</param>
        public LitExpr(object? value)
        {
            Value = value;
        }
    }

    /// <summary>
    /// Represents a reference to a column by name.
    /// </summary>
    public class ColExpr : Expr
    {
        /// <summary>The name of the column.</summary>
        public string Name { get; }

        /// <summary>
        /// Initializes a new instance of the <see cref="ColExpr"/> class.
        /// </summary>
        /// <param name="name">The column name.</param>
        public ColExpr(string name)
        {
            Name = name;
        }
    }

    /// <summary>
    /// Represents a binary operation between two expressions.
    /// </summary>
    public class BinaryExpr : Expr
    {
        /// <summary>The left operand.</summary>
        public Expr Left { get; }

        /// <summary>The operation type.</summary>
        public BinaryOp Op { get; }

        /// <summary>The right operand.</summary>
        public Expr Right { get; }

        /// <summary>
        /// Initializes a new instance of the <see cref="BinaryExpr"/> class.
        /// </summary>
        /// <param name="left">The left operand.</param>
        /// <param name="op">The operation type.</param>
        /// <param name="right">The right operand.</param>
        public BinaryExpr(Expr left, BinaryOp op, Expr right)
        {
            Left = left;
            Op = op;
            Right = right;
        }
    }

    /// <summary>
    /// Represents an alias operation to rename a column or expression result.
    /// </summary>
    public class AliasExpr : Expr
    {
        /// <summary>The expression to alias.</summary>
        public Expr Child { get; }

        /// <summary>The new name.</summary>
        public string Alias { get; }

        /// <summary>
        /// Initializes a new instance of the <see cref="AliasExpr"/> class.
        /// </summary>
        /// <param name="child">The child expression.</param>
        /// <param name="alias">The alias name.</param>
        public AliasExpr(Expr child, string alias)
        {
            Child = child;
            Alias = alias;
        }
    }

    /// <summary>
    /// Defines the supported aggregation operation types.
    /// </summary>
    public enum AggOpType
    {
        /// <summary>Summation of values.</summary>
        Sum,
        /// <summary>Find the minimum value.</summary>
        Min,
        /// <summary>Find the maximum value.</summary>
        Max,
        /// <summary>Calculate the arithmetic mean.</summary>
        Mean,
        /// <summary>Count the number of rows.</summary>
        Count
    }

    /// <summary>
    /// Represents an aggregation expression (e.g., Sum(Amount)).
    /// </summary>
    public class AggExpr : Expr
    {
        /// <summary>The type of aggregation.</summary>
        public AggOpType Op { get; }

        /// <summary>The expression to aggregate (usually a column).</summary>
        public Expr Child { get; }

        /// <summary>
        /// Initializes a new aggregation expression.
        /// </summary>
        public AggExpr(AggOpType op, Expr child)
        {
            Op = op;
            Child = child;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Extensions/EnumerableDataFrameExtensions.cs =====
using System.Reflection;

namespace LeichtFrame.Core
{
    /// <summary>
    /// Provides extension methods to convert standard IEnumerable collections into LeichtFrame DataFrames.
    /// Supports batched processing to handle large datasets efficiently.
    /// </summary>
    public static class EnumerableDataFrameExtensions
    {
        /// <summary>
        /// Streams an IEnumerable of objects into multiple DataFrames (batches).
        /// This allows processing large collections without holding all objects in memory at once.
        /// </summary>
        /// <typeparam name="T">The type of the objects (POCO).</typeparam>
        /// <param name="source">The source collection.</param>
        /// <param name="batchSize">The number of rows per batch.</param>
        /// <returns>An enumerable of DataFrames.</returns>
        public static IEnumerable<DataFrame> ToDataFrameBatches<T>(this IEnumerable<T> source, int batchSize)
        {
            if (source == null) throw new ArgumentNullException(nameof(source));
            if (batchSize <= 0) throw new ArgumentOutOfRangeException(nameof(batchSize));

            // 1. One-time Setup: Infer Schema and Cache Reflection Data
            // We do this ONCE per stream, not per batch.
            var schema = DataFrameSchema.FromType<T>();
            var type = typeof(T);

            // Align PropertyInfos with Column Indices for fast access
            int colCount = schema.Columns.Count;
            var propertyCache = new PropertyInfo[colCount];

            for (int i = 0; i < colCount; i++)
            {
                string name = schema.Columns[i].Name;
                var prop = type.GetProperty(name);
                if (prop == null) throw new InvalidOperationException($"Property '{name}' not found on type '{type.Name}' during mapping.");
                propertyCache[i] = prop;
            }

            // 2. Iteration & Buffering
            var buffer = new List<T>(batchSize);

            foreach (var item in source)
            {
                buffer.Add(item);

                if (buffer.Count >= batchSize)
                {
                    yield return FlushBatch(buffer, schema, propertyCache);
                    buffer.Clear();
                }
            }

            // 3. Flush Remainder
            if (buffer.Count > 0)
            {
                yield return FlushBatch(buffer, schema, propertyCache);
            }
        }

        private static DataFrame FlushBatch<T>(List<T> buffer, DataFrameSchema schema, PropertyInfo[] propertyCache)
        {
            // Create DataFrame with exact capacity
            var df = DataFrame.Create(schema, buffer.Count);
            int colCount = df.ColumnCount;

            // Cache IColumn references to avoid indexer lookups in the loop
            var columns = new IColumn[colCount];
            for (int i = 0; i < colCount; i++)
            {
                columns[i] = df.Columns[i];
            }

            // Fill Data
            foreach (var item in buffer)
            {
                for (int c = 0; c < colCount; c++)
                {
                    object? val = propertyCache[c].GetValue(item);
                    columns[c].AppendObject(val);
                }
            }

            return df;
        }
    }
}
===== FILE: src/LeichtFrame.Core/LeichtFrame.Core.csproj =====
Ôªø<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <TargetFramework>net8.0</TargetFramework>
    <ImplicitUsings>enable</ImplicitUsings>
    <Nullable>enable</Nullable>
    <AllowUnsafeBlocks>true</AllowUnsafeBlocks>
    <Optimize>true</Optimize> 
  </PropertyGroup>

  <ItemGroup>
    <AssemblyAttribute Include="System.Runtime.CompilerServices.InternalsVisibleTo">
      <_Parameter1>LeichtFrame.Core.Tests</_Parameter1>
    </AssemblyAttribute>

    <AssemblyAttribute Include="System.Runtime.CompilerServices.InternalsVisibleTo">
    <_Parameter1>LeichtFrame.Benchmarks</_Parameter1>
    </AssemblyAttribute>
  </ItemGroup>

  

</Project>

===== FILE: src/LeichtFrame.Core/Operations/Aggregate/AggregationDef.cs =====
namespace LeichtFrame.Core.Operations.Aggregate
{
    /// <summary>
    /// Defines the supported aggregation operations.
    /// </summary>
    public enum AggOp
    {
        /// <summary>Summation of values.</summary>
        Sum,
        /// <summary>Arithmetic mean.</summary>
        Mean,
        /// <summary>Row count.</summary>
        Count,
        /// <summary>Minimum value.</summary>
        Min,
        /// <summary>Maximum value.</summary>
        Max
    }

    /// <summary>
    /// Represents a single aggregation instruction.
    /// </summary>
    /// <param name="SourceColumn">The name of the column to aggregate.</param>
    /// <param name="Operation">The operation to perform.</param>
    /// <param name="TargetName">The name of the resulting column.</param>
    public record AggregationDef(string SourceColumn, AggOp Operation, string TargetName);

    /// <summary>
    /// Static helper for building aggregation definitions (Fluent API).
    /// </summary>
    public static class Agg
    {
        /// <summary>Creates a Sum aggregation.</summary>
        public static AggregationDef Sum(string column, string? alias = null)
            => new(column, AggOp.Sum, alias ?? $"sum_{column}");

        /// <summary>Creates a Mean aggregation.</summary>
        public static AggregationDef Mean(string column, string? alias = null)
            => new(column, AggOp.Mean, alias ?? $"avg_{column}");

        /// <summary>Creates a Min aggregation.</summary>
        public static AggregationDef Min(string column, string? alias = null)
            => new(column, AggOp.Min, alias ?? $"min_{column}");

        /// <summary>Creates a Max aggregation.</summary>
        public static AggregationDef Max(string column, string? alias = null)
            => new(column, AggOp.Max, alias ?? $"max_{column}");

        /// <summary>Creates a Count aggregation.</summary>
        public static AggregationDef Count(string? alias = "count")
            => new(string.Empty, AggOp.Count, alias ?? "count");
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Aggregate/AggregationOps.cs =====
namespace LeichtFrame.Core.Operations.Aggregate

{
    /// <summary>
    /// Provides extension methods for calculating aggregations (Sum, Min, Max, Mean) on DataFrames.
    /// </summary>
    public static class DataFrameAggregationExtensions
    {
        /// <summary>
        /// Calculates the Sum of a numeric column.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="columnName">The name of the column.</param>
        /// <returns>The sum of all values.</returns>
        /// <exception cref="NotSupportedException">Thrown if the column type is not numeric.</exception>
        public static double Sum(this DataFrame df, string columnName)
        {
            var col = df[columnName];
            if (col is DoubleColumn doubleCol) return doubleCol.Sum();
            if (col is IntColumn intCol) return (double)intCol.Sum();
            throw new NotSupportedException($"Sum operation is not supported for column type '{col.DataType.Name}'.");
        }

        /// <summary>
        /// Calculates the Minimum value of a numeric column.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="columnName">The name of the column.</param>
        /// <returns>The minimum value.</returns>
        /// <exception cref="NotSupportedException">Thrown if the column type is not numeric.</exception>
        public static double Min(this DataFrame df, string columnName)
        {
            var col = df[columnName];
            if (col is DoubleColumn doubleCol) return doubleCol.Min();
            if (col is IntColumn intCol) return (double)intCol.Min();
            throw new NotSupportedException($"Min operation is not supported for column type '{col.DataType.Name}'.");
        }

        /// <summary>
        /// Calculates the Maximum value of a numeric column.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="columnName">The name of the column.</param>
        /// <returns>The maximum value.</returns>
        /// <exception cref="NotSupportedException">Thrown if the column type is not numeric.</exception>
        public static double Max(this DataFrame df, string columnName)
        {
            var col = df[columnName];
            if (col is DoubleColumn doubleCol) return doubleCol.Max();
            if (col is IntColumn intCol) return (double)intCol.Max();
            throw new NotSupportedException($"Max operation is not supported for column type '{col.DataType.Name}'.");
        }

        /// <summary>
        /// Calculates the arithmetic Mean (Average) of a numeric column.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="columnName">The name of the column.</param>
        /// <returns>The mean value.</returns>
        public static double Mean(this DataFrame df, string columnName)
        {
            var col = df[columnName];
            double sum = df.Sum(columnName);
            int count = 0;

            if (col.IsNullable)
            {
                for (int i = 0; i < col.Length; i++)
                    if (!col.IsNull(i)) count++;
            }
            else
            {
                count = col.Length;
            }

            if (count == 0) return 0;
            return sum / count;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Aggregate/GroupAggregationExtensions.cs =====
namespace LeichtFrame.Core.Operations.Aggregate
{
    /// <summary>
    /// Provides extension methods for performing aggregations on grouped dataframes.
    /// Optimized for high performance using CSR (Compressed Sparse Row) iteration via pointers (Native Memory).
    /// </summary>
    public static class GroupAggregationExtensions
    {
        // --- Custom Delegates to support ReadOnlySpan (Managed Fallback) ---
        private delegate long IntSpanNullOp(ReadOnlySpan<int> data, int[] indices);
        private delegate long IntSpanCsrOp(ReadOnlySpan<int> data, int start, int end, int[] indices);
        private delegate double DoubleSpanCsrOp(ReadOnlySpan<double> data, int start, int end, int[] indices);

        /// <summary>
        /// Aggregates the grouped data by counting rows in each group.
        /// Returns a new DataFrame with columns: [GroupColumn, "Count"].
        /// </summary>
        public static DataFrame Count(this GroupedDataFrame gdf)
        {
            // --- FAST PATH: Native Memory (Zero-Alloc) ---
            if (gdf.NativeData != null && !gdf.KeysAreRowIndices)
            {
                var native = gdf.NativeData;
                bool hasNulls = gdf.NullGroupIndices != null && gdf.NullGroupIndices.Length > 0;
                int totalRows = native.GroupCount + (hasNulls ? 1 : 0);

                var nativeCounts = new IntColumn("Count", totalRows);
                var keyCol = new IntColumn(gdf.GroupColumnNames[0], totalRows, isNullable: hasNulls);

                unsafe
                {
                    int* offsets = native.Offsets.Ptr;
                    int* keys = native.Keys.Ptr;

                    for (int i = 0; i < native.GroupCount; i++)
                    {
                        nativeCounts.Append(offsets[i + 1] - offsets[i]);
                        keyCol.Append(keys[i]);
                    }
                }

                if (hasNulls)
                {
                    nativeCounts.Append(gdf.NullGroupIndices!.Length);
                    keyCol.Append(null);
                }

                return new DataFrame(new IColumn[] { keyCol, nativeCounts });
            }

            // --- SLOW PATH: Managed Arrays (Fallback) ---
            var offsetsManaged = gdf.GroupOffsets;
            var count = gdf.GroupCount;
            bool hasNullsSlow = gdf.NullGroupIndices != null && gdf.NullGroupIndices.Length > 0;

            var countsCol = new IntColumn("Count", count + (hasNullsSlow ? 1 : 0));

            for (int i = 0; i < count; i++)
            {
                countsCol.Append(offsetsManaged[i + 1] - offsetsManaged[i]);
            }

            if (hasNullsSlow)
            {
                countsCol.Append(gdf.NullGroupIndices!.Length);
            }

            return CreateResultDataFrame(gdf, "Count", countsCol);
        }

        /// <summary>
        /// Aggregates the grouped data by summing values in the specified column.
        /// </summary>
        public static DataFrame Sum(this GroupedDataFrame gdf, string aggregateColumnName)
        {
            // --- FAST PATH: Native Memory ---
            if (gdf.NativeData != null && !gdf.KeysAreRowIndices)
            {
                var col = gdf.Source[aggregateColumnName];

                if (col is IntColumn ic)
                {
                    var native = gdf.NativeData;
                    bool hasNulls = gdf.NullGroupIndices != null && gdf.NullGroupIndices.Length > 0;
                    int totalRows = native.GroupCount + (hasNulls ? 1 : 0);

                    var sumCol = new DoubleColumn($"Sum_{aggregateColumnName}", totalRows);
                    var keyCol = new IntColumn(gdf.GroupColumnNames[0], totalRows, isNullable: hasNulls);

                    unsafe
                    {
                        int* pOffsets = native.Offsets.Ptr;
                        int* pIndices = native.Indices.Ptr;
                        int* pKeys = native.Keys.Ptr;

                        fixed (int* pSource = ic.Values.Span)
                        {
                            for (int i = 0; i < native.GroupCount; i++)
                            {
                                int start = pOffsets[i];
                                int end = pOffsets[i + 1];

                                long sum = 0;
                                for (int k = start; k < end; k++)
                                {
                                    sum += pSource[pIndices[k]];
                                }
                                sumCol.Append(sum);
                                keyCol.Append(pKeys[i]);
                            }
                        }
                    }

                    if (hasNulls)
                    {
                        long sumNull = 0;
                        var span = ic.Values.Span;
                        foreach (var idx in gdf.NullGroupIndices!) sumNull += span[idx];
                        sumCol.Append(sumNull);
                        keyCol.Append(null);
                    }

                    return new DataFrame(new IColumn[] { keyCol, sumCol });
                }
            }

            // --- SLOW PATH ---
            return ExecuteNumericAgg(gdf, aggregateColumnName, "Sum",
                (span, indices) =>
                {
                    long sum = 0;
                    foreach (var idx in indices) sum += span[idx];
                    return sum;
                },
                (span, start, end, indices) =>
                {
                    long sum = 0;
                    for (int k = start; k < end; k++) sum += span[indices[k]];
                    return sum;
                },
                (span, start, end, indices) =>
                {
                    double sum = 0;
                    for (int k = start; k < end; k++) sum += span[indices[k]];
                    return sum;
                }
            );
        }

        /// <summary>
        /// Aggregates the grouped data by finding the minimum value.
        /// </summary>
        public static DataFrame Min(this GroupedDataFrame gdf, string aggregateColumnName)
        {
            if (gdf.NativeData != null && !gdf.KeysAreRowIndices)
            {
                var col = gdf.Source[aggregateColumnName];
                if (col is IntColumn ic)
                {
                    var native = gdf.NativeData;
                    bool hasNulls = gdf.NullGroupIndices != null && gdf.NullGroupIndices.Length > 0;
                    int totalRows = native.GroupCount + (hasNulls ? 1 : 0);

                    var minCol = new IntColumn($"Min_{aggregateColumnName}", totalRows);
                    var keyCol = new IntColumn(gdf.GroupColumnNames[0], totalRows, isNullable: hasNulls);

                    unsafe
                    {
                        int* pOffsets = native.Offsets.Ptr;
                        int* pIndices = native.Indices.Ptr;
                        int* pKeys = native.Keys.Ptr;

                        fixed (int* pSource = ic.Values.Span)
                        {
                            for (int i = 0; i < native.GroupCount; i++)
                            {
                                int start = pOffsets[i];
                                int end = pOffsets[i + 1];
                                if (start == end) { minCol.Append(0); keyCol.Append(pKeys[i]); continue; }

                                int min = int.MaxValue;
                                for (int k = start; k < end; k++)
                                {
                                    int val = pSource[pIndices[k]];
                                    if (val < min) min = val;
                                }
                                minCol.Append(min);
                                keyCol.Append(pKeys[i]);
                            }
                        }
                    }

                    if (hasNulls)
                    {
                        int min = int.MaxValue;
                        var span = ic.Values.Span;
                        foreach (var idx in gdf.NullGroupIndices!) if (span[idx] < min) min = span[idx];
                        minCol.Append(min);
                        keyCol.Append(null);
                    }

                    return new DataFrame(new IColumn[] { keyCol, minCol });
                }
            }

            // --- SLOW PATH ---
            return ExecuteNumericAgg(gdf, aggregateColumnName, "Min",
                (span, indices) =>
                {
                    if (indices.Length == 0) return 0;
                    int min = int.MaxValue;
                    foreach (var idx in indices) if (span[idx] < min) min = span[idx];
                    return min;
                },
                (span, start, end, indices) =>
                {
                    if (start == end) return 0;
                    int min = int.MaxValue;
                    for (int k = start; k < end; k++)
                    {
                        int val = span[indices[k]];
                        if (val < min) min = val;
                    }
                    return min;
                },
                (span, start, end, indices) =>
                {
                    if (start == end) return 0;
                    double min = double.MaxValue;
                    for (int k = start; k < end; k++)
                    {
                        double val = span[indices[k]];
                        if (val < min) min = val;
                    }
                    return min;
                }
            );
        }

        /// <summary>
        /// Aggregates the grouped data by finding the maximum value.
        /// </summary>
        public static DataFrame Max(this GroupedDataFrame gdf, string aggregateColumnName)
        {
            if (gdf.NativeData != null && !gdf.KeysAreRowIndices)
            {
                var col = gdf.Source[aggregateColumnName];
                if (col is IntColumn ic)
                {
                    var native = gdf.NativeData;
                    bool hasNulls = gdf.NullGroupIndices != null && gdf.NullGroupIndices.Length > 0;
                    int totalRows = native.GroupCount + (hasNulls ? 1 : 0);

                    var maxCol = new IntColumn($"Max_{aggregateColumnName}", totalRows);
                    var keyCol = new IntColumn(gdf.GroupColumnNames[0], totalRows, isNullable: hasNulls);

                    unsafe
                    {
                        int* pOffsets = native.Offsets.Ptr;
                        int* pIndices = native.Indices.Ptr;
                        int* pKeys = native.Keys.Ptr;

                        fixed (int* pSource = ic.Values.Span)
                        {
                            for (int i = 0; i < native.GroupCount; i++)
                            {
                                int start = pOffsets[i];
                                int end = pOffsets[i + 1];
                                if (start == end) { maxCol.Append(0); keyCol.Append(pKeys[i]); continue; }

                                int max = int.MinValue;
                                for (int k = start; k < end; k++)
                                {
                                    int val = pSource[pIndices[k]];
                                    if (val > max) max = val;
                                }
                                maxCol.Append(max);
                                keyCol.Append(pKeys[i]);
                            }
                        }
                    }

                    if (hasNulls)
                    {
                        int max = int.MinValue;
                        var span = ic.Values.Span;
                        foreach (var idx in gdf.NullGroupIndices!) if (span[idx] > max) max = span[idx];
                        maxCol.Append(max);
                        keyCol.Append(null);
                    }

                    return new DataFrame(new IColumn[] { keyCol, maxCol });
                }
            }

            // --- SLOW PATH ---
            return ExecuteNumericAgg(gdf, aggregateColumnName, "Max",
                (span, indices) =>
                {
                    if (indices.Length == 0) return 0;
                    int max = int.MinValue;
                    foreach (var idx in indices) if (span[idx] > max) max = span[idx];
                    return max;
                },
                (span, start, end, indices) =>
                {
                    if (start == end) return 0;
                    int max = int.MinValue;
                    for (int k = start; k < end; k++)
                    {
                        int val = span[indices[k]];
                        if (val > max) max = val;
                    }
                    return max;
                },
                (span, start, end, indices) =>
                {
                    if (start == end) return 0;
                    double max = double.MinValue;
                    for (int k = start; k < end; k++)
                    {
                        double val = span[indices[k]];
                        if (val > max) max = val;
                    }
                    return max;
                }
            );
        }

        /// <summary>
        /// Aggregates the grouped data by calculating the mean value.
        /// </summary>
        public static DataFrame Mean(this GroupedDataFrame gdf, string aggregateColumnName)
        {
            if (gdf.NativeData != null && !gdf.KeysAreRowIndices)
            {
                var col = gdf.Source[aggregateColumnName];
                if (col is IntColumn ic)
                {
                    var native = gdf.NativeData;
                    bool hasNulls = gdf.NullGroupIndices != null && gdf.NullGroupIndices.Length > 0;
                    int totalRows = native.GroupCount + (hasNulls ? 1 : 0);

                    var meanCol = new DoubleColumn($"Mean_{aggregateColumnName}", totalRows);
                    var keyCol = new IntColumn(gdf.GroupColumnNames[0], totalRows, isNullable: hasNulls);

                    unsafe
                    {
                        int* pOffsets = native.Offsets.Ptr;
                        int* pIndices = native.Indices.Ptr;
                        int* pKeys = native.Keys.Ptr;

                        fixed (int* pSource = ic.Values.Span)
                        {
                            for (int i = 0; i < native.GroupCount; i++)
                            {
                                int start = pOffsets[i];
                                int end = pOffsets[i + 1];
                                int count = end - start;

                                if (count == 0) { meanCol.Append(0); keyCol.Append(pKeys[i]); continue; }

                                double sum = 0;
                                for (int k = start; k < end; k++)
                                {
                                    sum += pSource[pIndices[k]];
                                }
                                meanCol.Append(sum / count);
                                keyCol.Append(pKeys[i]);
                            }
                        }
                    }

                    if (hasNulls)
                    {
                        double sum = 0;
                        int count = gdf.NullGroupIndices!.Length;
                        var span = ic.Values.Span;
                        foreach (var idx in gdf.NullGroupIndices!) sum += span[idx];
                        meanCol.Append(count == 0 ? 0 : sum / count);
                        keyCol.Append(null);
                    }

                    return new DataFrame(new IColumn[] { keyCol, meanCol });
                }
            }

            // --- SLOW PATH ---
            var colManaged = gdf.Source[aggregateColumnName];
            int groupCount = gdf.GroupCount;
            var offsets = gdf.GroupOffsets;
            var indices = gdf.RowIndices;
            bool hasNullsSlow = gdf.NullGroupIndices != null;

            var res = new DoubleColumn($"Mean_{aggregateColumnName}", groupCount + (hasNullsSlow ? 1 : 0));

            if (colManaged is IntColumn icManaged)
            {
                ReadOnlySpan<int> data = icManaged.Values.Span;
                for (int i = 0; i < groupCount; i++)
                {
                    int start = offsets[i];
                    int end = offsets[i + 1];
                    if (start == end) { res.Append(0); continue; }

                    double sum = 0;
                    int count = 0;
                    for (int k = start; k < end; k++)
                    {
                        sum += data[indices[k]];
                        count++;
                    }
                    res.Append(count == 0 ? 0 : sum / count);
                }
                if (hasNullsSlow)
                {
                    double sum = 0; int cnt = 0;
                    foreach (var idx in gdf.NullGroupIndices!) { sum += data[idx]; cnt++; }
                    res.Append(cnt == 0 ? 0 : sum / cnt);
                }
            }
            else if (colManaged is DoubleColumn dcManaged)
            {
                ReadOnlySpan<double> data = dcManaged.Values.Span;
                for (int i = 0; i < groupCount; i++)
                {
                    int start = offsets[i];
                    int end = offsets[i + 1];
                    if (start == end) { res.Append(0); continue; }

                    double sum = 0;
                    int count = 0;
                    for (int k = start; k < end; k++)
                    {
                        sum += data[indices[k]];
                        count++;
                    }
                    res.Append(count == 0 ? 0 : sum / count);
                }
                if (hasNullsSlow)
                {
                    double sum = 0; int cnt = 0;
                    foreach (var idx in gdf.NullGroupIndices!) { sum += data[idx]; cnt++; }
                    res.Append(cnt == 0 ? 0 : sum / cnt);
                }
            }
            else
            {
                throw new NotSupportedException($"Mean not supported for {colManaged.DataType.Name}");
            }

            return CreateResultDataFrame(gdf, $"Mean_{aggregateColumnName}", res);
        }

        // =======================================================================
        // AGGREGATE (GENERAL)
        // =======================================================================

        /// <summary>
        /// Aggregation DataFrame
        /// </summary>
        /// <param name="gdf"></param>
        /// <param name="aggregations"></param>
        /// <returns></returns>
        public static DataFrame Aggregate(this GroupedDataFrame gdf, params AggregationDef[] aggregations)
        {
            int groupCount = gdf.GroupCount;
            int totalRows = groupCount + (gdf.NullGroupIndices != null ? 1 : 0);

            var resultCols = new List<IColumn>();

            // =========================================================
            // PHASE 1: RECONSTRUCT KEYS
            // =========================================================
            var keysArray = gdf.GetKeys();

            bool isRowIndexKeyMultiCol = (gdf.KeysAreRowIndices && gdf.GroupColumnNames.Length > 1)
                          || (gdf is GroupedDataFrame<int> && gdf.GroupColumnNames.Length > 1 && !gdf.KeysAreRowIndices);

            if (isRowIndexKeyMultiCol)
            {
                int[] rowIndices = (int[])keysArray;

                foreach (var colName in gdf.GroupColumnNames)
                {
                    var sourceCol = gdf.Source[colName];
                    bool isNullable = sourceCol.IsNullable || gdf.NullGroupIndices != null;
                    var keyCol = ColumnFactory.Create(colName, sourceCol.DataType, totalRows, isNullable: isNullable);

                    for (int i = 0; i < rowIndices.Length; i++)
                    {
                        keyCol.AppendObject(sourceCol.GetValue(rowIndices[i]));
                    }
                    resultCols.Add(keyCol);
                }
            }
            else
            {
                string colName = gdf.GroupColumnNames[0];
                Type keyType = keysArray.GetType().GetElementType()!;
                bool isNullable = gdf.NullGroupIndices != null;

                var keyCol = ColumnFactory.Create(colName, keyType, totalRows, isNullable: isNullable);

                for (int i = 0; i < keysArray.Length; i++)
                {
                    keyCol.AppendObject(keysArray.GetValue(i));
                }
                resultCols.Add(keyCol);
            }

            // =========================================================
            // PHASE 2: PREPARE AGGREGATION COLUMNS
            // =========================================================
            var aggTargetCols = new IColumn[aggregations.Length];
            for (int i = 0; i < aggregations.Length; i++)
            {
                var agg = aggregations[i];
                Type targetType = agg.Operation switch
                {
                    AggOp.Count => typeof(int),
                    AggOp.Mean => typeof(double),
                    AggOp.Sum => typeof(double),
                    _ => gdf.Source[agg.SourceColumn].DataType
                };

                aggTargetCols[i] = ColumnFactory.Create(agg.TargetName, targetType, totalRows, isNullable: true);
                resultCols.Add(aggTargetCols[i]);
            }

            // =========================================================
            // PHASE 3: CALCULATE AGGREGATES
            // =========================================================
            var offsets = gdf.GroupOffsets;
            var indices = gdf.RowIndices;

            for (int g = 0; g < groupCount; g++)
            {
                int start = offsets[g];
                int end = offsets[g + 1];

                for (int a = 0; a < aggregations.Length; a++)
                {
                    var agg = aggregations[a];
                    object? res = ExecuteSingleAgg(gdf.Source, agg, indices, start, end);
                    aggTargetCols[a].AppendObject(res);
                }
            }

            // =========================================================
            // PHASE 4: HANDLE NULL GROUP
            // =========================================================
            if (gdf.NullGroupIndices is int[] nullIndices)
            {
                // Append Null to Keys
                for (int k = 0; k < gdf.GroupColumnNames.Length; k++)
                {
                    resultCols[k].AppendObject(null);
                }

                // Calc Aggs for Nulls
                for (int a = 0; a < aggregations.Length; a++)
                {
                    aggTargetCols[a].AppendObject(
                        ExecuteSingleAgg(gdf.Source, aggregations[a], nullIndices, 0, 0, isNullGroup: true)
                    );
                }
            }

            return new DataFrame(resultCols);
        }

        private static object? ExecuteSingleAgg(DataFrame source, AggregationDef agg, int[] indices, int start, int end, bool isNullGroup = false)
        {
            if (agg.Operation == AggOp.Count) return isNullGroup ? indices.Length : (end - start);

            var col = source[agg.SourceColumn];
            return agg.Operation switch
            {
                AggOp.Sum => col.ComputeSum(indices, start, end),
                AggOp.Mean => col.ComputeMean(indices, start, end),
                AggOp.Min => col.ComputeMin(indices, start, end),
                AggOp.Max => col.ComputeMax(indices, start, end),
                _ => null
            };
        }

        // --- Helper: Create Result DataFrame ---
        private static DataFrame CreateResultDataFrame(GroupedDataFrame gdf, string valColName, IColumn valCol)
        {
            var resultCols = new List<IColumn>();
            int totalCount = gdf.GroupCount + (gdf.NullGroupIndices != null ? 1 : 0);
            var keysArray = gdf.GetKeys();
            bool isRowIndexKeyMultiCol = (gdf.KeysAreRowIndices && gdf.GroupColumnNames.Length > 1)
                          || (gdf is GroupedDataFrame<int> && gdf.GroupColumnNames.Length > 1 && !gdf.KeysAreRowIndices);

            if (isRowIndexKeyMultiCol)
            {
                int[] rowIndices = (int[])keysArray;
                foreach (var colName in gdf.GroupColumnNames)
                {
                    var sourceCol = gdf.Source[colName];
                    var keyCol = ColumnFactory.Create(colName, sourceCol.DataType, totalCount, isNullable: true);

                    for (int i = 0; i < rowIndices.Length; i++)
                    {
                        keyCol.AppendObject(sourceCol.GetValue(rowIndices[i]));
                    }
                    if (gdf.NullGroupIndices != null) keyCol.AppendObject(null);

                    resultCols.Add(keyCol);
                }
            }
            else
            {
                var colName = gdf.GroupColumnNames[0];
                var keyType = keysArray.GetType().GetElementType()!;
                var keyCol = ColumnFactory.Create(colName, keyType, totalCount, isNullable: true);

                foreach (var key in keysArray) keyCol.AppendObject(key);
                if (gdf.NullGroupIndices != null) keyCol.AppendObject(null);

                resultCols.Add(keyCol);
            }

            resultCols.Add(valCol);
            return new DataFrame(resultCols);
        }

        // --- Helper: Generic Numeric Agg Execution ---
        private static DataFrame ExecuteNumericAgg(
            GroupedDataFrame gdf,
            string colName,
            string opName,
            IntSpanNullOp intNullHandler,
            IntSpanCsrOp intCsrHandler,
            DoubleSpanCsrOp dblCsrHandler)
        {
            var col = gdf.Source[colName];
            int groupCount = gdf.GroupCount;
            var offsets = gdf.GroupOffsets;
            var indices = gdf.RowIndices;
            bool hasNulls = gdf.NullGroupIndices != null;

            IColumn resultCol;

            if (col is IntColumn ic)
            {
                // Falls Operation double returniert (Mean), wird das hier ber√ºcksichtigt?
                // Derzeit wird DoubleColumn f√ºr alle Ergebnisse genutzt in diesem Helper.
                var res = new DoubleColumn($"{opName}_{colName}", groupCount + (hasNulls ? 1 : 0));
                ReadOnlySpan<int> data = ic.Values.Span;

                for (int i = 0; i < groupCount; i++)
                {
                    res.Append(intCsrHandler(data, offsets[i], offsets[i + 1], indices));
                }
                if (hasNulls)
                {
                    res.Append(intNullHandler(data, gdf.NullGroupIndices!));
                }
                resultCol = res;
            }
            else if (col is DoubleColumn dc)
            {
                var res = new DoubleColumn($"{opName}_{colName}", groupCount + (hasNulls ? 1 : 0));
                ReadOnlySpan<double> data = dc.Values.Span;

                for (int i = 0; i < groupCount; i++)
                {
                    res.Append(dblCsrHandler(data, offsets[i], offsets[i + 1], indices));
                }
                if (hasNulls)
                {
                    double sum = 0;
                    foreach (var idx in gdf.NullGroupIndices!) sum += data[idx];
                    res.Append(sum);
                }
                resultCol = res;
            }
            else throw new NotSupportedException($"Operation {opName} not supported for {col.DataType.Name}");

            return CreateResultDataFrame(gdf, $"{opName}_{colName}", resultCol);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Aggregate/StreamingAggregationExtensions.cs =====
using System.Runtime.CompilerServices;

namespace LeichtFrame.Core.Operations.Aggregate
{
    /// <summary>
    /// Extension methods for streaming aggregations.
    /// </summary>
    public static class StreamingAggregationExtensions
    {
        /// <summary>
        /// Provides a zero-allocation streaming enumerator over group key/count pairs.
        /// Enables high-performance aggregation scenarios.
        /// </summary>
        public static GroupCountStream CountStream(this GroupedDataFrame gdf)
        {
            return new GroupCountStream(gdf);
        }
    }

    /// <summary>
    /// The "Enumerable" (Struct, to avoid allocation).
    /// </summary>
    public readonly struct GroupCountStream
    {
        private readonly GroupedDataFrame _gdf;

        internal GroupCountStream(GroupedDataFrame gdf)
        {
            _gdf = gdf;
        }

        /// <summary>
        /// Gets the enumerator.
        /// </summary>
        /// <returns></returns>
        public GroupCountEnumerator GetEnumerator() => new GroupCountEnumerator(_gdf);
    }

    /// <summary>
    /// The "Enumerator" (Ref Struct). 
    /// Wrapper for NativeGroupCountReader.
    /// </summary>
    public ref struct GroupCountEnumerator
    {
        private NativeGroupCountEnumerator _reader;
        private readonly GroupedDataFrame _gdf;
        private int _currentKey;
        private int _currentCount;

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        internal GroupCountEnumerator(GroupedDataFrame gdf)
        {
            _gdf = gdf;
            _reader = gdf.GetCountReader();
            _currentKey = 0;
            _currentCount = 0;
        }

        /// <summary>
        /// Advances to the next key/count pair.
        /// </summary>
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public bool MoveNext()
        {
            return _reader.Read(out _currentKey, out _currentCount);
        }
        /// <summary>
        /// Gets the current key/count pair.
        /// </summary>
        public (int Key, int Count) Current
        {
            [MethodImpl(MethodImplOptions.AggressiveInlining)]
            get => (_currentKey, _currentCount);
        }

        /// <summary>
        /// Disposes the enumerator.
        /// </summary>
        public void Dispose()
        {
            _gdf.Dispose();
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Delegates/FilterDelegateOps.cs =====
namespace LeichtFrame.Core.Operations.Delegates
{
    internal static class FilterDelegateOps
    {
        public static DataFrame Execute(DataFrame df, Func<RowView, bool> predicate)
        {
            if (predicate == null) throw new ArgumentNullException(nameof(predicate));

            var indices = new List<int>(df.RowCount / 2);

            for (int i = 0; i < df.RowCount; i++)
            {
                var row = new RowView(i, df.Columns, df.Schema);
                if (predicate(row))
                {
                    indices.Add(i);
                }
            }

            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                newColumns.Add(col.CloneSubset(indices));
            }

            return new DataFrame(newColumns);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Filter/FilterOps.cs =====
namespace LeichtFrame.Core.Operations.Filter
{
    /// <summary>
    /// Provides extension methods for filtering <see cref="DataFrame"/> rows based on predicates.
    /// </summary>
    public static class DataFrameFilterExtensions
    {
        /// <summary>
        /// Filters rows based on a predicate function.
        /// Creates a new <see cref="DataFrame"/> with COPIED data containing only the matching rows.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="predicate">A function to test each row. Return <c>true</c> to keep the row, <c>false</c> to drop it.</param>
        /// <returns>A new DataFrame containing only the rows that satisfy the condition.</returns>
        /// <exception cref="ArgumentNullException">Thrown if <paramref name="predicate"/> is null.</exception>
        public static DataFrame Where(this DataFrame df, Func<RowView, bool> predicate)
        {
            if (predicate == null) throw new ArgumentNullException(nameof(predicate));

            // 1. Phase: Scan (Collect indices)
            // We use a capacity estimate to minimize resizing of the list
            var indices = new List<int>(df.RowCount / 2);

            // RowView is a struct (stack-only), so very cheap to create
            for (int i = 0; i < df.RowCount; i++)
            {
                var row = new RowView(i, df.Columns, df.Schema);
                if (predicate(row))
                {
                    indices.Add(i);
                }
            }

            // 2. Phase: Copy (Column-wise)
            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                // Each column takes care of efficiently copying the indices
                newColumns.Add(col.CloneSubset(indices));
            }

            return new DataFrame(newColumns);
        }

        /// <summary>
        /// Filters rows based on a predicate, returning a Zero-Copy View (<see cref="IndirectColumn{T}"/>).
        /// <para>
        /// **Performance:** Extremely fast creation (O(N) scan, 0 allocation for data). 
        /// **Trade-off:** Subsequent read access is slower due to indirection, and Vectorized operations (SIMD) 
        /// are not supported on the result until it is materialized.
        /// </para>
        /// </summary>
        public static DataFrame WhereView(this DataFrame df, Func<RowView, bool> predicate)
        {
            if (predicate == null) throw new ArgumentNullException(nameof(predicate));

            // 1. Scan (Collect indices)
            var indicesList = new List<int>(df.RowCount / 4); // Heuristic
            for (int i = 0; i < df.RowCount; i++)
            {
                var row = new RowView(i, df.Columns, df.Schema);
                if (predicate(row))
                {
                    indicesList.Add(i);
                }
            }
            int[] indicesArray = indicesList.ToArray();

            // 2. Create Views
            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                // Create IndirectColumn via Reflection (Generic Factory)
                var genericType = typeof(IndirectColumn<>).MakeGenericType(col.DataType);
                var view = Activator.CreateInstance(genericType, col, indicesArray);
                newColumns.Add((IColumn)view!);
            }

            return new DataFrame(newColumns);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Filter/VectorizedFilterOps.cs =====
using System.Numerics;
using System.Runtime.InteropServices;

namespace LeichtFrame.Core.Operations.Filter
{
    /// <summary>
    /// Provides high-performance, SIMD-accelerated filtering operations for DataFrames.
    /// </summary>
    public static class VectorizedFilterOps
    {
        /// <summary>
        /// Filters the DataFrame using hardware-accelerated SIMD instructions.
        /// Supports primitive types like int, double, float.
        /// </summary>
        /// <typeparam name="T">The type of the column (must be an INumber).</typeparam>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="columnName">The name of the column to filter by.</param>
        /// <param name="op">The comparison operator.</param>
        /// <param name="value">The value to compare against.</param>
        /// <returns>A new DataFrame containing only the matching rows.</returns>
        public static DataFrame WhereVec<T>(this DataFrame df, string columnName, CompareOp op, T value)
            where T : struct, INumber<T>
        {
            var col = df[columnName];
            if (col is not IColumn<T> typedCol)
            {
                throw new ArgumentException($"Column '{columnName}' is not of type {typeof(T).Name}");
            }

            ReadOnlySpan<T> data = typedCol.AsSpan();
            var indices = new List<int>(data.Length / 4);

            int i = 0;

            if (Vector.IsHardwareAccelerated)
            {
                int vectorSize = Vector<T>.Count;
                var valueVec = new Vector<T>(value);

                var vectorSpan = MemoryMarshal.Cast<T, Vector<T>>(data);

                for (int vIdx = 0; vIdx < vectorSpan.Length; vIdx++)
                {
                    var dataVec = vectorSpan[vIdx];
                    Vector<T> resultVec;

                    switch (op)
                    {
                        case CompareOp.Equal:
                            resultVec = Vector.Equals(dataVec, valueVec);
                            break;
                        case CompareOp.GreaterThan:
                            resultVec = Vector.GreaterThan(dataVec, valueVec);
                            break;
                        case CompareOp.GreaterThanOrEqual:
                            resultVec = Vector.GreaterThanOrEqual(dataVec, valueVec);
                            break;
                        case CompareOp.LessThan:
                            resultVec = Vector.LessThan(dataVec, valueVec);
                            break;
                        case CompareOp.LessThanOrEqual:
                            resultVec = Vector.LessThanOrEqual(dataVec, valueVec);
                            break;
                        case CompareOp.NotEqual:
                            resultVec = Vector.OnesComplement(Vector.Equals(dataVec, valueVec));
                            break;
                        default:
                            throw new NotSupportedException($"Operator {op} not supported.");
                    }

                    if (resultVec == Vector<T>.Zero)
                    {
                        i += vectorSize;
                        continue;
                    }

                    for (int k = 0; k < vectorSize; k++)
                    {
                        if (resultVec[k] != T.Zero)
                        {
                            int absoluteIndex = i + k;
                            if (!col.IsNullable || !col.IsNull(absoluteIndex))
                            {
                                indices.Add(absoluteIndex);
                            }
                        }
                    }
                    i += vectorSize;
                }
            }

            for (; i < data.Length; i++)
            {
                T val = data[i];
                if (col.IsNullable && col.IsNull(i)) continue;

                bool match = op switch
                {
                    CompareOp.Equal => val == value,
                    CompareOp.NotEqual => val != value,
                    CompareOp.GreaterThan => val > value,
                    CompareOp.GreaterThanOrEqual => val >= value,
                    CompareOp.LessThan => val < value,
                    CompareOp.LessThanOrEqual => val <= value,
                    _ => false
                };

                if (match) indices.Add(i);
            }

            var newColumns = new List<IColumn>();
            foreach (var originalCol in df.Columns)
            {
                newColumns.Add(originalCol.CloneSubset(indices));
            }

            return new DataFrame(newColumns);
        }

        /// <summary>
        /// Specialized vectorized filter for DateTime columns.
        /// </summary>
        public static DataFrame WhereVec(this DataFrame df, string columnName, CompareOp op, DateTime value)
        {
            var col = df[columnName];
            if (col is not DateTimeColumn dtCol)
            {
                throw new ArgumentException($"Column '{columnName}' is not a DateTimeColumn.");
            }

            var data = dtCol.Values.Span;
            var indices = new List<int>(data.Length / 4);

            // Scalar Loop Optimization (DateTime comparison is cheap)
            // Future Optimization: Cast<DateTime, long> and use SIMD on Ticks
            for (int i = 0; i < data.Length; i++)
            {
                // Null Check
                if (dtCol.IsNullable && dtCol.IsNull(i)) continue;

                var val = data[i];
                bool match = op switch
                {
                    CompareOp.Equal => val == value,
                    CompareOp.NotEqual => val != value,
                    CompareOp.GreaterThan => val > value,
                    CompareOp.GreaterThanOrEqual => val >= value,
                    CompareOp.LessThan => val < value,
                    CompareOp.LessThanOrEqual => val <= value,
                    _ => false
                };

                if (match) indices.Add(i);
            }

            // Materialize Result
            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var originalCol in df.Columns)
            {
                newColumns.Add(originalCol.CloneSubset(indices));
            }

            return new DataFrame(newColumns);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/GroupBy/GroupingOps.cs =====
using LeichtFrame.Core.Engine.Kernels.GroupBy;

namespace LeichtFrame.Core.Operations.GroupBy
{
    /// <summary>
    /// Exposes single- and multi-column group by operations
    /// </summary>
    public static class GroupingOps
    {
        /// <summary>
        /// Single-column GroupBy
        /// </summary>
        /// <param name="df"></param>
        /// <param name="columnName"></param>
        /// /// <exception cref="ArgumentNullException"></exception>
        public static GroupedDataFrame GroupBy(this DataFrame df, string columnName)
        {
            if (string.IsNullOrEmpty(columnName)) throw new ArgumentNullException(nameof(columnName));

            return GroupByDispatcher.DecideAndExecute(df, columnName);
        }

        /// <summary>
        /// Multi-column GroupBy
        /// </summary>
        /// <param name="df"></param>
        /// <param name="columnNames"></param>
        /// <returns></returns>
        /// <exception cref="ArgumentException"></exception>
        public static GroupedDataFrame GroupBy(this DataFrame df, params string[] columnNames)
        {
            if (columnNames == null || columnNames.Length == 0)
                throw new ArgumentException("At least one column required.");

            return GroupByDispatcher.DecideAndExecute(df, columnNames);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Join/JoinOps.cs =====
namespace LeichtFrame.Core.Operations.Join
{
    /// <summary>
    /// Provides extension methods for joining multiple <see cref="DataFrame"/> objects.
    /// Optimized with typed HashMaps to avoid boxing overhead.
    /// </summary>
    public static class DataFrameJoinExtensions
    {
        /// <summary>
        /// Joins two DataFrames based on a common key column using a Hash Join algorithm.
        /// Supports Inner and Left joins.
        /// </summary>
        public static DataFrame Join(this DataFrame left, DataFrame right, string on, JoinType joinType = JoinType.Inner)
        {
            Type type = left[on].DataType;

            Type coreType = Nullable.GetUnderlyingType(type) ?? type;

            if (coreType == typeof(int))
                return ExecuteJoin<int>(left, right, on, joinType);

            if (coreType == typeof(double))
                return ExecuteJoin<double>(left, right, on, joinType);

            if (coreType == typeof(string))
                return ExecuteJoin<string>(left, right, on, joinType);

            if (coreType == typeof(bool))
                return ExecuteJoin<bool>(left, right, on, joinType);

            if (coreType == typeof(DateTime))
                return ExecuteJoin<DateTime>(left, right, on, joinType);

            return ExecuteJoin<object>(left, right, on, joinType);
        }

        private static DataFrame ExecuteJoin<T>(DataFrame left, DataFrame right, string on, JoinType joinType)
            where T : notnull
        {
            var leftKeyCol = (IColumn<T>)left[on];
            var rightKeyCol = (IColumn<T>)right[on];

            var hashTable = new Dictionary<T, List<int>>();

            for (int r = 0; r < right.RowCount; r++)
            {
                if (rightKeyCol.IsNull(r)) continue;

                T key = rightKeyCol.GetValue(r);

                if (key == null) continue;

                if (!hashTable.TryGetValue(key, out var indices))
                {
                    indices = new List<int>();
                    hashTable[key] = indices;
                }
                indices.Add(r);
            }

            var leftIndices = new List<int>(left.RowCount);
            var rightIndices = new List<int>(left.RowCount);

            for (int l = 0; l < left.RowCount; l++)
            {
                if (leftKeyCol.IsNull(l))
                {
                    if (joinType == JoinType.Left)
                    {
                        leftIndices.Add(l);
                        rightIndices.Add(-1);
                    }
                    continue;
                }

                T key = leftKeyCol.GetValue(l);
                if (key == null)
                {
                    if (joinType == JoinType.Left) { leftIndices.Add(l); rightIndices.Add(-1); }
                    continue;
                }

                if (hashTable.TryGetValue(key, out var matchingRightIndices))
                {
                    foreach (var rIdx in matchingRightIndices)
                    {
                        leftIndices.Add(l);
                        rightIndices.Add(rIdx);
                    }
                }
                else if (joinType == JoinType.Left)
                {
                    leftIndices.Add(l);
                    rightIndices.Add(-1);
                }
            }

            return MaterializeResult(left, right, on, leftIndices, rightIndices, joinType);
        }

        private static DataFrame MaterializeResult(
            DataFrame left,
            DataFrame right,
            string on,
            List<int> leftIndices,
            List<int> rightIndices,
            JoinType joinType)
        {
            var newColumns = new List<IColumn>();

            foreach (var col in left.Columns)
            {
                newColumns.Add(col.CloneSubset(leftIndices));
            }

            foreach (var col in right.Columns)
            {
                if (col.Name == on) continue;

                if (left.Schema.HasColumn(col.Name))
                    throw new NotSupportedException($"Column collision: '{col.Name}' exists in both DataFrames.");

                bool forceNullable = joinType == JoinType.Left || col.IsNullable;
                IColumn newCol = MaterializeRightColumn(col, rightIndices, forceNullable);
                newColumns.Add(newCol);
            }

            return new DataFrame(newColumns);
        }

        private static IColumn MaterializeRightColumn(IColumn source, List<int> indices, bool isNullable)
        {
            IColumn newCol = ColumnFactory.Create(source.Name, source.DataType, indices.Count, isNullable);

            if (source is IntColumn ic && newCol is IntColumn nic)
            {
                for (int i = 0; i < indices.Count; i++)
                {
                    int idx = indices[i];
                    if (idx == -1 || ic.IsNull(idx)) nic.Append(null);
                    else nic.Append(ic.Get(idx));
                }
            }
            else if (source is DoubleColumn dc && newCol is DoubleColumn ndc)
            {
                for (int i = 0; i < indices.Count; i++)
                {
                    int idx = indices[i];
                    if (idx == -1 || dc.IsNull(idx)) ndc.Append(null);
                    else ndc.Append(dc.Get(idx));
                }
            }
            else if (source is StringColumn sc && newCol is StringColumn nsc)
            {
                for (int i = 0; i < indices.Count; i++)
                {
                    int idx = indices[i];
                    if (idx == -1) nsc.Append(null);
                    else nsc.Append(sc.Get(idx));
                }
            }
            else if (source is BoolColumn bc && newCol is BoolColumn nbc)
            {
                for (int i = 0; i < indices.Count; i++)
                {
                    int idx = indices[i];
                    if (idx == -1 || bc.IsNull(idx)) nbc.Append(null);
                    else nbc.Append(bc.Get(idx));
                }
            }
            else if (source is DateTimeColumn dtc && newCol is DateTimeColumn ndtc)
            {
                for (int i = 0; i < indices.Count; i++)
                {
                    int idx = indices[i];
                    if (idx == -1 || dtc.IsNull(idx)) ndtc.Append(null);
                    else ndtc.Append(dtc.Get(idx));
                }
            }
            else
            {
                // Fallback
                for (int i = 0; i < indices.Count; i++)
                {
                    int idx = indices[i];
                    if (idx == -1) newCol.AppendObject(null);
                    else newCol.AppendObject(source.GetValue(idx));
                }
            }

            return newCol;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Sort/OrderOps.cs =====
namespace LeichtFrame.Core.Operations.Sort
{
    /// <summary>
    /// Provides extension methods for sorting DataFrames.
    /// </summary>
    public static class OrderOps
    {
        /// <summary>
        /// Sorts the DataFrame rows in ascending order based on the values in the specified column.
        /// </summary>
        public static DataFrame OrderBy(this DataFrame df, string columnName)
        {
            return SortInternal(df, columnName, ascending: true);
        }

        /// <summary>
        /// Sorts the DataFrame rows in descending order based on the values in the specified column.
        /// </summary>
        public static DataFrame OrderByDescending(this DataFrame df, string columnName)
        {
            return SortInternal(df, columnName, ascending: false);
        }

        // --- NEU: Multi-Column Support ---

        /// <summary>
        /// Sorts the DataFrame by multiple columns.
        /// </summary>
        public static DataFrame OrderBy(this DataFrame df, string[] columnNames, bool[] ascending)
        {
            if (columnNames.Length != ascending.Length)
                throw new ArgumentException("Column names and ascending flags count mismatch");

            var cols = new IColumn[columnNames.Length];
            for (int i = 0; i < columnNames.Length; i++)
            {
                cols[i] = df[columnNames[i]];
            }

            int[] sortedIndices = SortingOps.GetSortedIndices(cols, ascending);

            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                newColumns.Add(col.CloneSubset(sortedIndices));
            }

            return new DataFrame(newColumns);
        }

        // ---

        private static DataFrame SortInternal(DataFrame df, string columnName, bool ascending)
        {
            if (string.IsNullOrEmpty(columnName)) throw new ArgumentNullException(nameof(columnName));

            var sortCol = df[columnName];
            int[] sortedIndices = sortCol.GetSortedIndices(ascending);

            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                newColumns.Add(col.CloneSubset(sortedIndices));
            }

            return new DataFrame(newColumns);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Sort/SortingOps.cs =====
namespace LeichtFrame.Core.Operations.Sort
{
    /// <summary>
    /// Provides sorting functionality for DataFrames and Columns.
    /// </summary>
    public static class SortingOps
    {
        /// <summary>
        /// Returns sorted indices for a single column.
        /// </summary>
        public static int[] GetSortedIndices(this IColumn column, bool ascending = true)
        {
            int[] indices = new int[column.Length];
            for (int i = 0; i < indices.Length; i++) indices[i] = i;

            IComparer<int> comparer = CreateComparer(column, ascending);
            Array.Sort(indices, comparer);

            return indices;
        }

        // --- NEU: Multi-Column Support ---

        /// <summary>
        /// Returns sorted indices based on multiple columns (stable sort logic).
        /// </summary>
        public static int[] GetSortedIndices(this IColumn[] columns, bool[] ascending)
        {
            if (columns == null || columns.Length == 0) throw new ArgumentException("No columns provided");
            int length = columns[0].Length;
            int[] indices = new int[length];
            for (int i = 0; i < length; i++) indices[i] = i;

            var comparer = new MultiColumnIndirectComparer(columns, ascending);
            Array.Sort(indices, comparer);

            return indices;
        }

        // --- Comparers ---

        private static IComparer<int> CreateComparer(IColumn column, bool asc)
        {
            if (column is IntColumn ic) return new IntIndirectComparer(ic, asc);
            if (column is DoubleColumn dc) return new DoubleIndirectComparer(dc, asc);
            if (column is StringColumn sc) return new StringIndirectComparer(sc, asc);
            if (column is DateTimeColumn dtc) return new DateTimeIndirectComparer(dtc, asc);
            if (column is BoolColumn bc) return new BoolIndirectComparer(bc, asc);
            return new ObjectIndirectComparer(column, asc);
        }

        internal readonly struct MultiColumnIndirectComparer : IComparer<int>
        {
            private readonly IComparer<int>[] _comparers;

            public MultiColumnIndirectComparer(IColumn[] cols, bool[] ascending)
            {
                _comparers = new IComparer<int>[cols.Length];
                for (int i = 0; i < cols.Length; i++)
                {
                    _comparers[i] = CreateComparer(cols[i], ascending[i]);
                }
            }

            public int Compare(int x, int y)
            {
                for (int i = 0; i < _comparers.Length; i++)
                {
                    int cmp = _comparers[i].Compare(x, y);
                    if (cmp != 0) return cmp;
                }
                return 0;
            }
        }

        private static int CompareNulls(bool isNullX, bool isNullY)
        {
            if (isNullX && isNullY) return 0;
            if (isNullX) return -1;
            return 1;
        }

        private readonly struct IntIndirectComparer : IComparer<int>
        {
            private readonly IntColumn _col;
            private readonly int _direction;
            public IntIndirectComparer(IntColumn col, bool asc) { _col = col; _direction = asc ? 1 : -1; }
            public int Compare(int x, int y)
            {
                bool nx = _col.IsNull(x); bool ny = _col.IsNull(y);
                if (nx || ny) return CompareNulls(nx, ny) * _direction;
                return _col.Get(x).CompareTo(_col.Get(y)) * _direction;
            }
        }

        private readonly struct DoubleIndirectComparer : IComparer<int>
        {
            private readonly DoubleColumn _col;
            private readonly int _direction;
            public DoubleIndirectComparer(DoubleColumn col, bool asc) { _col = col; _direction = asc ? 1 : -1; }
            public int Compare(int x, int y)
            {
                bool nx = _col.IsNull(x); bool ny = _col.IsNull(y);
                if (nx || ny) return CompareNulls(nx, ny) * _direction;
                return _col.Get(x).CompareTo(_col.Get(y)) * _direction;
            }
        }

        private readonly struct StringIndirectComparer : IComparer<int>
        {
            private readonly StringColumn _col;
            private readonly int _direction;
            public StringIndirectComparer(StringColumn col, bool asc) { _col = col; _direction = asc ? 1 : -1; }
            public int Compare(int x, int y) { return _col.CompareRaw(x, y) * _direction; }
        }

        private readonly struct DateTimeIndirectComparer : IComparer<int>
        {
            private readonly DateTimeColumn _col;
            private readonly int _direction;
            public DateTimeIndirectComparer(DateTimeColumn col, bool asc) { _col = col; _direction = asc ? 1 : -1; }
            public int Compare(int x, int y)
            {
                bool nx = _col.IsNull(x); bool ny = _col.IsNull(y);
                if (nx || ny) return CompareNulls(nx, ny) * _direction;
                return _col.Get(x).CompareTo(_col.Get(y)) * _direction;
            }
        }

        private readonly struct BoolIndirectComparer : IComparer<int>
        {
            private readonly BoolColumn _col;
            private readonly int _direction;
            public BoolIndirectComparer(BoolColumn col, bool asc) { _col = col; _direction = asc ? 1 : -1; }
            public int Compare(int x, int y)
            {
                bool nx = _col.IsNull(x); bool ny = _col.IsNull(y);
                if (nx || ny) return CompareNulls(nx, ny) * _direction;
                return _col.Get(x).CompareTo(_col.Get(y)) * _direction;
            }
        }

        private readonly struct ObjectIndirectComparer : IComparer<int>
        {
            private readonly IColumn _col;
            private readonly int _direction;
            public ObjectIndirectComparer(IColumn col, bool asc) { _col = col; _direction = asc ? 1 : -1; }
            public int Compare(int x, int y)
            {
                bool nx = _col.IsNull(x); bool ny = _col.IsNull(y);
                if (nx || ny) return CompareNulls(nx, ny) * _direction;
                var valX = _col.GetValue(x) as IComparable;
                var valY = _col.GetValue(y) as IComparable;
                if (valX == null && valY == null) return 0;
                if (valX == null) return -1 * _direction;
                if (valY == null) return 1 * _direction;
                return valX.CompareTo(valY) * _direction;
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Sort/TopNOps.cs =====
using LeichtFrame.Core.Operations.Transform;

namespace LeichtFrame.Core.Operations.Sort
{
    /// <summary>
    /// Provides optimized Top-N / Bottom-N selection algorithms (Heap-based).
    /// </summary>
    public static class TopNOps
    {
        /// <summary>
        /// Returns the N rows with the smallest values in the specified column.
        /// (Equivalent to OrderBy(column).Head(n), but significantly faster).
        /// </summary>
        public static DataFrame Smallest(this DataFrame df, int n, string columnName)
        {
            return TopNInternal(df, n, columnName, ascending: true);
        }

        /// <summary>
        /// Returns the N rows with the largest values in the specified column.
        /// (Equivalent to OrderByDescending(column).Head(n), but significantly faster).
        /// </summary>
        public static DataFrame Largest(this DataFrame df, int n, string columnName)
        {
            return TopNInternal(df, n, columnName, ascending: false);
        }

        private static DataFrame TopNInternal(DataFrame df, int n, string columnName, bool ascending)
        {
            if (n <= 0) return df.Head(0); // Return empty schema
            if (n >= df.RowCount) return ascending ? df.OrderBy(columnName) : df.OrderByDescending(columnName);

            var col = df[columnName];
            var indices = new int[n];
            int finalCount = 0;

            if (col is IntColumn ic)
            {
                finalCount = GetIndices(ic, n, ascending, indices);
            }
            else if (col is DoubleColumn dc)
            {
                finalCount = GetIndices(dc, n, ascending, indices);
            }
            else if (col is StringColumn sc)
            {
                finalCount = GetIndices(sc, n, ascending, indices);
            }
            else
            {
                // Fallback: Full Sort
                return ascending ? df.OrderBy(columnName).Head(n) : df.OrderByDescending(columnName).Head(n);
            }

            Array.Resize(ref indices, finalCount);

            // Final Sort of the small result set
            var subsetIndices = SortIndicesByColumn(indices, col, ascending);

            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var c in df.Columns)
            {
                newColumns.Add(c.CloneSubset(subsetIndices));
            }
            return new DataFrame(newColumns);
        }

        // --- Type Specific Implementations ---

        private static int GetIndices(IntColumn col, int n, bool smallestN, int[] resultBuffer)
        {
            var comparer = smallestN
                ? Comparer<int>.Create((x, y) => y.CompareTo(x))
                : Comparer<int>.Default;

            var queue = new PriorityQueue<int, int>(n, comparer);

            for (int i = 0; i < col.Length; i++)
            {
                int val = col.Get(i);

                if (queue.Count < n)
                {
                    queue.Enqueue(i, val);
                }
                else
                {
                    if (queue.TryPeek(out int _, out int worstVal))
                    {
                        bool shouldSwap = smallestN
                            ? val < worstVal
                            : val > worstVal;

                        if (shouldSwap)
                        {
                            queue.Dequeue();
                            queue.Enqueue(i, val);
                        }
                    }
                }
            }

            int count = queue.Count;
            for (int i = count - 1; i >= 0; i--)
            {
                resultBuffer[i] = queue.Dequeue();
            }
            return count;
        }

        private static int GetIndices(DoubleColumn col, int n, bool smallestN, int[] resultBuffer)
        {
            var comparer = smallestN
                ? Comparer<double>.Create((x, y) => y.CompareTo(x))
                : Comparer<double>.Default;

            var queue = new PriorityQueue<int, double>(n, comparer);

            for (int i = 0; i < col.Length; i++)
            {
                if (col.IsNull(i)) continue;

                double val = col.Get(i);

                if (queue.Count < n)
                {
                    queue.Enqueue(i, val);
                }
                else
                {
                    if (queue.TryPeek(out int _, out double worstVal))
                    {
                        bool shouldSwap = smallestN ? val < worstVal : val > worstVal;
                        if (shouldSwap)
                        {
                            queue.Dequeue();
                            queue.Enqueue(i, val);
                        }
                    }
                }
            }

            int count = queue.Count;
            for (int i = count - 1; i >= 0; i--) resultBuffer[i] = queue.Dequeue();
            return count;
        }

        private static int GetIndices(StringColumn col, int n, bool smallestN, int[] resultBuffer)
        {
            var comparer = Comparer<int>.Create((x, y) =>
            {
                int cmp = col.CompareRaw(x, y);
                return smallestN ? cmp * -1 : cmp;
            });

            var queue = new PriorityQueue<int, int>(n, comparer);

            for (int i = 0; i < col.Length; i++)
            {
                if (queue.Count < n)
                {
                    queue.Enqueue(i, i);
                }
                else
                {
                    int worstIndex = queue.Peek();

                    int cmp = col.CompareRaw(i, worstIndex);

                    bool isBetter = smallestN
                        ? cmp < 0
                        : cmp > 0;

                    if (isBetter)
                    {
                        queue.Dequeue();
                        queue.Enqueue(i, i);
                    }
                }
            }

            int count = queue.Count;
            for (int i = count - 1; i >= 0; i--)
            {
                resultBuffer[i] = queue.Dequeue();
            }
            return count;
        }

        private static int[] SortIndicesByColumn(int[] indices, IColumn col, bool ascending)
        {
            if (col is IntColumn ic)
            {
                Array.Sort(indices, (a, b) => (ascending ? 1 : -1) * ic.Get(a).CompareTo(ic.Get(b)));
            }
            else if (col is DoubleColumn dc)
            {
                Array.Sort(indices, (a, b) => (ascending ? 1 : -1) * dc.Get(a).CompareTo(dc.Get(b)));
            }
            else if (col is StringColumn sc)
            {
                Array.Sort(indices, (a, b) => (ascending ? 1 : -1) * sc.CompareRaw(a, b));
            }

            return indices;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Transform/CleaningOps.cs =====
namespace LeichtFrame.Core.Operations.Transform
{
    /// <summary>
    /// Provides extension methods for cleaning data (handling nulls).
    /// </summary>
    public static class CleaningOps
    {
        /// <summary>
        /// Removes all rows that contain at least one null value in any column.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <returns>A new DataFrame with only complete rows.</returns>
        public static DataFrame DropNulls(this DataFrame df)
        {
            // 1. Identify columns that are actually nullable (Optimization)
            var nullableCols = new List<IColumn>();
            foreach (var col in df.Columns)
            {
                if (col.IsNullable) nullableCols.Add(col);
            }

            // If no columns are nullable, return the original (or a clone if we want strict immutability semantics? 
            // Usually DropNulls implies "if nothing dropped, return self" is acceptable for performance).
            if (nullableCols.Count == 0) return df; // Zero-Copy optimization

            var indices = new List<int>(df.RowCount);

            // 2. Scan rows
            for (int i = 0; i < df.RowCount; i++)
            {
                bool hasNull = false;
                // Only check nullable columns
                for (int c = 0; c < nullableCols.Count; c++)
                {
                    if (nullableCols[c].IsNull(i))
                    {
                        hasNull = true;
                        break;
                    }
                }

                if (!hasNull)
                {
                    indices.Add(i);
                }
            }

            // 3. Create Subset
            // If we kept all rows, return original
            if (indices.Count == df.RowCount) return df;

            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                newColumns.Add(col.CloneSubset(indices));
            }

            return new DataFrame(newColumns);
        }

        /// <summary>
        /// Replaces null values in the specified column with a constant value.
        /// </summary>
        /// <typeparam name="T">The type of the column data.</typeparam>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="columnName">The name of the column to fill.</param>
        /// <param name="value">The value to replace nulls with.</param>
        /// <returns>A new DataFrame with the filled column.</returns>
        public static DataFrame FillNull<T>(this DataFrame df, string columnName, T value)
        {
            var targetCol = df[columnName];

            // If column is not nullable, nothing to do (return self logic, but we need to reconstruct DF to be safe?)
            // Let's assume we modify the specific column in the new DF.
            if (!targetCol.IsNullable) return df;

            // 1. Create a deep copy of the target column but WITHOUT NullBitmap support (IsNullable = false)
            // Strategy: We create a new column, copy all values manually, replacing nulls on the fly.

            // Note: We cannot easily "Deep Copy" an IColumn via Interface.
            // We use ColumnFactory to create a fresh one.
            var newCol = ColumnFactory.Create<T>(columnName, df.RowCount, isNullable: false);

            if (targetCol is IColumn<T> typedSource)
            {
                for (int i = 0; i < df.RowCount; i++)
                {
                    if (typedSource.IsNull(i))
                    {
                        newCol.Append(value);
                    }
                    else
                    {
                        newCol.Append(typedSource.GetValue(i));
                    }
                }
            }
            else
            {
                throw new ArgumentException($"Column '{columnName}' is not of type {typeof(T).Name}");
            }

            // 2. Build new DataFrame
            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                if (col.Name == columnName)
                {
                    newColumns.Add(newCol);
                }
                else
                {
                    // Zero-Copy for other columns
                    newColumns.Add(col);
                }
            }

            return new DataFrame(newColumns);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Transform/DeduplicationOps.cs =====
namespace LeichtFrame.Core.Operations.Transform
{
    /// <summary>
    /// Provides extension methods for removing duplicate rows.
    /// </summary>
    public static class DeduplicationOps
    {
        /// <summary>
        /// Returns a new DataFrame containing only unique rows.
        /// </summary>
        public static DataFrame Distinct(this DataFrame df)
        {
            return Distinct(df, df.GetColumnNames().ToArray());
        }

        /// <summary>
        /// Returns a new DataFrame containing rows that are unique based on the specified subset of columns.
        /// </summary>
        public static DataFrame Distinct(this DataFrame df, params string[] columnNames)
        {
            if (columnNames == null || columnNames.Length == 0)
                throw new ArgumentException("At least one column must be specified.");

            // Optimization: Single Column Distinct
            // We can use a typed HashSet<T> to avoid boxing/allocations
            if (columnNames.Length == 1)
            {
                return DistinctSingleColumn(df, columnNames[0]);
            }

            // Fallback: Multi-column slow path
            return DistinctMultiColumn(df, columnNames);
        }

        private static DataFrame DistinctSingleColumn(DataFrame df, string columnName)
        {
            var col = df[columnName];
            Type type = col.DataType;
            Type coreType = Nullable.GetUnderlyingType(type) ?? type;

            if (coreType == typeof(int)) return ExecuteDistinctSingle<int>(df, col);
            if (coreType == typeof(double)) return ExecuteDistinctSingle<double>(df, col);
            if (coreType == typeof(string)) return ExecuteDistinctSingle<string>(df, col);
            if (coreType == typeof(bool)) return ExecuteDistinctSingle<bool>(df, col);
            if (coreType == typeof(DateTime)) return ExecuteDistinctSingle<DateTime>(df, col);

            return ExecuteDistinctSingle<object>(df, col);
        }

        private static DataFrame ExecuteDistinctSingle<T>(DataFrame df, IColumn colUntyped) where T : notnull
        {
            var col = (IColumn<T>)colUntyped;
            var seen = new HashSet<T>();
            var indices = new List<int>(df.RowCount);
            bool nullSeen = false;

            for (int i = 0; i < df.RowCount; i++)
            {
                if (col.IsNull(i))
                {
                    if (!nullSeen)
                    {
                        nullSeen = true;
                        indices.Add(i);
                    }
                    continue;
                }

                T val = col.GetValue(i);

                // HashSet.Add returns true if the element was added (new unique)
                if (val != null && seen.Add(val))
                {
                    indices.Add(i);
                }
                else if (val == null && !nullSeen) // Handle nulls in reference types (strings)
                {
                    nullSeen = true;
                    indices.Add(i);
                }
            }

            return CreateSubset(df, indices);
        }

        private static DataFrame DistinctMultiColumn(DataFrame df, string[] columnNames)
        {
            var colsToCheck = new IColumn[columnNames.Length];
            for (int i = 0; i < columnNames.Length; i++)
            {
                colsToCheck[i] = df[columnNames[i]];
            }

            var comparer = new RowIndexComparer(colsToCheck);
            var seenRows = new HashSet<int>(comparer);
            var uniqueIndices = new List<int>(df.RowCount);

            for (int i = 0; i < df.RowCount; i++)
            {
                if (seenRows.Add(i))
                {
                    uniqueIndices.Add(i);
                }
            }

            return CreateSubset(df, uniqueIndices);
        }

        private static DataFrame CreateSubset(DataFrame df, List<int> indices)
        {
            if (indices.Count == df.RowCount) return df;

            var newColumns = new List<IColumn>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                newColumns.Add(col.CloneSubset(indices));
            }
            return new DataFrame(newColumns);
        }

        private class RowIndexComparer : IEqualityComparer<int>
        {
            private readonly IColumn[] _columns;

            public RowIndexComparer(IColumn[] columns)
            {
                _columns = columns;
            }

            public bool Equals(int x, int y)
            {
                for (int c = 0; c < _columns.Length; c++)
                {
                    var col = _columns[c];
                    object? valX = col.GetValue(x);
                    object? valY = col.GetValue(y);
                    if (!object.Equals(valX, valY)) return false;
                }
                return true;
            }

            public int GetHashCode(int obj)
            {
                var hash = new HashCode();
                for (int c = 0; c < _columns.Length; c++)
                {
                    hash.Add(_columns[c].GetValue(obj));
                }
                return hash.ToHashCode();
            }
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Transform/RenameOps.cs =====
using LeichtFrame.Core.Engine.Kernels.Rename;

namespace LeichtFrame.Core.Operations.Transform
{
    /// <summary>
    /// Executes Rename Operations using Engine
    /// </summary>
    public static class RenameOps
    {
        /// <summary>
        /// /// Executes Rename Operations using Engine
        /// </summary>
        /// <param name="col"></param>
        /// <param name="newName"></param>
        /// <returns></returns>
        public static IColumn Rename(this IColumn col, string newName)
        {
            return RenameDispatcher.Execute(col, newName);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Transform/SelectionOps.cs =====
namespace LeichtFrame.Core.Operations.Transform
{
    /// <summary>
    /// Provides extension methods for selecting columns and slicing rows from a <see cref="DataFrame"/>.
    /// </summary>
    public static class DataFrameSelectionExtensions
    {
        /// <summary>
        /// Projects the DataFrame to a new DataFrame containing only the selected columns.
        /// Uses the new Lazy Engine internally.
        /// </summary>
        public static DataFrame Select(this DataFrame df, params string[] columnNames)
        {
            // BRIDGE: Eager Call -> Lazy Build -> Optimize -> Collect
            return df.Lazy()
                     .Select(columnNames)
                     .Collect();
        }

        /// <summary>
        /// Returns a zero-copy view of the DataFrame restricted to the specified row range.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="start">The zero-based starting row index.</param>
        /// <param name="length">The number of rows to include in the slice.</param>
        /// <returns>A new DataFrame containing the subset of rows.</returns>
        public static DataFrame Slice(this DataFrame df, int start, int length)
        {
            if (start < 0) throw new ArgumentOutOfRangeException(nameof(start));

            // Bounds adjusting (Robustness)
            if (start >= df.RowCount)
            {
                // Return empty DataFrame with same schema
                return DataFrame.Create(df.Schema, 0);
            }

            int validLength = Math.Min(length, df.RowCount - start);

            var newColumns = new List<IColumn>(df.ColumnCount);

            foreach (var col in df.Columns)
            {
                // Magic: Create SlicedColumn<T> dynamically
                var genericType = typeof(SlicedColumn<>).MakeGenericType(col.DataType);

                // Invoke Constructor: SlicedColumn(source, offset, length)
                var slicedCol = Activator.CreateInstance(genericType, col, start, validLength);

                newColumns.Add((IColumn)slicedCol!);
            }

            return new DataFrame(newColumns);
        }

        /// <summary>
        /// Returns the first <paramref name="count"/> rows of the DataFrame.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="count">The number of rows to return from the beginning.</param>
        /// <returns>A new DataFrame containing the first rows.</returns>
        public static DataFrame Head(this DataFrame df, int count)
        {
            return df.Slice(0, count);
        }

        /// <summary>
        /// Returns the last <paramref name="count"/> rows of the DataFrame.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="count">The number of rows to return from the end.</param>
        /// <returns>A new DataFrame containing the last rows.</returns>
        public static DataFrame Tail(this DataFrame df, int count)
        {
            int start = Math.Max(0, df.RowCount - count);
            // length is count, but Slice logic handles if start+count > RowCount (though here it matches)
            int length = Math.Min(count, df.RowCount);
            return df.Slice(start, length);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Operations/Transform/TransformationOps.cs =====
using System.Runtime.CompilerServices;

namespace LeichtFrame.Core.Operations.Transform
{
    /// <summary>
    /// Provides extension methods for transforming data and adding computed columns.
    /// </summary>
    public static class TransformationOps
    {
        /// <summary>
        /// Creates a new column by applying a function to every row and adds it to the DataFrame.
        /// Returns a new DataFrame instance (the original remains unchanged).
        /// </summary>
        /// <typeparam name="T">The type of the new column (e.g., int, double, string).</typeparam>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="newColumnName">The name of the new column.</param>
        /// <param name="computer">A function that takes a RowView and returns the calculated value.</param>
        /// <returns>A new DataFrame containing the original columns plus the new computed column.</returns>
        public static DataFrame AddColumn<T>(this DataFrame df, string newColumnName, Func<RowView, T> computer)
        {
            if (string.IsNullOrEmpty(newColumnName)) throw new ArgumentNullException(nameof(newColumnName));
            if (computer == null) throw new ArgumentNullException(nameof(computer));

            if (df.HasColumn(newColumnName))
            {
                throw new ArgumentException($"Column '{newColumnName}' already exists in DataFrame.");
            }

            Type typeT = typeof(T);
            Type underlying = Nullable.GetUnderlyingType(typeT) ?? typeT;
            bool isNullable = underlying != typeT || !typeT.IsValueType; // e.g. int? or string

            // 1. Create the Column (Non-Generic Factory)
            // Wir nutzen hier Create(Type), das dank deines Fixes Nullable Types auspacken kann.
            IColumn newCol = ColumnFactory.Create(newColumnName, typeT, df.RowCount, isNullable);

            // 2. Compute & Append loop (Type-Dispatcher)

            // Case A: Exact Match (e.g. T=int, Col=IColumn<int>)
            // Auch StringColumn f√§llt hierunter (T=string, Col=IColumn<string>)
            if (newCol is IColumn<T> typedCol)
            {
                for (int i = 0; i < df.RowCount; i++)
                {
                    var row = new RowView(i, df.Columns, df.Schema);
                    typedCol.Append(computer(row));
                }
            }
            // Case B: Nullable Primitives (e.g. T=int?, Col=IntColumn)
            // IntColumn implementiert NICHT IColumn<int?>, hat aber eine Methode Append(int?).
            // Wir nutzen Unsafe.As, um den Delegaten zu casten ohne Boxing.
            else if (newCol is IntColumn ic && typeT == typeof(int?))
            {
                var func = Unsafe.As<Func<RowView, T>, Func<RowView, int?>>(ref computer);
                for (int i = 0; i < df.RowCount; i++)
                    ic.Append(func(new RowView(i, df.Columns, df.Schema)));
            }
            else if (newCol is DoubleColumn dc && typeT == typeof(double?))
            {
                var func = Unsafe.As<Func<RowView, T>, Func<RowView, double?>>(ref computer);
                for (int i = 0; i < df.RowCount; i++)
                    dc.Append(func(new RowView(i, df.Columns, df.Schema)));
            }
            else if (newCol is BoolColumn bc && typeT == typeof(bool?))
            {
                var func = Unsafe.As<Func<RowView, T>, Func<RowView, bool?>>(ref computer);
                for (int i = 0; i < df.RowCount; i++)
                    bc.Append(func(new RowView(i, df.Columns, df.Schema)));
            }
            else if (newCol is DateTimeColumn dtc && typeT == typeof(DateTime?))
            {
                var func = Unsafe.As<Func<RowView, T>, Func<RowView, DateTime?>>(ref computer);
                for (int i = 0; i < df.RowCount; i++)
                    dtc.Append(func(new RowView(i, df.Columns, df.Schema)));
            }
            else
            {
                // Fallback (Safe but slower due to boxing)
                for (int i = 0; i < df.RowCount; i++)
                {
                    var row = new RowView(i, df.Columns, df.Schema);
                    object? val = computer(row);
                    newCol.AppendObject(val);
                }
            }

            // 3. Construct new DataFrame
            var newColumnList = new List<IColumn>(df.Columns);
            newColumnList.Add(newCol);

            return new DataFrame(newColumnList);
        }
    }
}
===== FILE: src/LeichtFrame.Core/Optimizer/IOptimizerRule.cs =====
using LeichtFrame.Core.Plans;

namespace LeichtFrame.Core.Optimizer
{
    /// <summary>
    /// Defines a rule for transforming a logical plan into an optimized version.
    /// </summary>
    public interface IOptimizerRule
    {
        /// <summary>
        /// Applies the rule to the given logical plan.
        /// </summary>
        /// <param name="plan">The input plan.</param>
        /// <returns>A transformed plan, or the original if no optimization applies.</returns>
        LogicalPlan Apply(LogicalPlan plan);
    }
}
===== FILE: src/LeichtFrame.Core/Optimizer/OptimizerEngine.cs =====
using LeichtFrame.Core.Plans;

namespace LeichtFrame.Core.Optimizer
{
    /// <summary>
    /// Coordinates the application of optimization rules to a logical plan.
    /// </summary>
    public class OptimizerEngine
    {
        private readonly List<IOptimizerRule> _rules;

        /// <summary>
        /// Initializes a new instance of the <see cref="OptimizerEngine"/> class with default rules.
        /// </summary>
        public OptimizerEngine()
        {
            _rules = new List<IOptimizerRule>
            {
                // Future rules: ConstantFolding, PredicatePushdown, etc.
            };
        }

        /// <summary>
        /// Optimizes the given logical plan by applying all registered rules.
        /// </summary>
        /// <param name="plan">The initial logical plan.</param>
        /// <returns>The optimized logical plan.</returns>
        public LogicalPlan Optimize(LogicalPlan plan)
        {
            var current = plan;
            foreach (var rule in _rules)
            {
                current = rule.Apply(current);
            }
            return current;
        }
    }
}
===== FILE: src/LeichtFrame.Core/Plans/LogicalPlan.cs =====
namespace LeichtFrame.Core.Plans
{
    /// <summary>
    /// Represents the abstract base class for a node in the logical query plan.
    /// </summary>
    public abstract record LogicalPlan
    {
        /// <summary>
        /// Gets the child nodes of this plan node.
        /// </summary>
        public abstract LogicalPlan[] Children { get; }

        /// <summary>
        /// Gets the schema produced by this plan node.
        /// </summary>
        public abstract DataFrameSchema OutputSchema { get; }
    }
}
===== FILE: src/LeichtFrame.Core/Plans/PlanNodes.cs =====
using LeichtFrame.Core.Expressions;

namespace LeichtFrame.Core.Plans
{
    /// <summary>
    /// Represents a leaf node in the plan that scans an existing DataFrame.
    /// </summary>
    /// <param name="Source">The source DataFrame.</param>
    public record DataFrameScan(DataFrame Source) : LogicalPlan
    {
        /// <inheritdoc/>
        public override LogicalPlan[] Children => Array.Empty<LogicalPlan>();

        /// <inheritdoc/>
        public override DataFrameSchema OutputSchema => Source.Schema;
    }

    /// <summary>
    /// Represents a filtering operation on the dataset.
    /// </summary>
    /// <param name="Input">The input plan node.</param>
    /// <param name="Predicate">The boolean expression used to filter rows.</param>
    public record Filter(LogicalPlan Input, Expr Predicate) : LogicalPlan
    {
        /// <inheritdoc/>
        public override LogicalPlan[] Children => new[] { Input };

        /// <inheritdoc/>
        public override DataFrameSchema OutputSchema => Input.OutputSchema;
    }

    /// <summary>
    /// Represents a projection (select) operation that transforms or selects columns.
    /// </summary>
    /// <param name="Input">The input plan node.</param>
    /// <param name="Expressions">The list of expressions to calculate.</param>
    public record Projection(LogicalPlan Input, List<Expr> Expressions) : LogicalPlan
    {
        /// <inheritdoc/>
        public override LogicalPlan[] Children => new[] { Input };

        /// <inheritdoc/>
        public override DataFrameSchema OutputSchema => Input.OutputSchema;
    }

    /// <summary>
    /// Represents a GroupBy operation followed by aggregations.
    /// </summary>
    public record Aggregate(LogicalPlan Input, List<Expr> GroupExprs, List<Expr> AggExprs) : LogicalPlan
    {
        /// <inheritdoc/>
        public override LogicalPlan[] Children => new[] { Input };

        /// <inheritdoc/>
        // Schema inference skipped for skeleton simplicity
        public override DataFrameSchema OutputSchema => Input.OutputSchema;
    }

    /// <summary>
    /// Represents a Join operation between two plans.
    /// </summary>
    public record Join(LogicalPlan Left, LogicalPlan Right, string LeftOn, string RightOn, JoinType JoinType) : LogicalPlan
    {
        /// <inheritdoc/>
        public override LogicalPlan[] Children => new[] { Left, Right };

        /// <inheritdoc/>
        public override DataFrameSchema OutputSchema => Left.OutputSchema; // Simplified
    }

    /// <summary>
    /// Represents a sorting operation (Multi-Column supported).
    /// </summary>
    public record Sort(LogicalPlan Input, List<(string Name, bool Ascending)> SortColumns) : LogicalPlan
    {
        /// <summary>
        /// Logical plan children
        /// </summary>
        public override LogicalPlan[] Children => new[] { Input };

        /// <summary>
        /// Logical plan output schema
        /// </summary>
        public override DataFrameSchema OutputSchema => Input.OutputSchema;
    }
}
===== FILE: src/LeichtFrame.Examples/LeichtFrame.Examples.csproj =====
Ôªø<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net8.0</TargetFramework>
    <ImplicitUsings>enable</ImplicitUsings>
    <Nullable>enable</Nullable>
    <IsPackable>false</IsPackable>
    <GenerateDocumentationFile>false</GenerateDocumentationFile>
    <NoWarn>$(NoWarn);CS1591</NoWarn>
  </PropertyGroup>

  <ItemGroup>
    <ProjectReference Include="..\LeichtFrame.Core\LeichtFrame.Core.csproj" />
    <ProjectReference Include="..\LeichtFrame.IO\LeichtFrame.IO.csproj" />
  </ItemGroup>

</Project>
===== FILE: src/LeichtFrame.Examples/Program.cs =====
Ôªøusing System.Text;
using LeichtFrame.Core;
using LeichtFrame.IO;
using LeichtFrame.Core.Operations.GroupBy;
using LeichtFrame.Core.Operations.Aggregate;

Console.WriteLine("=========================================================");
Console.WriteLine("   üöÄ LeichtFrame - End-to-End Data Pipeline Demo");
Console.WriteLine("=========================================================");

// ---------------------------------------------------------
// 1. SETUP: Simulate Messy Input Data (CSV)
// ---------------------------------------------------------
string rawCsvData =
@"TransactionId,Department,SalesAmount,IsRefund
1,Sales,50.00,false
2,IT,120.50,false
3,,0.00,false
4,Sales,300.00,true
5,HR,45.00,false
6,Sales,15.50,false
7,,90.00,false
8,IT,200.00,false";

Console.WriteLine("\n[1] Generating simulated CSV data stream...");
using var memoryStream = new MemoryStream(Encoding.UTF8.GetBytes(rawCsvData));

// ---------------------------------------------------------
// 2. DEFINE SCHEMA (The "Gold Standard" Way via POCO)
// ---------------------------------------------------------
// Instead of manually building ColumnDefinitions, we simply use a class.
// This ensures type safety and clean code.
// ---------------------------------------------------------
var df = CsvReader.Read<TransactionData>(memoryStream);

Console.WriteLine($"[2] Read CSV into DataFrame. Loaded {df.RowCount} rows.");
Console.WriteLine("    Raw Data Preview:");
Console.WriteLine(df.Inspect());

// ---------------------------------------------------------
// 3. CLEANING (Filter)
// ---------------------------------------------------------
Console.WriteLine("[3] Cleaning Data (Removing missing Departments & Refunds)...");

var cleanedDf = df.Where(row =>
{
    // High-performance access via generic Get<T>
    string dept = row.Get<string>("Department");
    bool isRefund = row.Get<bool>("IsRefund");

    // Keep only if Department exists AND it's not a refund
    return !string.IsNullOrEmpty(dept) && !isRefund;
});

Console.WriteLine($"    Cleaned Data: {cleanedDf.RowCount} rows remaining.");
Console.WriteLine(cleanedDf.Inspect());

// ---------------------------------------------------------
// 4. AGGREGATION (GroupBy & Sum)
// ---------------------------------------------------------
Console.WriteLine("[4] Aggregating: Total Sales by Department...");

var reportDf = cleanedDf.GroupBy("Department").Sum("SalesAmount");

Console.WriteLine("    Report Result:");
Console.WriteLine(reportDf.Inspect());

// ---------------------------------------------------------
// 5. EXPORT (Parquet)
// ---------------------------------------------------------
string outputPath = "sales_report.parquet";
Console.WriteLine($"[5] Exporting Report to Parquet: '{outputPath}'...");

if (File.Exists(outputPath)) File.Delete(outputPath);
reportDf.WriteParquet(outputPath);

Console.WriteLine("‚úÖ Done! Pipeline executed successfully.");
Console.WriteLine("=========================================================");

// ---------------------------------------------------------
// POCO Definition
// ---------------------------------------------------------
public class TransactionData
{
    public int TransactionId { get; set; }

    // Nullable because input data might have missing values (e.g., "3,,0.00")
    public string? Department { get; set; }

    public double SalesAmount { get; set; }
    public bool IsRefund { get; set; }
}
===== FILE: src/LeichtFrame.IO/Arrow/ArrowConverter.cs =====
using Apache.Arrow;
using Apache.Arrow.Types;
using LeichtFrame.Core;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides interoperability methods to convert between LeichtFrame <see cref="DataFrame"/> 
    /// and Apache Arrow <see cref="RecordBatch"/>.
    /// Enables integration with the broader data ecosystem (Spark, Python, etc.).
    /// </summary>
    public static class ArrowConverter
    {
        /// <summary>
        /// Converts an Apache Arrow RecordBatch into a LeichtFrame DataFrame.
        /// <para>
        /// **Note:** Currently performs a deep copy of the data.
        /// Zero-copy integration is planned for future releases.
        /// </para>
        /// </summary>
        /// <param name="batch">The source Apache Arrow RecordBatch.</param>
        /// <returns>A new <see cref="DataFrame"/> containing the data from the RecordBatch.</returns>
        /// <exception cref="ArgumentNullException">Thrown if the batch is null.</exception>
        /// <exception cref="NotSupportedException">Thrown if the Arrow data type is not supported by LeichtFrame.</exception>
        public static DataFrame ToDataFrame(RecordBatch batch)
        {
            if (batch == null) throw new ArgumentNullException(nameof(batch));

            var columns = new List<IColumn>(batch.ColumnCount);
            int rowCount = batch.Length;

            // Iterate over Arrow Arrays (Columns)
            foreach (var field in batch.Schema.FieldsList)
            {
                var arrowArray = batch.Column(field.Name);

                // We use the name from the schema
                string name = field.Name;

                // Conversion based on the Arrow type
                IColumn lfCol = ConvertArray(name, arrowArray, rowCount);
                columns.Add(lfCol);
            }

            return new DataFrame(columns);
        }

        /// <summary>
        /// Converts a LeichtFrame DataFrame into an Apache Arrow RecordBatch.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <returns>A new <see cref="RecordBatch"/> containing the data.</returns>
        /// <exception cref="ArgumentNullException">Thrown if the DataFrame is null.</exception>
        /// <exception cref="NotSupportedException">Thrown if a column type cannot be mapped to Arrow.</exception>
        public static RecordBatch ToRecordBatch(DataFrame df)
        {
            if (df == null) throw new ArgumentNullException(nameof(df));

            // 1. Build Arrow Schema
            var builder = new Schema.Builder();
            foreach (var col in df.Columns)
            {
                builder.Field(f => f.Name(col.Name).DataType(GetArrowType(col.DataType)).Nullable(col.IsNullable));
            }
            var arrowSchema = builder.Build();

            // 2. Build Arrow Arrays
            var arrowArrays = new List<IArrowArray>(df.ColumnCount);
            foreach (var col in df.Columns)
            {
                arrowArrays.Add(BuildArrowArray(col));
            }

            // 3. Create Batch
            return new RecordBatch(arrowSchema, arrowArrays, df.RowCount);
        }

        private static IArrowType GetArrowType(Type type)
        {
            if (type == typeof(int)) return Int32Type.Default;
            if (type == typeof(double)) return DoubleType.Default;
            if (type == typeof(bool)) return BooleanType.Default;
            if (type == typeof(string)) return StringType.Default;
            if (type == typeof(DateTime)) return TimestampType.Default;

            throw new NotSupportedException($"Type '{type.Name}' cannot be mapped to Arrow.");
        }

        private static IArrowArray BuildArrowArray(IColumn col)
        {
            if (col is IntColumn ic)
            {
                var builder = new Int32Array.Builder();
                for (int i = 0; i < ic.Length; i++)
                {
                    if (ic.IsNull(i)) builder.AppendNull();
                    else builder.Append(ic.Get(i));
                }
                return builder.Build();
            }

            if (col is DoubleColumn dc)
            {
                var builder = new DoubleArray.Builder();
                for (int i = 0; i < dc.Length; i++)
                {
                    if (dc.IsNull(i)) builder.AppendNull();
                    else builder.Append(dc.Get(i));
                }
                return builder.Build();
            }

            if (col is StringColumn sc)
            {
                var builder = new StringArray.Builder();
                for (int i = 0; i < sc.Length; i++)
                {
                    // StringColumn handles nulls internally in Get() usually, but checking IsNull is safer/consistent
                    if (sc.IsNull(i)) builder.AppendNull();
                    else builder.Append(sc.Get(i));
                }
                return builder.Build();
            }

            if (col is BoolColumn bc)
            {
                var builder = new BooleanArray.Builder();
                for (int i = 0; i < bc.Length; i++)
                {
                    if (bc.IsNull(i)) builder.AppendNull();
                    else builder.Append(bc.Get(i));
                }
                return builder.Build();
            }

            if (col is DateTimeColumn dtc)
            {
                var builder = new TimestampArray.Builder();
                for (int i = 0; i < dtc.Length; i++)
                {
                    if (dtc.IsNull(i)) builder.AppendNull();
                    else
                    {
                        // Arrow prefers DateTimeOffset usually, but creates TimestampArray from it.
                        builder.Append(new DateTimeOffset(dtc.Get(i)));
                    }
                }
                return builder.Build();
            }

            throw new NotSupportedException($"Column type '{col.GetType().Name}' is not supported for Arrow export.");
        }

        private static IColumn ConvertArray(string name, IArrowArray array, int length)
        {
            // 1. Int32
            if (array is Int32Array intArray)
            {
                var col = new IntColumn(name, length, isNullable: true); // Arrow is usually nullable
                for (int i = 0; i < length; i++)
                {
                    if (intArray.IsNull(i)) col.Append(null);
                    else col.Append(intArray.GetValue(i));
                }
                return col;
            }

            // 2. Double
            if (array is DoubleArray doubleArray)
            {
                var col = new DoubleColumn(name, length, isNullable: true);
                for (int i = 0; i < length; i++)
                {
                    if (doubleArray.IsNull(i)) col.Append(null);
                    else col.Append(doubleArray.GetValue(i));
                }
                return col;
            }

            // 3. String
            if (array is StringArray stringArray)
            {
                var col = new StringColumn(name, length, isNullable: true);
                for (int i = 0; i < length; i++)
                {
                    // GetString returns null if null
                    col.Append(stringArray.GetString(i));
                }
                return col;
            }

            // 4. Bool
            if (array is BooleanArray boolArray)
            {
                var col = new BoolColumn(name, length, isNullable: true);
                for (int i = 0; i < length; i++)
                {
                    if (boolArray.IsNull(i)) col.Append(null);
                    else col.Append(boolArray.GetValue(i));
                }
                return col;
            }

            // 5. Date/Timestamp (Arrow has many time types, we support basic Timestamp here)
            if (array is TimestampArray tsArray)
            {
                var col = new DateTimeColumn(name, length, isNullable: true);
                for (int i = 0; i < length; i++)
                {
                    if (tsArray.IsNull(i)) col.Append(null);
                    else
                    {
                        // Arrow Timestamp is usually DateTimeOffset, we take DateTime
                        col.Append(tsArray.GetTimestamp(i)?.DateTime);
                    }
                }
                return col;
            }

            // Fallback for Date32 (Common in Parquet/Arrow)
            if (array is Date32Array date32Array)
            {
                var col = new DateTimeColumn(name, length, isNullable: true);
                for (int i = 0; i < length; i++)
                {
                    if (date32Array.IsNull(i)) col.Append(null);
                    else col.Append(date32Array.GetDateTime(i));
                }
                return col;
            }

            throw new NotSupportedException($"Arrow array type '{array.GetType().Name}' is not supported yet.");
        }
    }
}
===== FILE: src/LeichtFrame.IO/Arrow/ArrowExtensions.cs =====
using Apache.Arrow;
using LeichtFrame.Core;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides extension methods for seamless integration with Apache Arrow.
    /// Allows converting <see cref="RecordBatch"/> to <see cref="DataFrame"/> and vice versa via fluent syntax.
    /// </summary>
    public static class ArrowExtensions
    {
        /// <summary>
        /// Converts an Apache Arrow <see cref="RecordBatch"/> directly into a LeichtFrame <see cref="DataFrame"/>.
        /// </summary>
        /// <param name="batch">The source Arrow RecordBatch.</param>
        /// <returns>A new DataFrame containing the data from the batch.</returns>
        /// <exception cref="ArgumentNullException">Thrown if the batch is null.</exception>
        public static DataFrame ToDataFrame(this RecordBatch batch)
        {
            return ArrowConverter.ToDataFrame(batch);
        }

        /// <summary>
        /// Converts the <see cref="DataFrame"/> into an Apache Arrow <see cref="RecordBatch"/>.
        /// Useful for passing data to other libraries like ML.NET, Spark, or Python (via interop).
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <returns>A new Arrow RecordBatch representing the DataFrame.</returns>
        /// <exception cref="ArgumentNullException">Thrown if the DataFrame is null.</exception>
        public static RecordBatch ToArrow(this DataFrame df)
        {
            return ArrowConverter.ToRecordBatch(df);
        }
    }
}
===== FILE: src/LeichtFrame.IO/Csv/CsvReadOptions.cs =====
using System.Globalization;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Configuration options for reading CSV files.
    /// </summary>
    public class CsvReadOptions
    {
        /// <summary>
        /// Gets or sets the delimiter used to separate fields. Default is ",".
        /// </summary>
        public string Separator { get; set; } = ",";

        /// <summary>
        /// Gets or sets a value indicating whether the first row of the CSV contains column headers. 
        /// Default is <c>true</c>.
        /// </summary>
        public bool HasHeader { get; set; } = true;

        /// <summary>
        /// Gets or sets the culture information used to parse numbers and dates. 
        /// Default is <see cref="CultureInfo.InvariantCulture"/> (dot decimal separator).
        /// </summary>
        public CultureInfo Culture { get; set; } = CultureInfo.InvariantCulture;

        /// <summary>
        /// Gets or sets a specific date format string (e.g. "yyyy-MM-dd").
        /// If null (default), the parser attempts to detect the format automatically based on the Culture.
        /// </summary>
        public string? DateFormat { get; set; } = null;
        /// <summary>
        /// Gets or sets a value indicating whether lines have a trailing delimiter. 
        /// Default is <c>false</c>.
        /// </summary>
        public bool HasTrailingDelimiter { get; set; } = false;
    }
}
===== FILE: src/LeichtFrame.IO/Csv/CsvReader.cs =====
using System.Buffers;
using System.Globalization;
using System.Text;
using LeichtFrame.Core;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides functionality to read CSV files into DataFrames.
    /// </summary>
    public static class CsvReader
    {
        private static readonly ArrayPool<object?> _objPool = ArrayPool<object?>.Shared;

        // =======================================================================
        // STANDARD READ METHODS
        // =======================================================================

        /// <summary>
        /// Reads a CSV file from the specified path into a DataFrame using the provided schema and options.
        /// </summary>
        /// <param name="path"></param>
        /// <param name="schema"></param>
        /// <param name="options"></param>
        /// <returns></returns>
        public static DataFrame Read(string path, DataFrameSchema schema, CsvReadOptions? options = null)
        {
            options ??= new CsvReadOptions();
            using var stream = new FileStream(path, FileMode.Open, FileAccess.Read, FileShare.Read, 65536);
            return Read(stream, schema, options);
        }

        /// <summary>
        /// Reads a CSV from the provided stream into a DataFrame using the specified schema and options.
        /// </summary>
        /// <param name="stream"></param>
        /// <param name="schema"></param>
        /// <param name="options"></param>
        /// <returns></returns>
        public static DataFrame Read(Stream stream, DataFrameSchema schema, CsvReadOptions? options = null)
        {
            options ??= new CsvReadOptions();
            using var reader = new StreamReader(stream, Encoding.UTF8, true, 65536, leaveOpen: true);

            var df = DataFrame.Create(schema, 10000);
            int colCount = df.ColumnCount;
            var columns = new IColumn[colCount];
            var colTypes = new Type[colCount];
            var sourceIndices = new int[colCount];

            for (int i = 0; i < colCount; i++)
            {
                columns[i] = df.Columns[i];
                colTypes[i] = df.Columns[i].DataType;
                sourceIndices[i] = schema.Columns[i].SourceIndex ?? i;
            }

            if (options.HasHeader && !reader.EndOfStream) reader.ReadLine();

            char separator = options.Separator[0];
            int chunkSize = 50_000;
            var lineBuffer = new List<string>(chunkSize);

            while (!reader.EndOfStream)
            {
                string? line = reader.ReadLine();
                if (line == null) break;
                lineBuffer.Add(line);

                if (lineBuffer.Count >= chunkSize)
                {
                    ProcessChunkParallel(lineBuffer, columns, colTypes, sourceIndices, separator, options);
                    lineBuffer.Clear();
                }
            }

            if (lineBuffer.Count > 0) ProcessChunkParallel(lineBuffer, columns, colTypes, sourceIndices, separator, options);
            return df;
        }

        /// <summary>
        /// Reads a CSV file from the specified path into a DataFrame by inferring the schema from the data.
        /// </summary>
        /// <typeparam name="T"></typeparam>
        /// <param name="path"></param>
        /// <param name="options"></param>
        /// <returns></returns>
        public static DataFrame Read<T>(string path, CsvReadOptions? options = null)
        {
            return Read(path, DataFrameSchema.FromType<T>(), options);
        }

        /// <summary>
        /// Reads a CSV from the provided stream into a DataFrame by inferring the schema from the data.
        /// </summary>
        /// <typeparam name="T"></typeparam>
        /// <param name="stream"></param>
        /// <param name="options"></param>
        /// <returns></returns>
        public static DataFrame Read<T>(Stream stream, CsvReadOptions? options = null)
        {
            return Read(stream, DataFrameSchema.FromType<T>(), options);
        }

        // =======================================================================
        // BATCHED READ METHODS (Streaming)
        // =======================================================================

        /// <summary>
        /// Reads a CSV file from the specified path into multiple DataFrame batches using the provided schema and options.
        /// </summary>
        /// <param name="path"></param>
        /// <param name="schema"></param>
        /// <param name="batchSize"></param>
        /// <param name="options"></param>
        /// <returns></returns>
        public static IEnumerable<DataFrame> ReadBatches(string path, DataFrameSchema schema, int batchSize = 1000, CsvReadOptions? options = null)
        {
            using var stream = new FileStream(path, FileMode.Open, FileAccess.Read, FileShare.Read, 65536);
            foreach (var batch in ReadBatches(stream, schema, batchSize, options)) yield return batch;
        }

        /// <summary>
        /// Reads a CSV from the provided stream into multiple DataFrame batches using the specified schema and options.
        /// </summary>
        /// <param name="stream"></param>
        /// <param name="schema"></param>
        /// <param name="batchSize"></param>
        /// <param name="options"></param>
        /// <returns></returns>
        public static IEnumerable<DataFrame> ReadBatches(Stream stream, DataFrameSchema schema, int batchSize, CsvReadOptions? options = null)
        {
            options ??= new CsvReadOptions();
            using var reader = new StreamReader(stream, Encoding.UTF8, true, 65536, leaveOpen: true);
            if (options.HasHeader && !reader.EndOfStream) reader.ReadLine();

            char separator = options.Separator[0];
            int colCount = schema.Columns.Count;
            int[] sourceIndices = schema.Columns.Select((c, i) => c.SourceIndex ?? i).ToArray();

            while (!reader.EndOfStream)
            {
                var batchDf = DataFrame.Create(schema, batchSize);
                for (int r = 0; r < batchSize && !reader.EndOfStream; r++)
                {
                    string? line = reader.ReadLine();
                    if (line == null) break;

                    ReadOnlySpan<char> lineSpan = line.AsSpan();
                    for (int i = 0; i < colCount; i++)
                    {
                        var field = GetFieldAt(lineSpan, separator, sourceIndices[i]);
                        batchDf[i].AppendObject(ParseValueFromSpan(schema.Columns[i].DataType, field, options));
                    }
                }
                if (batchDf.RowCount > 0) yield return batchDf;
            }
        }

        // =======================================================================
        // SCHEMA INFERENCE
        // =======================================================================

        /// <summary>
        /// Infers the schema of a CSV file by sampling a specified number of rows.
        /// </summary>
        /// <param name="path"></param>
        /// <param name="options"></param>
        /// <param name="sampleRows"></param>
        /// <returns></returns>
        /// <exception cref="IOException"></exception>
        public static DataFrameSchema InferSchema(string path, CsvReadOptions? options = null, int sampleRows = 100)
        {
            options ??= new CsvReadOptions();
            using var reader = new StreamReader(path, Encoding.UTF8, true, 65536);

            string? firstLine = reader.ReadLine();
            if (firstLine == null) throw new IOException("File is empty.");

            char sep = options.Separator[0];
            string[] headers;
            if (options.HasHeader)
            {
                headers = firstLine.Split(sep).Select(h => h.Trim('\"', ' ')).ToArray();
            }
            else
            {
                headers = firstLine.Split(sep).Select((_, i) => $"Column{i}").ToArray();
                reader.BaseStream.Seek(0, SeekOrigin.Begin);
                reader.DiscardBufferedData();
            }

            var colTypes = new Type?[headers.Length];
            var colNullable = new bool[headers.Length];

            for (int r = 0; r < sampleRows && !reader.EndOfStream; r++)
            {
                string? line = reader.ReadLine();
                if (line == null) break;
                if (string.IsNullOrWhiteSpace(line))
                {
                    for (int i = 0; i < headers.Length; i++) colNullable[i] = true;
                    continue;
                }

                ReadOnlySpan<char> lineSpan = line.AsSpan();
                for (int i = 0; i < headers.Length; i++)
                {
                    var field = GetFieldAt(lineSpan, sep, i);
                    if (field.IsEmpty || field.IsWhiteSpace())
                    {
                        colNullable[i] = true;
                        continue;
                    }
                    Type detected = DetectTypeFromSpan(field, options);
                    colTypes[i] = MergeTypes(colTypes[i], detected);
                }
            }

            return new DataFrameSchema(headers.Select((h, i) =>
                new ColumnDefinition(h, colTypes[i] ?? typeof(string), colNullable[i], SourceIndex: i)).ToList());
        }

        // =======================================================================
        // INTERNAL HELPERS
        // =======================================================================

        private static void ProcessChunkParallel(List<string> lines, IColumn[] columns, Type[] types, int[] sourceIndices, char separator, CsvReadOptions options)
        {
            int rowCount = lines.Count;
            int colCount = columns.Length;
            object?[] results = _objPool.Rent(rowCount * colCount);

            Parallel.For(0, rowCount, r =>
            {
                ReadOnlySpan<char> lineSpan = lines[r].AsSpan();
                for (int c = 0; c < colCount; c++)
                {
                    var field = GetFieldAt(lineSpan, separator, sourceIndices[c]);
                    results[r * colCount + c] = ParseValueFromSpan(types[c], field, options);
                }
            });

            for (int r = 0; r < rowCount; r++)
                for (int c = 0; c < colCount; c++)
                    columns[c].AppendObject(results[r * colCount + c]);

            _objPool.Return(results);
        }

        private static ReadOnlySpan<char> GetFieldAt(ReadOnlySpan<char> line, char separator, int targetIndex)
        {
            int currentIdx = 0;
            int start = 0;
            bool inQuotes = false;

            for (int i = 0; i < line.Length; i++)
            {
                if (line[i] == '\"') inQuotes = !inQuotes;
                else if (line[i] == separator && !inQuotes)
                {
                    if (currentIdx == targetIndex) return Unescape(line.Slice(start, i - start));
                    start = i + 1;
                    currentIdx++;
                }
            }
            return (currentIdx == targetIndex) ? Unescape(line.Slice(start)) : ReadOnlySpan<char>.Empty;
        }

        private static ReadOnlySpan<char> Unescape(ReadOnlySpan<char> s)
        {
            s = s.Trim();
            return (s.Length >= 2 && s[0] == '\"' && s[^1] == '\"') ? s.Slice(1, s.Length - 2) : s;
        }

        private static object? ParseValueFromSpan(Type type, ReadOnlySpan<char> span, CsvReadOptions options)
        {
            if (span.IsEmpty || span.IsWhiteSpace()) return null;
            if (type == typeof(int) && int.TryParse(span, NumberStyles.Integer, options.Culture, out int i)) return i;
            if (type == typeof(double) && double.TryParse(span, NumberStyles.Float | NumberStyles.AllowThousands, options.Culture, out double d)) return d;
            if (type == typeof(DateTime) && DateTime.TryParse(span, options.Culture, DateTimeStyles.None, out DateTime dt)) return dt;
            if (type == typeof(bool) && bool.TryParse(span, out bool b)) return b;
            return (type == typeof(string)) ? span.ToString() : null;
        }

        private static Type DetectTypeFromSpan(ReadOnlySpan<char> span, CsvReadOptions options)
        {
            if (int.TryParse(span, NumberStyles.Integer, options.Culture, out _)) return typeof(int);
            if (double.TryParse(span, NumberStyles.Float | NumberStyles.AllowThousands, options.Culture, out _)) return typeof(double);
            if (DateTime.TryParse(span, options.Culture, DateTimeStyles.None, out _)) return typeof(DateTime);
            if (bool.TryParse(span, out _)) return typeof(bool);
            return typeof(string);
        }

        private static Type MergeTypes(Type? current, Type newType)
        {
            if (current == null || current == newType) return newType;
            if ((current == typeof(int) && newType == typeof(double)) || (current == typeof(double) && newType == typeof(int))) return typeof(double);
            return typeof(string);
        }
    }
}
===== FILE: src/LeichtFrame.IO/Csv/CsvWriteOptions.cs =====
using System.Globalization;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Configuration options for writing CSV files.
    /// </summary>
    public class CsvWriteOptions
    {
        /// <summary>
        /// Gets or sets the delimiter used to separate fields. Default is ",".
        /// </summary>
        public string Separator { get; set; } = ",";

        /// <summary>
        /// Gets or sets a value indicating whether to write column names as the first row. 
        /// Default is <c>true</c>.
        /// </summary>
        public bool WriteHeader { get; set; } = true;

        /// <summary>
        /// Gets or sets the culture information used to format numbers and dates.
        /// Default is <see cref="CultureInfo.InvariantCulture"/> (dot decimal separator) to ensure compatibility.
        /// </summary>
        public CultureInfo Culture { get; set; } = CultureInfo.InvariantCulture;

        /// <summary>
        /// Gets or sets the format string for <see cref="DateTime"/> values.
        /// Default is "o" (ISO 8601 round-trip pattern), which is the safest standard for machine processing.
        /// </summary>
        public string DateFormat { get; set; } = "o";

        /// <summary>
        /// Gets or sets the string representation for null values. 
        /// Default is an empty string.
        /// </summary>
        public string NullValue { get; set; } = "";
    }
}
===== FILE: src/LeichtFrame.IO/Csv/CsvWriter.cs =====
using System.Text;
using LeichtFrame.Core;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides methods for writing <see cref="DataFrame"/> content to CSV format.
    /// Handles proper escaping (RFC 4180) and formatting based on configurable options.
    /// </summary>
    public static class CsvWriter
    {
        /// <summary>
        /// Writes the DataFrame to a CSV file at the specified path.
        /// Overwrites the file if it already exists.
        /// </summary>
        /// <param name="df">The DataFrame to write.</param>
        /// <param name="path">The full file path.</param>
        /// <param name="options">Optional formatting options (separator, date format, etc.).</param>
        public static void Write(DataFrame df, string path, CsvWriteOptions? options = null)
        {
            // File.Create overrides existing files
            using var stream = File.Create(path);
            Write(df, stream, options);
        }

        /// <summary>
        /// Writes the DataFrame to a stream in CSV format.
        /// </summary>
        /// <param name="df">The DataFrame to write.</param>
        /// <param name="stream">The output stream (must be writable).</param>
        /// <param name="options">Optional formatting options.</param>
        public static void Write(DataFrame df, Stream stream, CsvWriteOptions? options = null)
        {
            options ??= new CsvWriteOptions();

            // UTF8 without BOM is the standard nowadays, let the stream decide or enforce it
            using var writer = new StreamWriter(stream, new UTF8Encoding(false), 1024, leaveOpen: true);

            // 1. Write Header
            if (options.WriteHeader)
            {
                // Uses the API from B.5.2
                var headers = df.GetColumnNames();
                writer.WriteLine(string.Join(options.Separator, headers));
            }

            // 2. Write Rows
            var sb = new StringBuilder();

            for (int i = 0; i < df.RowCount; i++)
            {
                sb.Clear();
                for (int c = 0; c < df.ColumnCount; c++)
                {
                    if (c > 0) sb.Append(options.Separator);

                    var col = df[c];
                    // Untyped access (GetValue) is okay here since IO is the bottleneck anyway
                    object? val = col.GetValue(i);

                    string valStr = FormatValue(val, options);

                    // CSV Escaping (RFC 4180): 
                    // If separator, quote, or newline are present -> enclose text in quotes
                    if (NeedsEscaping(valStr, options.Separator))
                    {
                        // Escape double quotes (" -> "")
                        valStr = "\"" + valStr.Replace("\"", "\"\"") + "\"";
                    }

                    sb.Append(valStr);
                }
                writer.WriteLine(sb.ToString());
            }

            writer.Flush();
        }

        private static bool NeedsEscaping(string val, string separator)
        {
            // Performance check: Contains any of the critical characters?
            return val.Contains(separator) || val.Contains("\"") || val.Contains("\n") || val.Contains("\r");
        }

        private static string FormatValue(object? val, CsvWriteOptions options)
        {
            if (val == null) return options.NullValue;

            if (val is DateTime dt)
            {
                return dt.ToString(options.DateFormat, options.Culture);
            }
            if (val is bool b)
            {
                // Lowercase (true/false) or C# standard (True/False)? 
                // C# ToString() produces "True", JSON/JS prefers "true". We stick to C# standard for consistency.
                return b.ToString(options.Culture);
            }
            if (val is IFormattable formattable)
            {
                return formattable.ToString(null, options.Culture);
            }

            return val.ToString() ?? "";
        }
    }
}
===== FILE: src/LeichtFrame.IO/Csv/DataFrameCsvExtensions.cs =====
using System.Globalization;
using System.Text;
using LeichtFrame.Core;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides extension methods for importing and exporting <see cref="DataFrame"/> objects via CSV.
    /// </summary>
    public static class DataFrameCsvExtensions
    {
        // =========================================================
        // WRITE EXTENSIONS (Export)
        // =========================================================

        /// <summary>
        /// Writes the DataFrame to a CSV file at the specified path.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="path">The full path to the output file. Will be overwritten if it exists.</param>
        /// <param name="options">Optional configuration for writing (separator, date format, etc.).</param>
        public static void WriteCsv(this DataFrame df, string path, CsvWriteOptions? options = null)
        {
            CsvWriter.Write(df, path, options);
        }

        /// <summary>
        /// Writes the DataFrame to a stream in CSV format.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="stream">The output stream.</param>
        /// <param name="options">Optional configuration for writing.</param>
        public static void WriteCsv(this DataFrame df, Stream stream, CsvWriteOptions? options = null)
        {
            CsvWriter.Write(df, stream, options);
        }

        // =========================================================
        // READ EXTENSIONS (Import)
        // =========================================================

        /// <summary>
        /// Reads a CSV file from a given path using a specific schema.
        /// </summary>
        /// <param name="path">The file path to the CSV.</param>
        /// <param name="schema">The schema definition describing column names and types.</param>
        /// <param name="hasHeader">Indicates if the first row contains column headers.</param>
        /// <param name="separator">The character used to separate fields.</param>
        /// <returns>A populated <see cref="DataFrame"/>.</returns>
        public static DataFrame ReadCsv(string path, DataFrameSchema schema, bool hasHeader = true, char separator = ',')
        {
            using var stream = File.OpenRead(path);
            return ReadCsv(stream, schema, hasHeader, separator);
        }

        /// <summary>
        /// Reads a CSV from a stream using a specific schema.
        /// </summary>
        /// <param name="stream">The input stream containing CSV data.</param>
        /// <param name="schema">The schema definition describing column names and types.</param>
        /// <param name="hasHeader">Indicates if the first row contains column headers.</param>
        /// <param name="separator">The character used to separate fields.</param>
        /// <returns>A populated <see cref="DataFrame"/>.</returns>
        /// <exception cref="ArgumentNullException">Thrown if stream or schema is null.</exception>
        public static DataFrame ReadCsv(Stream stream, DataFrameSchema schema, bool hasHeader = true, char separator = ',')
        {
            if (stream == null) throw new ArgumentNullException(nameof(stream));
            if (schema == null) throw new ArgumentNullException(nameof(schema));

            // Estimate row count roughly to minimize resizing (Performance optimization).
            // Assumption: approx. 100 bytes per row. Better than starting at capacity 0.
            int estimatedRows = (int)(stream.Length / 100);
            if (estimatedRows < 16) estimatedRows = 16;

            // 1. Create DataFrame (with schema and estimated capacity).
            var df = DataFrame.Create(schema, estimatedRows);

            using var reader = new StreamReader(stream, Encoding.UTF8, detectEncodingFromByteOrderMarks: true, bufferSize: 65536);

            // 2. Skip Header if present
            if (hasHeader && !reader.EndOfStream)
            {
                reader.ReadLine();
            }

            // Cache columns to avoid dictionary lookup per row
            var columns = df.Columns;
            int colCount = columns.Count;

            // 3. Read and parse lines
            while (!reader.EndOfStream)
            {
                var line = reader.ReadLine();
                if (string.IsNullOrWhiteSpace(line)) continue;

                var parts = line.Split(separator);

                // Strict schema check: Ignore lines with fewer columns than schema
                if (parts.Length < colCount) continue;

                for (int i = 0; i < colCount; i++)
                {
                    string rawValue = parts[i];
                    var col = columns[i];

                    try
                    {
                        ParseAndAppend(col, rawValue);
                    }
                    catch
                    {
                        // Fallback on parse error: Append null or default
                        if (col.IsNullable)
                        {
                            col.AppendObject(null);
                        }
                        else
                        {
                            col.AppendObject(GetDefault(col.DataType));
                        }
                    }
                }
            }

            return df;
        }

        // =========================================================
        // INTERNAL HELPERS
        // =========================================================

        private static void ParseAndAppend(IColumn col, string rawValue)
        {
            Type targetType = col.DataType;

            // Handle Nulls
            if (string.IsNullOrEmpty(rawValue))
            {
                if (col.IsNullable)
                {
                    col.AppendObject(null);
                    return;
                }
            }

            // Parsing Logic (Always use InvariantCulture for data interchange!)
            if (targetType == typeof(int))
            {
                if (int.TryParse(rawValue, NumberStyles.Any, CultureInfo.InvariantCulture, out int result))
                    col.AppendObject(result);
                else
                    col.AppendObject(GetDefault(typeof(int))); // Fallback
            }
            else if (targetType == typeof(double))
            {
                if (double.TryParse(rawValue, NumberStyles.Any, CultureInfo.InvariantCulture, out double result))
                    col.AppendObject(result);
                else
                    col.AppendObject(GetDefault(typeof(double)));
            }
            else if (targetType == typeof(bool))
            {
                if (bool.TryParse(rawValue, out bool result))
                    col.AppendObject(result);
                else
                    col.AppendObject(false);
            }
            else if (targetType == typeof(string))
            {
                col.AppendObject(rawValue);
            }
            else if (targetType == typeof(DateTime))
            {
                if (DateTime.TryParse(rawValue, CultureInfo.InvariantCulture, DateTimeStyles.None, out DateTime result))
                    col.AppendObject(result);
                else
                    col.AppendObject(GetDefault(typeof(DateTime)));
            }
            else
            {
                // General Fallback
                try
                {
                    col.AppendObject(Convert.ChangeType(rawValue, targetType, CultureInfo.InvariantCulture));
                }
                catch
                {
                    col.AppendObject(GetDefault(targetType));
                }
            }
        }

        private static object? GetDefault(Type type)
        {
            if (type.IsValueType)
            {
                return Activator.CreateInstance(type);
            }
            return null;
        }
    }
}
===== FILE: src/LeichtFrame.IO/LeichtFrame.IO.csproj =====
Ôªø<Project Sdk="Microsoft.NET.Sdk">

  <ItemGroup>
    <ProjectReference Include="..\LeichtFrame.Core\LeichtFrame.Core.csproj" />
  </ItemGroup>

  <ItemGroup>
    <PackageReference Include="Apache.Arrow" Version="22.1.0" />
    <PackageReference Include="Parquet.Net" Version="5.4.0" />
  </ItemGroup>

  <PropertyGroup>
    <TargetFramework>net8.0</TargetFramework>
    <ImplicitUsings>enable</ImplicitUsings>
    <Nullable>enable</Nullable>
  </PropertyGroup>

</Project>

===== FILE: src/LeichtFrame.IO/Parquet/DataFrameParquetExtensions.cs =====
using LeichtFrame.Core;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides extension methods for exporting <see cref="DataFrame"/> objects to Apache Parquet format.
    /// </summary>
    public static class DataFrameParquetExtensions
    {
        /// <summary>
        /// Writes the DataFrame to a Parquet file at the specified path.
        /// </summary>
        /// <param name="df">The source DataFrame to export.</param>
        /// <param name="path">The file path where the Parquet file will be created or overwritten.</param>
        public static void WriteParquet(this DataFrame df, string path)
        {
            ParquetWriter.Write(df, path);
        }

        /// <summary>
        /// Writes the DataFrame to a stream in Parquet format.
        /// </summary>
        /// <param name="df">The source DataFrame to export.</param>
        /// <param name="stream">The writable output stream.</param>
        public static void WriteParquet(this DataFrame df, Stream stream)
        {
            ParquetWriter.Write(df, stream);
        }

        /// <summary>
        /// Writes the DataFrame to a stream asynchronously in Parquet format.
        /// Recommended for Web APIs to avoid blocking threads during I/O.
        /// </summary>
        /// <param name="df">The source DataFrame to export.</param>
        /// <param name="stream">The writable output stream.</param>
        /// <returns>A task that represents the asynchronous write operation.</returns>
        public static Task WriteParquetAsync(this DataFrame df, Stream stream)
        {
            return ParquetWriter.WriteAsync(df, stream);
        }
    }
}
===== FILE: src/LeichtFrame.IO/Parquet/ParquetReader.cs =====
using LeichtFrame.Core;
using Parquet.Schema;
using Parquet;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides high-performance methods to read Apache Parquet files into a <see cref="DataFrame"/>.
    /// Automatically maps Parquet schema types to LeichtFrame column types.
    /// Supports both full-load and batched streaming (RowGroup-based).
    /// </summary>
    public static class ParquetReader
    {
        // =======================================================================
        // STANDARD READ METHODS (Full Load)
        // =======================================================================

        /// <summary>
        /// Reads a Parquet file from the specified file path.
        /// </summary>
        /// <param name="path">The full path to the Parquet file.</param>
        /// <returns>A populated <see cref="DataFrame"/> containing all data.</returns>
        public static DataFrame Read(string path)
        {
            using var stream = File.OpenRead(path);
            return Read(stream);
        }

        /// <summary>
        /// Reads a Parquet file from a stream synchronously.
        /// </summary>
        /// <param name="stream">The input stream containing Parquet data.</param>
        /// <returns>A populated <see cref="DataFrame"/>.</returns>
        public static DataFrame Read(Stream stream)
        {
            // Synchronous Wrapper for the Async method
            return ReadAsync(stream).GetAwaiter().GetResult();
        }

        /// <summary>
        /// Reads a Parquet file from a stream asynchronously.
        /// Recommended for I/O-bound operations in Web APIs to avoid blocking threads.
        /// </summary>
        /// <param name="stream">The input stream containing Parquet data.</param>
        /// <returns>A task that represents the asynchronous read operation, containing the resulting <see cref="DataFrame"/>.</returns>
        public static async Task<DataFrame> ReadAsync(Stream stream)
        {
            using var reader = await Parquet.ParquetReader.CreateAsync(stream);

            // 1. Schema Mapping (Parquet -> LeichtFrame)
            var dataFields = reader.Schema.GetDataFields();
            var colDefs = dataFields.Select(f => MapToColumnDefinition(f));
            var schema = new DataFrameSchema(colDefs);

            // 2. Create DataFrame
            // We start with a capacity estimate. Parquet has metadata for total rows, but reader.ThriftMetadata might be internal.
            // Safe default.
            var df = DataFrame.Create(schema, capacity: 1000);

            // 3. Read Data (RowGroup by RowGroup)
            for (int i = 0; i < reader.RowGroupCount; i++)
            {
                using var groupReader = reader.OpenRowGroupReader(i);

                foreach (var field in dataFields)
                {
                    var column = df[field.Name];

                    // Reads the entire column of this RowGroup as an array
                    var parquetColumn = await groupReader.ReadColumnAsync(field);

                    // 4. Copy Data to LeichtFrame Column
                    AppendData(column, parquetColumn.Data);
                }
            }

            return df;
        }

        // =======================================================================
        // BATCHED READ METHODS (Streaming)
        // =======================================================================

        /// <summary>
        /// Reads a Parquet file in batches, mapping 1 Parquet RowGroup to 1 DataFrame.
        /// This allows processing files larger than available memory.
        /// </summary>
        /// <param name="path">The file path.</param>
        /// <returns>An enumerable of DataFrames.</returns>
        public static IEnumerable<DataFrame> ReadBatches(string path)
        {
            using var stream = File.OpenRead(path);
            foreach (var batch in ReadBatches(stream))
            {
                yield return batch;
            }
        }

        /// <summary>
        /// Reads Parquet batches from a stream synchronously.
        /// Note: This performs blocking calls on the underlying async Parquet library.
        /// </summary>
        /// <param name="stream">The input stream.</param>
        /// <returns>An enumerable of DataFrames.</returns>
        public static IEnumerable<DataFrame> ReadBatches(Stream stream)
        {
            // 1. Open Reader (Blocking wait)
            var task = Parquet.ParquetReader.CreateAsync(stream);
            task.Wait();
            using var reader = task.Result;

            // 2. Schema Mapping
            var dataFields = reader.Schema.GetDataFields();
            var colDefs = dataFields.Select(f => MapToColumnDefinition(f));
            var schema = new DataFrameSchema(colDefs);

            // 3. Iterate RowGroups
            for (int i = 0; i < reader.RowGroupCount; i++)
            {
                using var groupReader = reader.OpenRowGroupReader(i);

                // If RowGroup is empty, we skip or produce empty DF. Parquet usually doesn't store empty groups.
                long groupRowCount = groupReader.RowCount;

                // Create a fresh DataFrame for this batch
                var batchDf = DataFrame.Create(schema, (int)groupRowCount);

                // Read all columns for this group
                foreach (var field in dataFields)
                {
                    var column = batchDf[field.Name];

                    // Read Column Data (Blocking wait)
                    var readTask = groupReader.ReadColumnAsync(field);
                    readTask.Wait();
                    var parquetColumn = readTask.Result;

                    AppendData(column, parquetColumn.Data);
                }

                yield return batchDf;
            }
        }

        // =======================================================================
        // INTERNAL HELPERS
        // =======================================================================

        private static ColumnDefinition MapToColumnDefinition(DataField field)
        {
            // Mapping Parquet Types -> .NET Types
            Type targetType = field.ClrNullableIfHasNullsType;

            // We want the core type for LeichtFrame (int instead of int?) + IsNullable flag
            Type coreType = Nullable.GetUnderlyingType(targetType) ?? targetType;
            bool isNullable = field.IsNullable || targetType.IsGenericType; // rough rule

            // Support Check
            if (coreType != typeof(int) && coreType != typeof(double) &&
                coreType != typeof(string) && coreType != typeof(bool) &&
                coreType != typeof(DateTime))
            {
                // Fallback for types not strictly typed in our system
                throw new NotSupportedException($"Parquet type '{coreType.Name}' for column '{field.Name}' is not supported yet.");
            }

            return new ColumnDefinition(field.Name, coreType, isNullable);
        }

        private static void AppendData(IColumn col, Array data)
        {
            // The array from Parquet.Net is typed (e.g., int[] or int?[])
            // We iterate and append.

            if (col is IntColumn ic)
            {
                foreach (var item in data) ic.Append((int?)item);
            }
            else if (col is DoubleColumn dc)
            {
                foreach (var item in data) dc.Append((double?)item);
            }
            else if (col is StringColumn sc)
            {
                foreach (var item in data) sc.Append((string?)item);
            }
            else if (col is BoolColumn bc)
            {
                foreach (var item in data) bc.Append((bool?)item);
            }
            else if (col is DateTimeColumn dtc)
            {
                foreach (var item in data) dtc.Append((DateTime?)item);
            }
        }
    }
}
===== FILE: src/LeichtFrame.IO/Parquet/ParquetWriter.cs =====
using LeichtFrame.Core;
using Parquet.Data;
using Parquet.Schema;

namespace LeichtFrame.IO
{
    /// <summary>
    /// Provides methods for writing <see cref="DataFrame"/> objects into Apache Parquet format.
    /// Handles schema mapping and efficient data conversion for storage.
    /// </summary>
    public static class ParquetWriter
    {
        /// <summary>
        /// Writes the DataFrame to a Parquet file at the specified path.
        /// If the file exists, it will be overwritten.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="path">The output file path.</param>
        public static void Write(DataFrame df, string path)
        {
            // Allow overwrite
            using var stream = File.Create(path);
            Write(df, stream);
        }

        /// <summary>
        /// Writes the DataFrame to a stream in Parquet format synchronously.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="stream">The writable output stream.</param>
        public static void Write(DataFrame df, Stream stream)
        {
            // Synchronous Wrapper
            WriteAsync(df, stream).GetAwaiter().GetResult();
        }

        /// <summary>
        /// Writes the DataFrame to a stream in Parquet format asynchronously.
        /// </summary>
        /// <param name="df">The source DataFrame.</param>
        /// <param name="stream">The writable output stream.</param>
        /// <returns>A task representing the asynchronous write operation.</returns>
        public static async Task WriteAsync(DataFrame df, Stream stream)
        {
            // 1. Schema Mapping (LeichtFrame -> Parquet)
            var dataFields = df.Schema.Columns.Select(MapToDataField).ToArray();
            var parquetSchema = new ParquetSchema(dataFields);

            // 2. Writer Setup
            using var writer = await Parquet.ParquetWriter.CreateAsync(parquetSchema, stream);

            // We write everything in one RowGroup (simplest solution for MVP)
            using var groupWriter = writer.CreateRowGroup();

            // 3. Column Data Conversion & Write
            for (int i = 0; i < df.ColumnCount; i++)
            {
                var col = df.Columns[i];
                var field = dataFields[i];

                // Data conversion (NullBitmap -> Nullable Array)
                Array data = ConvertToParquetArray(col);

                var dataColumn = new DataColumn(field, data);
                await groupWriter.WriteColumnAsync(dataColumn);
            }
        }

        private static DataField MapToDataField(ColumnDefinition def)
        {
            // Int -> Int32, Nullable handling via Type?
            Type t = def.DataType;
            if (def.IsNullable && t.IsValueType)
            {
                t = typeof(Nullable<>).MakeGenericType(t);
            }
            return new DataField(def.Name, t);
        }

        private static Array ConvertToParquetArray(IColumn col)
        {
            // Fast Path: If not nullable and primitive, we might be able to use the array directly?
            // Unfortunately, Values.Span does not return the array, and Parquet.Net requires an Array.
            // We usually have to copy to be safe (snapshot).

            if (col is IntColumn ic)
            {
                if (!ic.IsNullable) return ic.Values.ToArray(); // int[]

                // Nullable Conversion: int[] + bitmap -> int?[]
                var result = new int?[ic.Length];
                for (int i = 0; i < ic.Length; i++)
                    result[i] = ic.IsNull(i) ? null : ic.Get(i);
                return result;
            }

            if (col is DoubleColumn dc)
            {
                if (!dc.IsNullable) return dc.Values.ToArray();

                var result = new double?[dc.Length];
                for (int i = 0; i < dc.Length; i++)
                    result[i] = dc.IsNull(i) ? null : dc.Get(i);
                return result;
            }

            if (col is BoolColumn bc)
            {
                // BoolColumn is bit-packed internally. We need to unpack to bool[] or bool?[]
                if (!bc.IsNullable)
                {
                    var result = new bool[bc.Length];
                    for (int i = 0; i < bc.Length; i++) result[i] = bc.Get(i);
                    return result;
                }
                else
                {
                    var result = new bool?[bc.Length];
                    for (int i = 0; i < bc.Length; i++)
                        result[i] = bc.IsNull(i) ? null : bc.Get(i);
                    return result;
                }
            }

            if (col is StringColumn sc)
            {
                var result = new string?[sc.Length];
                for (int i = 0; i < sc.Length; i++)
                {
                    result[i] = sc.Get(i);
                }
                return result;
            }

            if (col is DateTimeColumn dtc)
            {
                if (!dtc.IsNullable) return dtc.Values.ToArray();

                var result = new DateTime?[dtc.Length];
                for (int i = 0; i < dtc.Length; i++)
                    result[i] = dtc.IsNull(i) ? null : dtc.Get(i);
                return result;
            }

            throw new NotSupportedException($"Writing column type '{col.DataType.Name}' to Parquet is not supported.");
        }
    }
}
